

C o n f i d e n t i a l Introduction To improve health outcomes and research reproducibility, health sciences research has become increasingly focused on the production, management, and sharing of research data. The call to make health sciences research more reproducible and reusable has spearheaded a number of initiatives in the United States (1,2), Europe (3, 4) , and Canada (5) to improve data discoverability, accessibility, and transparency. The importance of data sharing in the health sciences has been well documented. Sharing research data improves the findability and availability of research outputs, which can spearhead new research discoveries (6) (7) (8) (9) (10) (11) ; encourages transparency and holds the research community accountable (12) (13) (14) ; and improves the interoperability of data across research communities and systems (15) (16) (17) .Canada is at a crucial stage of development with respect to improving its data management and sharing initiatives. The Canadian Tri-Agency is drafting research data management (RDM) and sharing funding requirements (18) , Canadian publishers have begun to release data sharing policies (19) , the Federated Research Data Repository (20) has made it possible to discover data that are produced and stored in Canadian repositories, and a New Digital Research Infrastructure Organization was established to respond to emerging data needs within the Canadian digital research landscape (21). Although these efforts aim to make datasets more discoverable, valuable data shared alongside publications, in external discipline-specific repositories, via websites, or by request, are difficult to locate, access, and reuse. The availability of Canadian health sciences research data is a topic that has yet to be explored in the literature but is vital for understanding researchers' data sharing practices in a Canadian context.As the Tri-Agency prepares to release a policy that encourages RDM and data sharing, and new initiatives are established to locate Canadian research products online, we see value in identifying how and where Canadian research data are being shared, and what steps have been taken to make these data reusable. To that end, this study aims to understand the Canadian data sharing landscape by reviewing how and where Canadian Institutes of Health Research (CIHR) funded data is shared, and comparing CIHR-funded researchers' current data sharing practices to the Tri-Agency principles for RDM and sharing (22).

Identification of CIHR-funded publications

This study identified all CIHR-funded publications within the PubMed and PubMed Central (PMC) databases that indicated they shared research data underlying their published results. Both PubMed and PMC have developed dataset search filters (23) that identify publications that indicate data underlying the results have been shared. Within the context of this study, we define research data as "data that are used as primary sources to support technical or scientific This study identified all CIHR-funded publications within the PubMed and PubMed Central (PMC) databases that indicated they shared research data underlying their published results. Both PubMed and PMC have developed dataset search filters (23) that identify publications that indicate data underlying the results have been shared. Within the context of this study, we define research data as "data that are used as primary sources to support technical or scientific C o n f i d e n t i a l 3 enquiry, research, scholarship, or artistic activity, and that are used as evidence in the research process and/or are commonly accepted in the research community as necessary to validate research findings and results." (24) Using PMC, this study first identified all CIHR-funded publications that included a data availability statement. Data availability statements contain the authors' description of where and how to gain access to the research data underlying the published manuscript. Additional publications were identified using PubMed's data filter, which indicates when data have been shared in a data repository. These filters were combined with CIHR-related keywords in English and French, using the grants information field from both databases ( Table 1 ). The date range of our search strategy identified publications on or before December 31, 2019.

Author affiliation

Includes the institutional affiliation and address (including email address, when available) of the authors of the publication as it appears in the journal.x x xPublication date The date that the publication was published.x Examination of CIHR-funded data sharing practicesUsing the extracted metadata, we analyzed each publication (n=4,144) using descriptive statistics to explore data accessibility; how, where and by whom research data was shared; and the inclusion of documentation to support data reuse.Our data collection instrument and descriptive statistics were generated and captured in a REDCap database. The instrument and data dictionary used for our final analysis are available in our OSF project (26) .

Data sharing status

To frame our analysis, we grouped CIHR-funded data sharing practices into four categories representing the most commonly identified data sharing status types (Table 3) . We examined the frequency of each category across our entire sample (n=4,144) and over time. (2) Data available Authors stated either within the manuscript, the data availability statement, or the acknowledgements that research data was available upon request or via an application process.(3) Data sharing not applicable/possible Authors stated either within the manuscript, data availability statement, or acknowledgements that research data sharing was not possible or applicable. To frame our analysis, we grouped CIHR-funded data sharing practices into four categories representing the most commonly identified data sharing status types (Table 3) . We examined the frequency of each category across our entire sample (n=4,144) and over time.

Data sharing methods

Using the metadata available (Table 2 ) and building upon the high level data sharing status categories identified (Table 3) , we recorded all methods of data sharing evident within each publication. Methods of data sharing included but were not limited to sharing data via a data repository, within the supplementary files, by request or application, within the publication, via a website, or when an author stated data sharing was not applicable or possible.If an author's data sharing statement indicated that an application was required to access the data, we captured all reasons why authors insisted on this requirement. Similarly, if an author stated that data could not be shared at all, we captured all reasons provided why this was the case.Finally, we examined whether data sharing statements made by authors within a publication aligned with how data were shared in practice. When authors stated that all research data needed to understand the results were within the publication, we reviewed the publication for evidence that no additional research data files were needed to understand the findings. When authors stated that research data were available in the supplementary files of a publication, we attempted to locate and access the data within the supplementary files section. We documented instances of misalignment between author statements and if and/or how data were shared, as well as when we were unclear about whether author statements reflected data sharing accurately.Using the metadata available (Table 2 ) and building upon the high level data sharing status categories identified (Table 3) , we recorded all methods of data sharing evident within each 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l 4 publication. Methods of data sharing included but were not limited to sharing data via a data repository, within the supplementary files, by request or application, within the publication, via a website, or when an author stated data sharing was not applicable or possible.If an author's data sharing statement indicated that an application was required to access the data, we captured all reasons why authors insisted on this requirement. Similarly, if an author stated that data could not be shared at all, we captured all reasons provided why this was the case.Finally, we examined whether data sharing statements made by authors within a publication aligned with how data were shared in practice. When authors stated that all research data needed to understand the results were within the publication, we reviewed the publication for evidence that no additional research data files were needed to understand the findings. When authors stated that research data were available in the supplementary files of a publication, we attempted to locate and access the data within the supplementary files section. We documented instances of misalignment between author statements and if and/or how data were shared, as well as when we were unclear about whether author statements reflected data sharing accurately.The most frequent method of data sharing was via a repository (37%) followed by within the supplementary files (25%). Notably, 22% of articles stated that data were available either by request (17%) or application (5%), despite providing little detail about how to acquire these data. 21% of publications stated that all data underlying results were available within the content of the publication. 13% of publications had no evidence of or information about data sharing whatsoever. Some publications shared data in multiple formats, and therefore may be represented in more than one category. The types and frequency of all data sharing methods are illustrated in Figure 3 .Among publications that reported data sharing via a repository (n=1,549), there were 97 distinct repositories represented (see analysis report on the OSF for complete listing) (26) . The most represented repositories were the Protein Data Bank (PDB) (n=599, 39%), Gene Expression Omnibus (GEO) (n=377, 24%), and GenBank (n=194, 13%). A complete breakdown of repositories is shown in Figure 4 . 234 publications indicated that an application was required to access the data underlying the results. The most frequent justification for this requirement was the need to complete a data access, transfer or use agreement (28%), followed by general ethics concerns (24%), confidentiality (21%), license restrictions (10%), and Indigenous considerations (2.5%). Nearly 10% of publications provided no explanation for why an application was required. Among publications that required an application, none included metadata sufficiently outlining the requirements for access and approval.Among publications that indicated that data sharing was not applicable or possible (n=300), the most common reason cited for being unable to share data was confidentiality (36%). Over 29% of publications that indicated data sharing was not applicable or possible provided no justification at all. A complete list of reasons why data could not be shared is available in Figure  5 .Finally, our comparison of author data availability statements with actual data sharing practices revealed that 71.8% (n=752) of publications reporting data available in the supplementary files (n=1,048) did not share data in this way. Similarly, 39.7% (n=345) of statements that all data 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   C  o  n  f  i  d  e  n  t  i  a  l   6 were available within the publication (n=870) were flagged on the grounds that although there was clear evidence of data collection, no data was shared within the publication or supplementary files. The authors of this study agreed that in many cases authors may have incorrectly considered tables and figures to be research data.

Research data documentation

To expand our analysis, we explored the types of documentation that were included alongside accessible and available research data (Table 3 , Categories 1 and 2). We identified types of documentation based on the Tri-Agency Statement of Principles on Digital Data Management (22), which makes recommendations on adherence to standards, data collection and storage, and metadata documentation. We then examined each publication to determine whether or not documentation such as study protocols, data analysis plans, software and/or code, data dictionaries, readme files, data collection instruments, videos, or data management plans was provided. Documentation of this kind has been identified as necessary for improving the transparency, reproducibility, and reusability of research results (27) (28) (29) (30) . Recording the presence of these files also enabled an analysis of the frequency of documentation inclusion over time.The CIHR-funded data sharing landscapeOur study identified the institutions that most frequently share research data and the journals where CIHR-funded data sharing frequently occurs. Institutions and journals were categorized and ranked according to their data sharing status (Table 3) .All data collected during the present study were exported from the REDCap database and analyzed using Stata/SE 16.0 software. The raw data extracted from PubMed and PMC, the synthesized data exported from REDCap, and the analyzed data from Stata along with a summary analysis report are available in our OSF Project (26) .The documentation provided alongside the publications in our sample was varied, with supplementary figures and/or tables, study protocols, research data files, and transparent reporting forms most frequently represented (Table 5) . Referring to the recommended documentation types outlined in the Tri-Agency Data Management Principles (22), we examined how frequently these types of documentation were included alongside publications that made data accessible or indicated that data were available ( Table 3 , Categories 1 and 2), over time ( Figure 6 ). Our findings indicate that the types of documentation required to understand and reuse research data are seldom provided in CIHRfunded publications that share data (13%, n=554). Study protocols were the most frequently included at 13.9% and data management plans were least frequent at 0.1%. Although documentation supporting reuse was scarce, the practice of including data-related documentation has grown in the past three years, with our results showing the increasing availability of data analysis plans, software code, and data collection instruments. 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58 The CIHR-funded data sharing landscapeInstitutions associated with the CIHR-funded publications included in our sample are shown in Figure 7A and 7B. The University of Toronto had the greatest proportion of CIHR-funded publications (22.18%, n=919). Among institutions with more than 10 publications, those with the Of the 4,144 publications, the journals used most frequently were PLOS One (17.76%, n=736) followed by the Journal of Biological Chemistry (5.02%, n=208). Among journals with more than 25 CIHR-funded published publications, the top three journals that included examples of accessible data ( Figure 8A and 8B). 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59 60To expand our analysis, we explored the types of documentation that were included alongside accessible and available research data ( Table 3 , Categories 1 and 2). We identified types of documentation based on the Tri-Agency Statement of Principles on Digital Data Management (22), which makes recommendations on adherence to standards, data collection and storage, and metadata documentation. We then examined each publication to determine whether or not documentation such as study protocols, data analysis plans, software and/or code, data dictionaries, readme files, data collection instruments, videos, or data management plans was provided. Documentation of this kind has been identified as necessary for improving the transparency, reproducibility, and reusability of research results (27) (28) (29) (30) . Recording the presence of these files also enabled an analysis of the frequency of documentation inclusion over time.The CIHR-funded data sharing landscape Our study identified the institutions that most frequently share research data and the journals where CIHR-funded data sharing frequently occurs. Institutions and journals were categorized and ranked according to their data sharing status (Table 3) .All data collected during the present study were exported from the REDCap database and analyzed using Stata/SE 16.0 software. The raw data extracted from PubMed and PMC, the synthesized data exported from REDCap, and the analyzed data from Stata along with a summary analysis report are available in our OSF Project (26) . The documentation provided alongside the publications in our sample was varied, with supplementary figures and/or tables, study protocols, research data files, and transparent reporting forms most frequently represented (Table 5 ).Referring to the recommended documentation types outlined in the Tri-Agency Data Management Principles (22), we examined how frequently these types of documentation were included alongside publications that made data accessible or indicated that data were available (Table 3 , Categories 1 and 2), over time ( Figure 6 ). Our findings indicate that the types of documentation required to understand and reuse research data are seldom provided in CIHRfunded publications that share data (13%, n=554). Study protocols were the most frequently included at 13.9% and data management plans were least frequent at 0.1%. Although documentation supporting reuse was scarce, the practice of including data-related documentation has grown in the past three years, with our results showing the increasing availability of data analysis plans, software code, and data collection instruments.The CIHR-funded data sharing landscape Institutions associated with the CIHR-funded publications included in our sample are shown in Figure 7A and 7B. The University of Toronto had the greatest proportion of CIHR-funded publications (22.18%, n=919). Among institutions with more than 10 publications, those with the greatest proportion of publications where data were accessible (Table 3, Category 1) or  available (Table 3 , Category 2) were the Structural Genomics Consortium (95.2%, n=20) and the University of Waterloo (48%, n=12), respectively.Of the 4,144 publications, the journals used most frequently were PLOS One (17.76%, n=736) followed by the Journal of Biological Chemistry (5.02%, n=208). Among journals with more than 25 CIHR-funded published publications, the top three journals that included examples of accessible data (Table 3 This study highlights significant room for growth in improving the discoverability, accessibility, and usability of CIHR-funded research data. While, encouragingly, repositories were the most common venues authors chose to share their data (37%), the remainder of shared data was made available within the publication or its supplementary files, by request or application, via a website, or by a long tail of other means (see analysis report on OSF) (26) . Sharing data in a repository is recognized as best practice because it provides infrastructure for discovery, structured metadata, and long-term, reliable access. The other sharing methods we identified present by comparison significant barriers to discovery, interpretability, and access, in that they make data difficult to find, do not incorporate metadata, and do not facilitate data access consistently. These characteristics conflict with expectations outlined in Canada's Tri-Agency data management (22) and international FAIR guiding principles (15) .Metadata is an essential component of data sharing that provides valuable context about the nature of data, how they were collected, and how they can be reused (31) . Metadata also improve discoverability by applying structured descriptors to data that allow them to be searched for and retrieved. Most sharing methods we encountered during our study did not incorporate metadata, making data difficult to locate, interpret, and reuse. Inadequate metadata descriptions are a recognized problem in the data sharing landscape (32) (33) (34) , and the CIHRfunded data sharing practices assessed in this study are no different. Without adequate metadata, these data will remain hidden within the publication and their utility for future research remains in question (35) .In instances where authors indicated that data was available by request or application (22%), they did not provide adequate instructions on how to formally acquire the data, leaving interested researchers with no guidance on what a successful request would look like. The absence of metadata elaborating on application requirements calls into question the true availability of these data, and impedes future research based upon them. The challenges of requesting access to data in the health sciences have been studied elsewhere in relation to the inadequate transparency and standardization of data use agreements (33, 36, 37) . Because data available by request or application are often collected from human participants, improving the discoverability of and access to these sensitive data will help prevent unnecessary study replication, create opportunities for pooling related data, and increase research efficiency to accelerate new discoveries (33, 34, 36, 37) . Our findings indicate that current practices in CIHRfunded data sharing lack the standardization and transparency necessary to secure these outcomes.In our examination of practices that support reusability, we found that the most frequent types of documentation shared alongside data -supplementary figures and/or tables and study protocols -do not generally support the interpretation and reuse of data. Tables and figures were often reiterations of visualizations presented within the body of the publication; study protocols and data collection instruments, while helpful for contextualizing how data was gathered and analyzed, do little to help others understand the data themselves and how to C o n f i d e n t i a l 8 interact with them. Descriptive documentation for data such as codebooks and data dictionaries, and actionable supporting files such as code and software, are increasingly recognized as necessary components of human-and machine-readable research data (27, 30, 38) , and our results indicate that CIHR-funded research data sharing practices can vastly improve in this area.As Canada implements new data sharing requirements for federally funded research, our study highlights the importance of developing policies and standards at the federal and institutional levels to ensure that all research data underlying published findings have quality metadata attached to them, and that sufficient documentation to support interpretation and reuse is provided. Future directions of study should focus on the development of metadata standards for sensitive data to facilitate reuse and support transparent data request processes. We also recommend that Canadian data repositories explore how to better accommodate sensitive data so that they can be made discoverable while honouring access restrictions and privacy requirements.

Results

Of the 4,144 CIHR-funded publications included in this study, 45% made their data accessible, 22% made their data available (via request or application), 7% indicated data sharing was not applicable or possible, and despite isolating our sample to publications that had indicated data sharing of some kind, 38% provided no evidence of data sharing ( Table 4 ). Note that these categories are not mutually exclusive, as many publications shared multiple datasets in different ways. Figure 1 demonstrates the extent of overlap between the four data sharing status types, and Figure 2 examines the frequency of these four categories over time. 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59 Data sharing methodsThe most frequent method of data sharing was via a repository (37%) followed by within the supplementary files (25%). Notably, 22% of articles stated that data were available either by request (17%) or application (5%), despite providing little detail about how to acquire these data. 21% of publications stated that all data underlying results were available within the content of the publication. 13% of publications had no evidence of or information about data sharing whatsoever. Some publications shared data in multiple formats, and therefore may be represented in more than one category. The types and frequency of all data sharing methods are illustrated in Figure 3 . Figure 4 . *Asterisks denote repositories that were not anticipated within our instrument, and were added from a free-text "Other" category during data collection.234 publications indicated that an application was required to access the data underlying the results. The most frequent justification for this requirement was the need to complete a data access, transfer or use agreement (28%), followed by general ethics concerns (24%), confidentiality (21%), license restrictions (10%), and Indigenous considerations (2.5%). Nearly 10% of publications provided no explanation for why an application was required. Among publications that required an application, none included metadata sufficiently outlining the requirements for access and approval.Among publications that indicated that data sharing was not applicable or possible (n=300), the most common reason cited for being unable to share data was confidentiality (36%). Over 29% of publications that indicated data sharing was not applicable or possible provided no justification at all. A complete list of reasons why data could not be shared is available in Figure  5 . Of the 4,144 CIHR-funded publications included in this study, 45% made their data accessible, 22% made their data available (via request or application), 7% indicated data sharing was not applicable or possible, and despite isolating our sample to publications that had indicated data sharing of some kind, 38% provided no evidence of data sharing ( Table 4 ). Note that these categories are not mutually exclusive, as many publications shared multiple datasets in different ways. Figure 1 demonstrates the extent of overlap between the four data sharing status types, and Figure 2 examines the frequency of these four categories over time.

Interpretation

This study highlights significant room for growth in improving the discoverability, accessibility, and usability of CIHR-funded research data. While, encouragingly, repositories were the most common venues authors chose to share their data (37%), the remainder of shared data was made available within the publication or its supplementary files, by request or application, via a website, or by a long tail of other means (see analysis report on OSF) (26) . Sharing data in a repository is recognized as best practice because it provides infrastructure for discovery, structured metadata, and long-term, reliable access. The other sharing methods we identified present by comparison significant barriers to discovery, interpretability, and access, in that they make data difficult to find, do not incorporate metadata, and do not facilitate data access consistently. These characteristics conflict with expectations outlined in Canada's Tri-Agency data management (22) and international FAIR guiding principles (15) .Metadata is an essential component of data sharing that provides valuable context about the nature of data, how they were collected, and how they can be reused (31) . Metadata also improve discoverability by applying structured descriptors to data that allow them to be searched for and retrieved. Most sharing methods we encountered during our study did not incorporate metadata, making data difficult to locate, interpret, and reuse. Inadequate metadata descriptions are a recognized problem in the data sharing landscape (32) (33) (34) , and the CIHRfunded data sharing practices assessed in this study are no different. Without adequate metadata, these data will remain hidden within the publication and their utility for future research remains in question (35) .In instances where authors indicated that data was available by request or application (22%), they did not provide adequate instructions on how to formally acquire the data, leaving interested researchers with no guidance on what a successful request would look like. The absence of metadata elaborating on application requirements calls into question the true availability of these data, and impedes future research based upon them. The challenges of requesting access to data in the health sciences have been studied elsewhere in relation to the inadequate transparency and standardization of data use agreements (33, 36, 37) . Because data available by request or application are often collected from human participants, improving the discoverability of and access to these sensitive data will help prevent unnecessary study replication, create opportunities for pooling related data, and increase research efficiency to accelerate new discoveries (33, 34, 36, 37) . Our findings indicate that current practices in CIHRfunded data sharing lack the standardization and transparency necessary to secure these outcomes.In our examination of practices that support reusability, we found that the most frequent types of documentation shared alongside data -supplementary figures and/or tables and study protocols -do not generally support the interpretation and reuse of data. Tables and figures were often reiterations of visualizations presented within the body of the publication; study protocols and data collection instruments, while helpful for contextualizing how data was gathered and analyzed, do little to help others understand the data themselves and how tointeract with them. Descriptive documentation for data such as codebooks and data dictionaries, and actionable supporting files such as code and software, are increasingly recognized as necessary components of human-and machine-readable research data (27, 30, 38) , and our results indicate that CIHR-funded research data sharing practices can vastly improve in this area.As Canada implements new data sharing requirements for federally funded research, our study highlights the importance of developing policies and standards at the federal and institutional levels to ensure that all research data underlying published findings have quality metadata attached to them, and that sufficient documentation to support interpretation and reuse is provided. Future directions of study should focus on the development of metadata standards for sensitive data to facilitate reuse and support transparent data request processes. We also recommend that Canadian data repositories explore how to better accommodate sensitive data so that they can be made discoverable while honouring access restrictions and privacy requirements.

Limitations of the study

This study used a sample of CIHR-funded publications pulled exclusively from PubMed and PMC. While these are the most comprehensive biomedical databases available, there are likely other databases where CIHR-funded publications exist. To manage study feasibility, we limited our review of documentation to that which was shared or stated within the publication and did not extend this analysis to repositories or websites where some research data was shared.This study used a sample of CIHR-funded publications pulled exclusively from PubMed and PMC. While these are the most comprehensive biomedical databases available, there are likely other databases where CIHR-funded publications exist. To manage study feasibility, we limited our review of documentation to that which was shared or stated within the publication and did not extend this analysis to repositories or websites where some research data was shared.

Conclusion

This study surveys the complex landscape of CIHR-funded data sharing practices, revealing a diverse range of data sharing methods and, in 38% of cases, an absence of data sharing altogether. It is remarkable that over 70% of publications that shared data did not incorporate sufficient metadata or documentation to facilitate discovery, access, and reuse. Without policies and standards in place that anticipate the upcoming Tri-Agency data management policy, and enhanced support for researchers seeking to implement best practices in data management and sharing, the majority of publicly funded research data will remain hidden, inaccessible, and unusable. For CIHR-funded data in particular, transparent metadata and reporting guidelines for sensitive data will be essential for improving data discoverability and accessibility across the health sciences.This study surveys the complex landscape of CIHR-funded data sharing practices, revealing a diverse range of data sharing methods and, in 38% of cases, an absence of data sharing altogether. It is remarkable that over 70% of publications that shared data did not incorporate sufficient metadata or documentation to facilitate discovery, access, and reuse. Without policies and standards in place that anticipate the upcoming Tri-Agency data management policy, and enhanced support for researchers seeking to implement best practices in data management and sharing, the majority of publicly funded research data will remain hidden, inaccessible, and unusable. For CIHR-funded data in particular, transparent metadata and reporting guidelines for sensitive data will be essential for improving data discoverability and accessibility across the health sciences.

Data Availability Statement

All raw, processed, and analyzed data, as well as accompanying documentation, reports and scripts are available on the Open Science Framework at https://osf.io/n9jv5. 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l Introduction To improve health outcomes and research reproducibility, health sciences research has become increasingly focused on the production, management, and sharing of research data. The call to make health sciences research more reproducible and reusable has spearheaded a number of initiatives in the United States (1,2), Europe (3, 4) , and Canada (5) to improve data discoverability, accessibility, and transparency. The importance of data sharing in the health sciences has been well documented. Sharing research data improves the findability and availability of research outputs, which can spearhead new research discoveries (6-11); encourages transparency and holds the research community accountable (12) (13) (14) ; and improves the interoperability of data across research communities and systems (15) (16) (17) .Canada is at a crucial stage of development with respect to improving its data management and sharing initiatives. The Canadian Tri-Agency is drafting research data management (RDM) and sharing funding requirements (18) , Canadian publishers have begun to release data sharing policies (19) , the Federated Research Data Repository (20) has made it possible to discover data that are produced and stored in Canadian repositories, and a New Digital Research Infrastructure Organization was established to respond to emerging data needs within the Canadian digital research landscape (21). Although these efforts aim to make datasets more discoverable, valuable data shared alongside publications, in external discipline-specific repositories, via websites, or by request, are difficult to locate, access, and reuse. The availability of Canadian health sciences research data is a topic that has yet to be explored in the literature but is vital for understanding researchers' data sharing practices in a Canadian context.As the Tri-Agency prepares to release a policy that encourages RDM and data sharing, and new initiatives are established to locate Canadian research products online, we see value in identifying how and where Canadian research data are being shared, and what steps have been taken to make these data reusable. To that end, this study aims to understand the Canadian data sharing landscape by reviewing how and where Canadian Institutes of Health Research (CIHR) funded data is shared, and comparing CIHR-funded researchers' current data sharing practices to the Tri-Agency principles for RDM and sharing (22).All raw, processed, and analyzed data, as well as accompanying documentation, reports and scripts are available on the Open Science Framework at https://osf.io/n9jv5. 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l Figure 4 . Frequency of data repositories used to store CIHR-funded research data (n=1,544). Asterisks denote repositories that were not anticipated within our instrument, and were added from a free-text "Other" category during data collection.

Metadata extraction

After removing duplicates based on overlap between PubMed and PMC, 4,988 publications remained (PMC=4039, PubMed=949). Using this sample, select metadata fields were extracted from each publication for analysis using the Open Access Subset API (25) , which allows the full text metadata from a publication to be extracted under a Creative Commons license. When full text metadata was not available via the Open Access Subset, it was extracted using the minimal level of metadata available in PMC. Publications that were not available in PMC (n=949) had a limited set of metadata extracted from PubMed ( Table 2 ). The metadata extraction process was successful in retrieving the metadata for 4,144 publications, which served as the sample for our analysis. The Python scripts used to extract the metadata are available via our Open Science Framework (OSF) Project (26) .

Examination of CIHR-funded data sharing practices

Using the extracted metadata, we analyzed each publication (n=4,144) using descriptive statistics to explore data accessibility; how, where and by whom research data was shared; and the inclusion of documentation to support data reuse.Our data collection instrument and descriptive statistics were generated and captured in a REDCap database. The instrument and data dictionary used for our final analysis are available in our OSF project (26) .

Page 37 of 46

For Peer Review Only   1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   C  o  n  f  i  d  e  n  t  i  a  l   Table 3 . Data sharing status categories and their definitions

Status Category Definition

(1) Data accessible Research data files could be identified, accessed, and downloaded.(2) Data available Authors stated either within the manuscript, the data availability statement, or the acknowledgements that research data was available upon request or via an application process.(3) Data sharing not applicable/possible Authors stated either within the manuscript, data availability statement, or acknowledgements that research data sharing was not possible or applicable.(4) No evidence of data sharingAuthors made no mention of data sharing within the manuscript, data availability statement, or acknowledgements; indicated data sharing would be available at a future date; or the publication contained no research data files. 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 C o n f i d e n t i a l 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 

