{
    "paper_id": "0b25ce5c709536ed1a072f18c6bb96a3623ad5a8",
    "metadata": {
        "title": "Run-time Complexity Bounds Using Squeezers",
        "authors": [
            {
                "first": "Oren",
                "middle": [],
                "last": "Ish-Shalom",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Tel Aviv University",
                    "location": {
                        "settlement": "Tel Aviv",
                        "country": "Israel"
                    }
                },
                "email": ""
            },
            {
                "first": "Shachar",
                "middle": [],
                "last": "Itzhaky",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Technion",
                    "location": {
                        "settlement": "Haifa",
                        "country": "Israel"
                    }
                },
                "email": ""
            },
            {
                "first": "Noam",
                "middle": [],
                "last": "Rinetzky",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Tel Aviv University",
                    "location": {
                        "settlement": "Tel Aviv",
                        "country": "Israel"
                    }
                },
                "email": ""
            },
            {
                "first": "Sharon",
                "middle": [],
                "last": "Shoham",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Tel Aviv University",
                    "location": {
                        "settlement": "Tel Aviv",
                        "country": "Israel"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Determining upper bounds on the time complexity of a program is a fundamental problem with a variety of applications, such as performance debugging, resource certification, and compile-time optimizations. Automated techniques for cost analysis excel at bounding the resource complexity of programs that use integer values and linear arithmetic. Unfortunately, they fall short when execution traces become more involved, esp. when data dependencies may affect the termination conditions of loops. In such cases, state-of-the-art analyzers have shown to produce loose bounds, or even no bound at all. We propose a novel technique that generalizes the common notion of recurrence relations based on ranking functions. Existing methods usually unfold one loop iteration, and examine the resulting relations between variables. These relations assist in establishing a recurrence that bounds the number of loop iterations. We propose a different approach, where we derive recurrences by comparing whole traces with whole traces of a lower rank, avoiding the need to analyze the complexity of intermediate states. We offer a set of global properties, defined with respect to whole traces, that facilitate such a comparison, and show that these properties can be checked efficiently using a handful of local conditions. To this end, we adapt state squeezers, an induction mechanism previously used for verifying safety properties. We demonstrate that this technique encompasses the reasoning power of bounded unfolding, and more. We present some seemingly innocuous, yet intricate, examples where previous tools based on cost relations and control flow analysis fail to solve, and that our squeezer-powered approach succeeds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Cost analysis is the problem of estimating the resource usage of a given program, over all of its possible executions. It complements functional verification-of safety and liveness properties-and is an important task in formal software certification. When used in combination with functional verification, cost analysis ensures that a program is not only correct, but completes its processing in a reasonable amount of time, uses a reasonable amount of memory, communication bandwidth, etc. In this work we focus on run-time complexity analysis. While the area has been studied extensively, e.g., [19] , [28] , [3] , [14] , [6] , [16] , [21] , [12] , [9] , the general problem of constraining the number of iterations in programs containing loops with arbitrary termination conditions remains hard.",
            "cite_spans": [
                {
                    "start": 597,
                    "end": 601,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 604,
                    "end": 608,
                    "text": "[28]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 611,
                    "end": 614,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 617,
                    "end": 621,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 624,
                    "end": 627,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 630,
                    "end": 634,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 637,
                    "end": 641,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 644,
                    "end": 648,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 651,
                    "end": 654,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "A prominent approach to computing upper bounds on the time complexity of a program identifies a well-founded numerical measure over program states that decreases in every step of the program, also called a ranking function. In this case, an upper bound on the measure of the initial states comprises an upper bound on the program's time complexity. Finding such measures manually is often extremely difficult. The cost relations approach, dating back to [28] , attempts to automate this process by using the control flow graph of the program to extract recurrence formulas that characterize this measure. Roughly speaking, the recurrences relate the measures (costs) of adjacent nodes in the graph, taking into account the cost of the step between them. In this way, the cost relations track the evolution of the measure between every pair of consecutive states along the executions of the program.",
            "cite_spans": [
                {
                    "start": 454,
                    "end": 458,
                    "text": "[28]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "One limitation of cost relations is the need to capture the number of steps remaining for execution in every state, that is, all intermediate states along all executions. If the structure of the state is complex, this may require higher order expressions, e.g., summing over an unbounded number of elements. As an example, consider the program in Fig. 1 that implements a binary counter represented by an array of bits.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 347,
                    "end": 353,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "In this case, a ranking function that decreases between every two consecutive iterations of the loop, or even between two iterations that print the value of the counter, depends on the entire content of the array. Attempting to express a ranking function over the scalar variables of this program is analogous to abstracting the loop as a finitestate system that ignores the content of the array, and as such contains transition cycles (e.g. the abstract state n \u2192 n 0 , i \u2192 0 , obtained by projecting the state to the scalar variables only, repeats multiple times in any trace)-meaning that no strictly decreasing function can be defined in this way. Similarly, any attempt to consider a bounded number of bits will encounter the same difficulty.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we propose a novel approach for extracting recurrence relations capturing the time complexity of an imperative program, modeled as a transition system, by relating whole traces instead of individual states. The key idea is to relate a trace to (one or more) shorter traces. This allows to formulate a recurrence that resolves to the length of the trace and recurs over the values at the initial states only. We sidestep the need to take into account the more complex parts of the state that change along the trace (e.g., in the case of the binary counter, the array is initialized with zeros).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our approach relies on the notion of state squeezers [22] , previously used exclusively for the verification of safety properties. We present a novel aspect where the same squeezers can be used to determine complexity bounds, by replacing the safety property check with trace length judgements.",
            "cite_spans": [
                {
                    "start": 53,
                    "end": 57,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Squeezers provide a means to perform induction on the \"size\" of (initial) states to prove that all reachable states adhere to a given specification. This is accomplished by attaching ranks from a well-founded set to states, and defining a squeezer function that maps states to states of a lower rank. Note that the notion of a rank used in our work is distinct from that of a ranking function, and the two should not be confused; in particular, a rank is not required to decrease on execution steps. Previously, squeezers were utilized for safety verification: the ability to establish safety is achieved by having the squeezer map states in a way that forms a (relaxed form of) a simulation relation, ensuring that the traces of the lower-rank states simulate the traces of the higher rank states. Due to the simulation property, which is verified locally, safety over states with a base rank, carries over (by induction over the rank) to states of any higher rank.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this work, we use the construction of well-founded ranks and squeezers to define a recurrence formula representing (an upper bound on) the time complexity of the procedure being analyzed. We do so by expressing the complexity (length) of traces in terms of the complexity of lower-rank traces. This new setting raises additional challenges: it is no longer sufficient to relate traces to lower-rank traces; we also need to quantify the discrepancy between the lengths of the traces, as well as between their ranks. This is achieved by a certain form of simulation that is parameterized by stuttering shapes (for the lengths) and by means of a rank bounding function (for the ranks). Furthermore, while [22] limits each trace to relate to a single lower-rank trace, we have found that it is sometimes beneficial to employ a decomposition of the original trace into several consecutive trace segments, so that each segment corresponds to some (possibly different) lower-rank trace.The segmentation simplifies the analysis of the length of the entire trace, since it creates sub-analyses that are easier to carry out, and the sum of which gives the desired recurrence formula. This also enables a richer set of recurrences to be constructed automatically, namely non-single recurrences (meaning that the recursive reference may appear more than once on the right hand side of the equation).",
            "cite_spans": [
                {
                    "start": 705,
                    "end": 709,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The base case of the recurrence is obtained by computing an upper bound on the time complexity of base-rank states. This is typically a simpler problem that may be addressed, e.g., by symbolic execution due to the bounded nature of the base. The solution to the recurrence formula with the respective base case soundly overapproximates the time complexity of the procedure.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "We show that, conceptually, the classical approach for generating recurrences based on ranking functions can be viewed as a special case of our approach where the squeezer maps a state to its immediate successor. The real power of our approach is in the freedom to define other squeezers, producing simpler recursions, and avoiding the need for complex ranking functions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Our use of squeezers for extracting recurrences that bound the complexity of imperative programs is related to the way analyses for functional programs (e.g. [20] ) use the term(s) in recursive function calls to extract recurrences. The functional programming style coincidentally provides such candidate terms. The novelty of our approach is in introducing the concept of a squeezer explicitly, leading to a more flexible analysis as it does not restrict the squeezer to follow specific terms in the program. In particular, this allows reasoning over space in imperative programs as well.",
            "cite_spans": [
                {
                    "start": 158,
                    "end": 162,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The main results of this paper can be summarized as follows: -We propose a novel technique for run-time complexity analysis of imperative programs based on state squeezers. Squeezers, together with rank-bounding functions, are used for extracting recurrence relations whose solutions overapproximate the length of executions of the input program. -We formalize the notions of state squeezers, partitioned simulation and rank bounding functions that underlie the approach, and establish conditions that ensure soundness of the recurrence relations. -We demonstrate that squeezers and rank bounding functions can be efficiently synthesized and verified, due to their compactness, especially relative to explicit ranking functions. -We implemented our approach and applied it successfully to several small but intricate programs, some of which could not have been handled by existing techniques.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section we give a high level description of our technique for complexity analysis using the binary counter example in Fig. 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 126,
                    "end": 132,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Overview"
        },
        {
            "text": "Example: Binary counter The procedure in Fig. 1 receives as an input a number n of bits and iterates over all their possible values in the range 0...2 n \u2212 1. The \"current\" value is maintained in an array c which is initialized to zero and whose length is n. c[0] represents the least significant bit. The loop scans the array from the least significant bit forward looking for the leftmost 0 and zeroing the prefix of 1s. As soon as it encounters a 0, it sets it to 1 and starts the scan from the beginning. The program terminates when it reaches the end of the array (i = n), all array entries are zeros, and the last value was 111 . . .; at this point all the values have been enumerated.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 41,
                    "end": 47,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Overview"
        },
        {
            "text": "Existing analyses All recent methods that we are aware of (such as [16, 4, 20] ) fail to analyze the complexity of this procedure (in fact, most methods will fail to realize that the loop terminates at all). One reason for that is the need to model the contents of the array whose size in unknown at compile time. However, even if data were modeled somehow and taken into account, finding a ranking function, which underlies existing approaches, is hard since this function is required to decrease between any two consecutive iterations along any execution. Here for instance, to the best of our knowledge, such a function would depend on an unbounded number of elements of the array; it would need to extract the current value as an integer, along the lines of n\u22121 j=0 c[j] \u00b7 2 j . The use of a ranking function for complexity analysis is somewhat analogous to the use of inductive invariants in safety verification. Both are based on induction over time along an execution. This paper is inspired by previous work [22] showing that verification can also be done when the induction is performed on the size (rank) of the state rather than on the number of iterations, where the size of the state may correspond, e.g., to the size of an unbounded data structure. We argue that similar concepts can be applied in a framework for complexity classification. That is, we try to infer a recurrence relation that is based on the rank of the state and correlates the lengths of complete executions-executions that start from an initial state-of different ranks. This sidesteps the need to express the length of partial executions, which start from intermediate states. While the approach applies to bounded-state systems as well, its benefits become most apparent when the program contains a-priori unbounded stores, such as arrays.",
            "cite_spans": [
                {
                    "start": 67,
                    "end": 71,
                    "text": "[16,",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 72,
                    "end": 74,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 75,
                    "end": 78,
                    "text": "20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Overview"
        },
        {
            "text": "Our approach. Roughly speaking, our approach for computing recurrence formulas that provide an upper bound on the complexity of a procedure is based on the following ingredients:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Overview"
        },
        {
            "text": "-A rank function r : init \u2192 X that maps initial states to ranks from a well founded set (X, \u227a) with base B. Intuitively, the rank of the initial state governs the time complexity of the entire trace, and we also consider it to be the rank of the trace. As we shall soon see, this rank can be significantly simpler than a ranking function. .d] \u2192 X that provides an upper bound on the rank of the initial states of the d mini-traces based on the rank of the higher-rank trace. (The rank is not required to be uniform across mini-traces).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Overview"
        },
        {
            "text": "All of these ingredients are synthesized automatically, as we discuss in Section 4. Next, we elaborate on each of these ingredients, and illustrate them using the binary counter example. We further demonstrate how we use these ingredients to find recurrence formulas describing (an upper bound on) the complexity of the program.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Overview"
        },
        {
            "text": "We adopt a standard encoding of a program as a transition system over a state space \u03a3, with a set of initial states init \u2286 \u03a3 and transition function tr : \u03a3 \u2192 \u03a3, where a transition corresponds to a loop iteration. We use reach \u2286 \u03a3 to denote the set of reachable states, reach = {\u03c3 | \u2203\u03c3 0 , k. tr k (\u03c3 0 ) = \u03c3 \u2227 \u03c3 0 \u2208 init}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Defining the rank of a state Ranks are taken from a well founded set (X, \u227a) with a basis B \u2286 X that contains all the minimal elements of X. The rank function, r : init \u2192 X, aims to abstract away irrelevant data from the (initial) state that does not effect the execution time, and only uses state \"features\" that do. When proper ranks are used, the rank of an initial state is all that is needed to provide a tight bound on its trace length. Since ranks are taken from a well founded set, they can be recursed over. In the binary counter example, the chosen rank is n, namely, the rank function maps each state to the size of the array. (Notice that the rank does not depend on the contents of the array; in contrast, bounding the trace length from any intermediate state, and not just initial states, would have required considering the content of the array). Given the rank function, our analysis extracts a recurrence formula for the complexity function comp x : X \u2192 N \u222a {\u221e} that provides an upper bound on the number of iterations of tr based on the rank of the initial states. In our exposition, we sometimes also refer to a time complexity function over states, comp s : init \u2192 N \u222a {\u221e}, which is defined directly on the (initial) states, as the number of iterations in an execution that starts with some \u03c3 0 \u2208 init.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Defining a squeezer The squeezer : \u03a3 \u2192 \u03a3 is a function that maps states to states of lower-rank traces (where the rank of a trace is determined by the rank of its initial state), down to the base ranks B. Its importance is in defining a correspondence between higher-rank traces and lower-rank ones that can be verified locally, by examining individual states rather than full traces. The kind of correspondence that the squeezer is required to ensure affects the flexibility of the approach and the kind of recurrence formulas that it may yield. To start off, consider a rather naive squeezer that satisfies the following local properties:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "rank decrease of non-base initial states:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "\u2022 k-step: \u03c3 \u2208 reach \u21d2 \u2203k. tr ( (\u03c3)) = (tr k (\u03c3)).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "As an example, the squeezer we consider for the binary counter program is rather intuitive: it removes the least significant bit (c[0]), and adjusts the index i accordingly. Doing so yields a state with rank r ( (\u03c3 0 )) = r (\u03c3 0 ) \u2212 1. Fig. 2 shows the correspondence between a 4-bit binary counter, and a 3-bit one. The figure illustrates the simulation k-step property for k = 1, 2, 3: \u03c3 0 and \u03c3 3 are (3, 1)-stuttering, \u03c3 1 and \u03c3 4 are (2, 1)-stuttering, and \u03c3 2 , \u03c3 5 and \u03c3 6 are (1, 1)-stuttering.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 236,
                    "end": 242,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Some notations"
        },
        {
            "text": "The simulation property induces a correlation between a higher rank trace \u03c4 and a lower rank one \u03c4 , such that every step of \u03c4 is matched by k steps in \u03c4 . Whenever a state \u03c3 satisfies the k-step property, we will refer to it as being (k, 1)-stuttering. ( We usually only care about the smallest k that satisfies the property for a given \u03c3.) Now suppose that there exists some k \u2208 N + such that for every trace \u03c4 (\u03c3 0 ) and every state \u03c3 \u2208 \u03c4 (\u03c3 0 ), \u03c3 is (k, 1)-stuttering with 1 \u2264 k \u2264 k. This would yield the following complexity bound:",
            "cite_spans": [
                {
                    "start": 254,
                    "end": 255,
                    "text": "(",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "All your base 3 What should happen if we repeatedly apply to some initial state \u03c3 0 , each time obtaining a new, lower-rank trace? Since r ( (\u03c3 0 )) \u227a r (\u03c3 0 ), and since (X, \u227a) is well-founded, we will eventually hit some state of base rank:",
            "cite_spans": [
                {
                    "start": 14,
                    "end": 15,
                    "text": "3",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Hence, if we know the complexity of the initial states with a base rank, we can apply Eq. (1) iteratively to compute an upper bound of the complexity of any initial state. How many steps will be needed to get from an arbitrary initial state \u03c3 0 to \u03c3 \u2022 0 ? Clearly, this depends on the rank, and the way in which decreases it.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Consider the binary counter program again, with the rank r (\u03c3) = n. (N, <) is well-founded, with a single minimum 0. If we define, e.g., B = {0, 1}, we know that the length of any trace with n \u2208 B is bounded by a constant, 2. (Bounding the length of traces starting from an initial state \u03c3 0 where r (\u03c3 0 ) \u2208 B can be done with known methods, e.g., symbolic execution). Since the rank decreases by 1 on each \"squeeze\", we get the following exponential bound:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "The last logical step, going from (1) to (2) , is, in fact, highly involved: since Eq. (1) is a mapping of states, solving such a recurrence for arbitrary cannot be carried out using known automated methods. Instead, we implicitly used the rank of the state, n, to extract a recurrence over scalar values and obtain a closed-form expression. Let us make this reasoning explicit by first expressing Eq. (1) in terms of comp x instead of comp s :",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 44,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Here, n \u2212 1 denotes the rank obtained when squeezing an initial state of rank n. Unlike Eq. (1), this is a recurrence formula over (N, <) that may be solved algorithmically, leading to the solution comp x (n) = O(3 n ).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Surplus analysis Assuming the worst k for all the states in the trace can be too conservative; in particular, if there are only a few states that satisfy the k-step property, and all the others satisfy the 1-step property. In the latter case, if we know that at most b states in any one trace have k > 1, we can formulate the tighter bound:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Incidentally, in the current setting of the binary counter program, the number of ksteps (3-steps) is not bounded. So we cannot apply the inequality (3) repeatedly on any trace, as the number of 3-steps depends on the initial state. However, we can improve the analysis by partitioning the trace to two parts, as we explain next.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Segments and mini-traces Note that both (1) and (3) \"suffer\" from an inherent restriction that the right hand side contains exactly one recursive reference. As such, they are limited in expressing certain kinds of complexity classes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "In order to get more diverse recurrences, including non-single recurrences, we propose an extension of the simulation property that allows more than one lower-rank trace:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "This definition allows a new mini-trace to start at any point along a higher-rank trace \u03c4 , thus marking the beginning of a new segment of \u03c4 . When this occurs, we call tr (\u03c3) a switch state. For the sake of uniformity, we also refer to all initial states \u03c3 0 \u2208 init as switch states. Hence, each segment of \u03c4 starts with a switch state, and the mini-traces are the lower-level traces that correspond to the segments (these are the traces that start from (\u03c3 s ), where \u03c3 s is a switch state). The length of \u03c4 can now be expressed as the sum of lower-level mini-traces.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "However, there are two problems remaining. First, we need to extend the \"rank decrease of non-base initial states\" requirement to any switch state in order to ensure that the ranks of all mini-traces are indeed lower. Namely, we need to require that if \u03c3 s is any switch state in a trace from \u03c3 0 , then r (\u03c3 s ) \u227a r(\u03c3 0 ). Second, even if we extend the rank decrease requirement, this definition does not suggest a way to bound the number of correlated mini-traces and their respective ranks, and therefore suggests no effective way to produce an equation for comp s as before.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "To sidestep the problem of a potentially unbounded number of mini-traces, we augment the definition of simulation with a trace partition function; to address the challenge of the rank decrease we use a rank-bounding function, which is responsible both for ensuring that the rank of the mini-traces decreases and for bounding their ranks.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Defining a partition We define a function p d : \u03a3 \u2192 {1, . . . , d}, parameterized by a constant d, called a partition function, that is weakly monotone along any trace (p d (\u03c3) \u2264 p d (tr (\u03c3))). This function induces a partition of any trace \u03c4 into (at most) d segments by grouping states based on the value of p d (\u03c3). To ensure the segments and mini-traces are aligned, we require that switch states only occur at segment boundaries.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "d-partitioned simulation:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "\u2022 initial anchor:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "In our running example, let us change so that it shrinks the state by removing the most significant bit instead of the least. This leads to a partition of the execution trace for r (\u03c3 0 ) = n into two segments, as shown in Fig. 3 . The partition function is p d = (i \u2265 n || c[n \u2212 1]) ? 2 : 1 (essentially, c[n \u2212 1] + 1, except that the final state is slightly different). As can be seen from the figure, each segment simulates a mini-trace",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 223,
                    "end": 229,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Some notations"
        },
        {
            "text": "tr tr tr tr tr tr tr tr",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "tr tr tr tr tr tr of rank n \u2212 1, with k = 1 for all the steps except for the last step (at \u03c3 28 ) where k = 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "In this case, it would be folly to use the recurrence (1) with k = 2, since all the steps are 1:1 except one. Instead, we can formulate a tighter bound:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Where: comp s (\u03c3 0 ), comp s (\u03c3 0 ) are the lengths of the mini-traces, and 2 is the surplus from the switch transition \u03c3 14 \u2192 \u03c3 15 plus the 2-step at \u03c3 28 . In the case of this program, we know that r (\u03c3 0 ) = r (\u03c3 0 ) = r (\u03c3 0 )\u22121, for any initial state \u03c3 0 , therefore, turning to comp x , we can derive and solve the recurrence comp x (n) = 2\u00b7comp x (n\u22121)+2, which together with the base yields the bound:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Clearly, a general condition is required in order to identify the ranks of the corresponding initial states of the (lower-rank) mini-traces (and at the same time, ensure that they decrease).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Bounding the ranks of squeezed switch states This is not a trivial task, since as previously noted, the squeezed ranks could be different, and may depend on properties present in the corresponding switch states. To achieve this goal, once a partition function p d is defined, we also define a rank-bounding function\u02c6 : X \u00d7 {1, . . . , d} \u2192 X, where for any \u03c3 0 \u2208 init and switch state \u03c3 s ,\u02c6 provides a bound for the rank of (\u03c3 s ) based on that of \u03c3 0 :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "The rightmost inequality ensures that a mini-trace that starts from (\u03c3 s ) is of lowerrank than \u03c3 0 , and as such extends the \"rank decrease\" requirement to all mini-traces. Based on this restriction, we can formulate a recurrence for comp x based on the initial rank \u03c1 = r (\u03c3 0 ), as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Where b, as before, is the number of k-steps for which k > 1, and k is the bound on k (k \u2264 k). The expression (d \u2212 1) represents the transitions between segments, and k \u00b7 b represents the surplus of the \u03c1-rank trace over the total lengths of the mini-traces.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "It should be clear from the definition above, that\u02c6 is quite intricate. How would we compute it effectively? The rank decrease of the initial states and the simulation properties were local by nature, and thus amenable to validation with an SMT solver. The\u02c6 function is inherently global, defined w.r.t. an entire trace. This makes the property (4) challenging for verification methods based on SMT. To render this check more feasible with first-order reasoning, we introduce two special cases where the problem of checking (4) becomes easier: rank preservation and a single segment, explained next.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Taming\u02c6 with rank preservation To obtain rank preservation, we extend the rank function to all states (instead of just the initial states), and require that the rank is preserved along transitions. This is appropriate in some of the scenarios we encountered. For example, the binary counter illustration satisfies the property that along any execution",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": ". Rank preservation means that given a switch state \u03c3 s of an arbitrary segment i, we know that r(\u03c3 s ) = r(\u03c3 0 ). Once this is set, only needs to overapproximate the rank of (\u03c3 s ) in terms of the rank of the same state \u03c3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Taming\u02c6 with a single segment In this case, checking (4) reduces to a single check of the initial state, which is the only switch state. It turns out that the restriction to a single segment is still expressive enough to handle many loop types.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "Putting it all together Theoretically, r , , p d , and\u02c6 can be manually written by the user. However, this is a rather tedious task, that is straightforward enough to be automated. We observed that all the aforementioned functions are simple enough entities, that can be expressed through a strict syntax using first order logic. Similar to [22] , we apply a generate-and-test synthesis procedure to enumerate a space of possible expressions representing them. This process is explained in Section 4.",
            "cite_spans": [
                {
                    "start": 341,
                    "end": 345,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Some notations"
        },
        {
            "text": "In this section we develop the formal foundations of our approach for extracting recurrence relations describing the time complexity of an imperative program based on state squeezers. We present the ingredients that underly the approach, the conditions they are required to satisfy, and the recurrence relations they induce. In the next section, we explain how to extract the recurrences automatically. Given the recurrence relation, a dedicated (external) tool may be applied to end up with a closed formula, similar to [3] .",
            "cite_spans": [
                {
                    "start": 521,
                    "end": 524,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Complexity Analysis based on Squeezers"
        },
        {
            "text": "We use transition systems to capture the semantics of a program.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Complexity Analysis based on Squeezers"
        },
        {
            "text": "Definition 1 (Transition Systems). A transition system is a tuple (\u03a3, init, tr ), where \u03a3 is a set of states, init \u2286 \u03a3 is a set of initial states and tr : \u03a3 \u2192 \u03a3 is a transition function (rather than a transition relation, since only deterministic procedures are considered). The set of terminal states F \u2286 \u03a3 is implicitly defined by tr (\u03c3) = \u03c3. An execution trace (or a trace in short) is a finite or infinite sequence of states \u03c4 = \u03c3 0 , \u03c3 1 , . . .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Complexity Analysis based on Squeezers"
        },
        {
            "text": "The trace is initial if it starts from an initial state, i.e., \u03c3 \u2208 init. Unless explicitly stated otherwise, all traces we consider are initial. The set of reachable states",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Complexity Analysis based on Squeezers"
        },
        {
            "text": "Roughly, to represent a program by a transition system, we translate it into a single loop program, where init consists of the states encountered when entering the loop, and transitions correspond to iterations of the loop.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Complexity Analysis based on Squeezers"
        },
        {
            "text": "In the sequel, we fix a transition system (\u03a3, init, tr ) with a set F of terminal states and a set reach of reachable states. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Complexity Analysis based on Squeezers"
        },
        {
            "text": "The complexity function of the program maps each initial state \u03c3 0 \u2208 init to its time",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Complexity Analysis based on Squeezers"
        },
        {
            "text": "Our complexity analysis derives a recurrence relation for the complexity function by expressing the length of a trace in terms of the lengths of traces that start from lower rank states. This is achieved by (i) attaching to each initial state a rank from a wellfounded set that we use as the argument of the complexity function and that we recur over, and (ii) defining a squeezer that maps each state from the original trace to a state in a lower-rank trace; the mapping forms a partitioned simulation according to a partition function that decomposes a trace to segments; each segment is simulated by a (separate) lower-rank trace, allowing to express the length of the former in terms of the latter, and finally, (iii) defining a rank bounding function that expresses (an upper bound on) the ranks of the lower-rank traces in terms of the rank of the higher-rank trace. We elaborate on these components next.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Complexity Analysis based on Squeezers"
        },
        {
            "text": "We start by defining a rank function that allows us to express the time complexity of an initial state by means of its rank.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Time complexity as a function of rank"
        },
        {
            "text": ". Let X be a set, and \u227a be a well-founded partial order over X. Let B \u2287 min(X) be a base for X, where min(X) is the set of all the minimal elements of X w.r.t. \u227a. A rank function r : init \u2192 X maps each initial state to a rank in X. We extend the notion of a rank to initial traces as follows. Given an initial trace \u03c4 = \u03c4 (\u03c3 0 ), we define its rank to be the rank of \u03c3 0 . We refer to states \u03c3 0 such that r(\u03c3 0 ) \u2208 B as the base states. Similarly, (initial) traces whose ranks are in B are called base traces.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 3 (Rank)"
        },
        {
            "text": "In our analysis, ranks range over X = N m (for some m \u2208 N + ), with \u227a defined by the lexicographic order. Ranks let us abstract away data inside the initial execution states which does not affect the worst-case bound on the trace length. For example, the length of traces of the binary counter program (Fig. 1) is completely agnostic to the actual content of the array at the initial state. The only parameter that affects its trace length is the array size, and not which integers are stored inside it. Hence, a suitable rank function in this example maps an initial state to its array length. This is despite the fact that the execution does depend on the content of the array, and, in particular, the number of remaining iterations from an intermediate state within the execution depends on it. The partial order \u227a and the base set B will be used to define the recurrence formula as we explain in the sequel.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 302,
                    "end": 310,
                    "text": "(Fig. 1)",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Definition 3 (Rank)"
        },
        {
            "text": "We will assume from now on that (X, \u227a, B), as well as the rank function, are fixed, and can be understood from context. The rank function r induces a complexity function comp x : X \u2192 N \u222a {\u221e} over ranks, defined as follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 3 (Rank)"
        },
        {
            "text": "Definition 4 (Complexity over ranks). The complexity function over ranks, comp x :",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 3 (Rank)"
        },
        {
            "text": "The definition ensures that for every initial state \u03c3 0 \u2208 init, we can compute (an upper bound on) its time complexity based on its rank, as follows: comp s (\u03c3 0 ) \u2264 comp x (r(\u03c3 0 )). The complexity of \u03c1 takes into account all states with r (\u03c3) \u03c1 and not only those with rank exactly \u03c1, to ensure monotonicity of comp x in the rank (i.e., if \u03c1 1 \u03c1 2 then comp x (\u03c1 1 ) \u2264 comp x (\u03c1 2 )). Our approach is targeted at extracting a recurrence relation for comp x .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 3 (Rank)"
        },
        {
            "text": "In order to express the length of a trace in terms of the lengths of traces of lower ranks, we use a squeezer that maps states from the original trace to states of lower-rank traces and (implicitly) induces a correspondence between the original trace and the lower-rank trace(s). For now, we do not require the squeezer to decrease the rank of the trace; this requirement will be added later. The squeezer is accompanied by a partition function to form a partitioned simulation that allows a single higher-rank trace to be matched to multiple lower-rank traces such that their lengths may be correlated.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Complexity decomposition by partitioned simulation"
        },
        {
            "text": "The partition function partitions a trace into a bounded number of segments, where each segment consists of states with the same value of p d . We refer to the first state of a segment as a switch state, and to the last state of a finite segment as a last state (note that if \u03c4 is infinite, its last segment has no last state). In particular, this means that the initial state of a trace is a switch state. (Note that a state may be a switch state in one trace but not in another, while a last state is a last state in any trace, as long as the same partition function is considered.)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 5 (Squeezer, ). A squeezer is a function"
        },
        {
            "text": "Our complexity analysis requires the squeezer to form a partitioned simulation with respect to p d . Roughly, this means that the squeezer maps each segment of a trace to a (lower-rank) trace that \"simulates\" it. To this end, we require all the states \u03c3 within a segment of a trace to be (h, )-\"stuttering\", for some h \u2265 \u2265 1. Stuttering lets h consecutive transitions of \u03c3 be matched to consecutive transitions of its squeezed counterpart. If h = , the state \u03c3 contributes to the complexity the same number of steps as the squeezed state. Otherwise, \u03c3 contributes h \u2212 additional steps, resulting in a longer trace. Recall that terminal states also have outgoing transitions (to themselves), however these transitions do not capture actual steps; they do not contribute to the complexity. Hence, stuttering also requires that \"real\" transitions of \u03c3 are matched to \"real\" transitions of its squeezed counterpart, namely, if the latter encounter a terminal state, so must the former. For the last states of segments the requirement is slightly different as the simulation ends at the last state, and a new simulation begins in the next segment. In order to account for the transition from the last state of one segment to the first (switch) state of the next segment, last states are considered (2, 1)-stuttering if they are squeezed into terminal states, unless they are terminal themselves 4 . In any other case, they are considered (1, 1)-stuttering. The formal definitions follow.",
            "cite_spans": [
                {
                    "start": 1390,
                    "end": 1391,
                    "text": "4",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Definition 5 (Squeezer, ). A squeezer is a function"
        },
        {
            "text": "To obtain a partitioned simulation, switch states (along any trace), which start new segments, are further required to be squeezed into initial states (since our complexity analysis only applies to initial states). We denote by S p d (\u03c4 ) the switch states of trace \u03c4 according to partition p d and by S p d the switch states of all traces according to the partition p d . Namely, S p d = init \u222a tr (\u03c3) \u03c3 \u2208 reach \u2227 p d (\u03c3) < p d tr (\u03c3) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 7 (Stuttering States). A non-last state"
        },
        {
            "text": "if for every reachable state \u03c3 we have that:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 8 (Partitioned Simulation). We say that a squeezer"
        },
        {
            "text": "Note that Definition 7 implies that a non-terminal state may only be squeezed into a terminal state if it is the last state in its segments. When {(h i , i )} n i=1 is irrelevant or clear from the context, we omit it from the notation and simply write \u223c PS p d . 4 Considering a non-terminal last state that is squeezed into a terminal state as (1, 0)-stuttering may have been more intuitive than (2, 1)-stuttering, but both properly capture the discrepancy between the number of transitions in the higher and lower rank traces, and (2, 1) better fits the rest of the technical development, which assumes that h i, i \u2265 1.",
            "cite_spans": [
                {
                    "start": 263,
                    "end": 264,
                    "text": "4",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Definition 8 (Partitioned Simulation). We say that a squeezer"
        },
        {
            "text": "may have an unbounded number of (h i , i )-stuttering states, which hinders the ability to define a recurrence relation based on the simulation. To overcome this, our complexity decomposition may use k \u2265 1 to capture a common multiplicative factor of all the stuttering pairs, with the target of leaving only a bounded number of states whose stuttering exceeds k and needs to be added separately. This will become important in Theorem 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "A trace squeezed by"
        },
        {
            "text": ". . , n} be the set of indices such that hi i > k. Then for every \u03c3 0 \u2208 init we have that",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Observation 1 (Complexity decomposition)"
        },
        {
            "text": "In the observation, the first addend summarizes the complexity contributed by all the lower-rank traces, while using k as an upper bound on the \"inflation\" of the traces. However, the states that are (h i , i )-stuttering with hi i that exceeds k contribute additional h i \u2212 ( i \u00b7 k) steps to the complexity, and as a result, need to be taken into account separately. This is handled by the second addend, which adds the steps that were not accounted for by the first addend. While we use the same inflation factor k across the entire trace, a simple extension of the decomposition property may consider a different factor k in each segment. Note that the first addend always sums over a finite number of elements since the number of switch states is at most d -the number of segments. If \u03c4 (\u03c3 0 ) is finite, the second addend also sums over a finite number of elements.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Observation 1 (Complexity decomposition)"
        },
        {
            "text": "Observation 1 considers the complexity function over states, and is oblivious to the rank. In particular, it does not rely on the squeezer decreasing the rank of states. Next, we use this observation as the basis for extracting a recurrence relation for the complexity function over ranks, in which case, decreasing the rank becomes important.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Observation 1 (Complexity decomposition)"
        },
        {
            "text": "Based on the complexity decomposition, we define recurrence relations that capture comp x -the time complexity of the initial states as a function of their ranks. To go from the complexity as a function of the actual states (as in Observation 1) to the complexity as a function of their ranks, we need to express the rank of (\u03c3 s ) for a switch state \u03c3 s as a function of the rank of \u03c3 0 . To this end, we define\u02c6 : Definition 9. Given r, and p d such that \u223c PS p d , a function\u02c6 : X \u00d7{1, . . . , d} \u2192 X is a rank bounding function if for every \u03c1 \u2208 X \u2212 B and 1 \u2264 i \u2264 d, if \u03c4 (\u03c3 0 ) is an initial trace such that r(\u03c3 0 ) = \u03c1, and \u03c3 s \u2208 S p d (\u03c4 (\u03c3 0 )) is a switch state such that p d (\u03c3 s ) = i, the following holds: In other words, Definition 9 requires that for every non-base initial state \u03c3 0 \u2208 init and switch state \u03c3 s at segment i of \u03c4 (\u03c3 0 ), we have that r( (\u03c3 s )) \u02c6 (r(\u03c3 0 ), i) \u227a r(\u03c3 0 ). Recall that r( (\u03c3 s )) is well defined since (\u03c3 s ) is required to be an initial state. The definition states that\u02c6 (\u03c1, i) provides an upper bound on the rank of squeezed switch states in a non-base trace of rank \u03c1. comp x (r( (\u03c3))) \u2264 comp x (\u02c6 (\u03c1, i)) is ensured by the monotonicity of comp x . This definition also requires the rank of non-base traces to strictly decrease when they are squeezed, as captured by the \"rank decrease\" inequality.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extraction of recurrence relations over ranks"
        },
        {
            "text": "Obtaining a rank bounding function, or even verifying that a given\u02c6 satisfies this requirement, is a challenging task. We return to this question later in this section.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extraction of recurrence relations over ranks"
        },
        {
            "text": "These conditions allow to substitute the states for ranks in the first addend of Observation 1, and hence obtain recurrence relations for comp x over the (decreasing) ranks. To handle the second addend, we also need to bound the number of states whose stuttering, hi i , exceeds k. This is summarized by the following theorem: Theorem 1. Let r : init \u2192 X be a rank function,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extraction of recurrence relations over ranks"
        },
        {
            "text": ": \u03a3 \u2192 \u03a3 a squeezer and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extraction of recurrence relations over ranks"
        },
        {
            "text": ". . , d} \u2192 X be a rank bounding function w.r.t. r, and p d . If, for some k \u2265 1, the number of (h i , i )-stuttering states that appear along any non-base initial trace is bounded by a constant b i \u2208 N whenever i \u2208 E k , then",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extraction of recurrence relations over ranks"
        },
        {
            "text": "Note that a state may be (h i , i )-stuttering for several i's, in which case, it is sound to count it towards any of the b i 's; in particular, we choose the one that minimizes h i \u2212 i \u00b7 k.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Extraction of recurrence relations over ranks"
        },
        {
            "text": "for every \u03c1 \u2208 X \u2212 B, and comp x (\u03c1) \u2264 f (\u03c1) for every \u03c1 \u2208 B, then comp x (\u03c1) \u2264 f (\u03c1) for every \u03c1 \u2208 X. We conclude that comp s (\u03c3 0 ) \u2264 f (r(\u03c3 0 )) for every \u03c3 0 \u2208 init.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Corollary 1. Under the premises of Theorem 1, if"
        },
        {
            "text": "Base-case complexity In order to apply Cor. 1, we need to accompany Eq. (6) with a bound on comp x (\u03c1) for the base ranks, \u03c1 \u2208 B. Fortunately, this is usually a significantly easier task. In particular, the running time of the base cases is often constant, because intuitively, the following are correlated: (a) the rank, (b) the size of the underlying data structure, and (c) the number of iterations. In this case, symbolic execution may be used to obtain bounds for base cases (as we do in our work). In essence, any method that can yield a closed-form expression for the complexity of the base cases is viable. In particular, we can apply our technique on the base case as a subproblem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Corollary 1. Under the premises of Theorem 1, if"
        },
        {
            "text": "Theorem 1 defines a recurrence relation from which an upper bound on the complexity function, comp x , can be computed (Cor. 1). However, to ensure correctness, the premises of Theorem 1 must be verified. The requirement that \u223c PS p d ({(h i , i )} n i=1 ) (see Definition 8) may be verified locally by examining individual (reachable) states: for any (reachable) state \u03c3, the check for (h i , i )-stuttering and switch states can, and should, be done in tandem, and require only observing at most max i h i transition steps from \u03c3 and max i i from (\u03c3). In contrast, the property required of\u02c6 is global: it re-quires\u02c6 (\u03c1, i) to provide an upper bound on the rank of any squeezed switch state that may occur in any position along any non-base initial trace whose initial state has rank \u03c1. Similarly, the property required of the bounds b i is also global: that the number of (h i , i )-stuttering states along any non-base initial trace is at most b i . It is therefore not clear how these requirements may be verified in general. We overcome this difficulty by imposing additional restrictions, as we discuss next.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Establishing the requirements of the recurrence relations extraction"
        },
        {
            "text": "Establishing bounds on the number of occurrences of stuttering states Bounds on the number of occurrences per trace that are sound for every trace are difficult to obtain in general. While clever analysis methods exist that can do this kind of accounting, we found that a stronger, simpler condition applies in many cases:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Establishing the requirements of the recurrence relations extraction"
        },
        {
            "text": "-For every \u03c3 \u2208 reach, either:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Establishing the requirements of the recurrence relations extraction"
        },
        {
            "text": ", and either \u03c3 is a switch state or tr hi (\u03c3) is a last state.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Establishing the requirements of the recurrence relations extraction"
        },
        {
            "text": "This restricts these cases to occur only at the beginnings and ends of segments. It implies a total bound of 2d\u00b7 max i (h i \u2212 i \u00b7 k) on the \"surplus\" of any trace, therefore, we substitute this expression for the rightmost sum in Eq. (6) .",
            "cite_spans": [
                {
                    "start": 234,
                    "end": 237,
                    "text": "(6)",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Establishing the requirements of the recurrence relations extraction"
        },
        {
            "text": "Validating a rank bounding function The definition of a rank bounding function (Definition 9) encapsulates two parts. Part (ii) ensures that the rank decreases:\u02c6 (\u03c1, i) \u227a \u03c1 for every \u03c1 \u2208 X \u2212 B. Verifying that this requirement holds does not involve any reasoning about the states, nor traces, of the transition system. Part (i) ensures that\u02c6 provides an upper bound on the rank of squeezed switch states. Formally, it requires that r( (\u03c3 s )) \u02c6 (r(\u03c3 0 ), i) for every switch state \u03c3 s in segment i \u2208 {1, . . . , d} along a trace that starts from a non-base initial state \u03c3 0 . Namely, it relates the rank of the squeezed switch state, (\u03c3 s ), to the rank of the initial state, \u03c3 0 , where no bound on the length of the trace between the initial state \u03c3 0 and the switch state \u03c3 s is known a priori. As such, it involves global reasoning about traces. We identify two cases in which such reasoning may be avoided: (i) The partition p d consists of a single segment (i.e., d = 1); or (ii) The rank function extends to any state (and not just the initial states), while being preserved by tr . In both of these cases, we are able to verify the correctness of\u02c6 locally.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Establishing the requirements of the recurrence relations extraction"
        },
        {
            "text": "A single segment. In this case, the only switch state along a trace is the initial state, and hence the upper-bound requirement of\u02c6 boils down to the requirement that for every \u03c3 0 \u2208 init such that r(\u03c3 0 ) \u2208 X \u2212 B, we have that r( (\u03c3 0 )) \u02c6 (r(\u03c3 0 ), 1). Rank preservation. Another case in which the upper-bound property of\u02c6 may be verified locally is when the r can be extended to all states while being preserved by tr : Definition 10. A functionr : \u03a3 \u2192 X extends the rank function r : init \u2192 \u03a3 ifr agrees with r on the initial states, i.e.,r(\u03c3 0 ) = r(\u03c3 0 ) for every initial state \u03c3 0 \u2208 init. The extended rank functionr is preserved by tr , if for every reachable state \u03c3, we have thatr(tr (\u03c3)) =r(\u03c3).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Establishing the requirements of the recurrence relations extraction"
        },
        {
            "text": "Preservation ofr by tr ensures that all states along a (reachable) trace share the same rank. In particular, for a reachable switch state \u03c3 s that lies along \u03c4 (\u03c3 0 ), rank preservation ensures thatr(\u03c3 s ) =r(\u03c3 0 ) = r(\u03c3 0 ) (the last equality is due to the extension property), allowing us to recover the rank of \u03c3 0 from the rank of \u03c3 s . Therefore, the upper-bound requirement of\u02c6 simplifies into the local requirement that for every reachable switch state \u03c3 s such thatr(\u03c3 s ) \u2208 X \u2212 B, we have thatr( (\u03c3 s )) \u02c6 (r(\u03c3 s ), i), for every i \u2208 {1, . . . , d}. Remark 1. The notion of a partitioned simulation requires a switch state \u03c3 s to be squeezed into an initial state. This requirement may be relaxed into the requirement that \u03c3 s is squeezed into a reachable state (\u03c3 s ), provided that we are able to still ensure that the rank of (some) initial state \u03c3 0 leading to (\u03c3 s ) is smaller than the rank of the trace on which \u03c3 s lies, and that the rank of \u03c3 0 is properly captured by\u02c6 . One case in which this is possible, is when r is extended tor that is preserved by tr , as in this cas\u00ea r( (\u03c3 s )) =r(\u03c3 0 ) = r(\u03c3 0 ). This subsection described local properties that ensure that a given program satisfies the requirements of Theorem 1. The locality of the properties facilitates the use of SMT solvers to perform these checks automatically. This is a key step for effective application of the method.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Establishing the requirements of the recurrence relations extraction"
        },
        {
            "text": "A plethora of work exists for analyzing the complexity of programs (see Section 6 for a discussion of related works). Most existing techniques for automatic complexity analysis aim to find a recurrence relation on the length of the execution trace, relating the length of a trace from some state to the length of the remaining trace starting at its successor. These are recurrences on time, if you will, whereas our approach generates recurrences on the state size (captured by the rank). Is our approach completely orthogonal to preceding methods? Not quite. It turns out that from a conceptual point of view, our approach can formulate a recurrence on time as well, as we demonstrate in this section.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Trace-length vs. state-size recurrences with squeezers"
        },
        {
            "text": "The key idea is to use tr itself as a squeezer that squeezes each state into its immediate successor. Putting aside the initial-anchor requirement momentarily, such a squeezer forms a partitioned simulation with a single segment (i.e., p d \u2261 1), in which all the states along a trace are (1, 1)-stuttering, except for the last one (if the trace is finite), which is (2, 1)-stuttering. Recall that squeezers must also preserve initial states (see Definition 8), a property that may be violated when = tr , as the successor of an initial state is not necessarily an initial state. We restore the initial-anchor property by setting init = \u03a3, i.e., every state is considered an initial state 5 .",
            "cite_spans": [
                {
                    "start": 688,
                    "end": 689,
                    "text": "5",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Obtaining trace-length recurrences based on state squeezers"
        },
        {
            "text": "A consequence of this definition is that comp x will now provide an upper bound on the time complexity of every state and not only of the initial states, in terms of a rank that needs to be defined. If we further define a rank-bounding function\u02c6 we may extract a recurrence relation of the form comp x (\u03c1) = comp x (\u02c6 (\u03c1)) + 1 (we use\u02c6 (\u03c1) as an abbreviation of\u02c6 (\u03c1, 1), since this is a special case where d = 1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Obtaining trace-length recurrences based on state squeezers"
        },
        {
            "text": "Defining the rank and the rank bounding function Recall that the rank r : \u03a3 \u2192 X captures the features of the (initial) states that determine the complexity. To allow maximal precision, especially since all states are now initial, we set X to be the set of states \u03a3, and define r to be the identity function, r (\u03c3) = \u03c3. With this definition, comp x and comp s become one. Next, we need to define \u227a and B, while ensuring that squeezes the (non-base) initial states, which are now all the states, into states of a lower rank according to \u227a. Since squeezers act like transitions now, having that = tr , they have the effect of decreasing the number of transitions remaining to reach a terminal state (provided that the trace is finite). We use this observation to define \u227a \u2286 \u03a3 \u00d7 \u03a3. Care is needed to ensure that (\u03a3, \u227a) is well-founded, i.e., every descending chain is finite, even though the program may not terminate. Here is the definition that achieves this goal:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Obtaining trace-length recurrences based on state squeezers"
        },
        {
            "text": "Since = tr does not decrease comp s for states that belong to infinite (nonterminating) traces (comp s ( (\u03c3)) = comp s (\u03c3) = \u221e, hence (\u03c3) \u227a \u03c3), they must be included in B, together with the terminal states, which are minimal w.r.t. \u227a. Namely, B = F \u222a {\u03c3 | comp s (\u03c3) = \u221e}. Technically, this means that the base of the recurrence needs to define comp x for these states.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Obtaining trace-length recurrences based on state squeezers"
        },
        {
            "text": "The final piece in the puzzle is setting\u02c6 = tr . Since \u223c PS p d {(1, 1), (2, 1)} (when init = \u03a3), where the number of (2, 1)-stuttering states that appear along any non-base initial trace is bounded by 1, we may use Theorem 1, setting k = 1, to derive the following recurrence relation, which reflects induction over time: comp x (\u03c3) = comp x (tr (\u03c3)) + 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Obtaining trace-length recurrences based on state squeezers"
        },
        {
            "text": "The formulation above represents a degenerate, na\u00efve, choice of ingredients for the sake of a theoretical construction, whose purpose is to lay the foundation for a general framework that takes its strengths from both induction over time and induction over rank. This construction does not exploit the full flexibility of our framework. In particular, ranking functions obtained from termination proofs, as used in [5] , may be used to augment the rank in this setting. Further, invariants inferred from static analysis can be used to refine the recurrences.",
            "cite_spans": [
                {
                    "start": 415,
                    "end": 418,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [],
            "section": "Obtaining trace-length recurrences based on state squeezers"
        },
        {
            "text": "So far we have assumed that the rank function r, partition function p d , squeezer and a rank bounding function\u02c6 are all readily available. Clearly, they are specific to a given program. It would be too tedious for a programmer to provide these functions for the analysis of the underlying complexity. In this section we show how to automate the process of obtaining (r , p d , ,\u02c6 ) for a class of typical looping programs. We take advantage of the fact that these components are much more compact than other kinds of auxiliary functions commonly used for resource analysis, such as monotonically decreasing measures used as ranking functions. For example, a ranking function for the binary counter program shown in Fig. 1 is: This enables the use of a relatively na\u00efve enumerative approach of multi-phase generateand-test, employing some early pruning to discard obviously non-qualifying candidates.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 716,
                    "end": 722,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Synthesis"
        },
        {
            "text": "The generation step of the synthesis loop applies syntax guided synthesis (SyGuS [7] ). Like any other SyGuS method, defining the underlying grammars is more art than science. It should be expressive enough to capture the desired terms, but strict enough to effectively bound the search space. Ranks are taken from N m where m \u2208 {1, 2, 3} and \u227a is the usual lexicographic order. The rank function r comprises of one expression for each coordinate, constructed by adding / subtracting integer variables and array sizes. Boolean variables are not used in rank expressions.",
            "cite_spans": [
                {
                    "start": 81,
                    "end": 84,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "SyGuS"
        },
        {
            "text": "Partition functions p d . Our implementation currently supports a maximum number of two segments. This means that the partition function only assigns the values 1 and 2, and we synthesize it by generating a condition over the program's variables, cond , that selects between them: p d (\u03c3) = cond (\u03c3) ? 2 : 1. Handling up to two segments is not an inherent limitation, but we found that for typically occurring programs, two segments are sufficient.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SyGuS"
        },
        {
            "text": "Squeezers are the only ingredient that requires substantial synthesis effort. We represent squeezers as small loop-free imperative programs, which are natural for representing state transformations. We use a rather standard syntax with 'if-then-else' and assignments, plus a remove-adjust operation that removes array entries and adjusts indices relating to them accordingly. .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SyGuS"
        },
        {
            "text": "Rank bounding functions\u02c6 . With a well-chosen squeezer , it suffices to consider quite simple rank bounds for the mini-traces. Hence, the rank-bounds defined by\u02c6 are obtained by adding, subtracting and multiplying variables with small constants (for each coordinate of the rank). Similar to the choice of ranks, targeting simple expressions for helps reduce the complexity of the final recurrence that is generated from the process.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "SyGuS"
        },
        {
            "text": "For the sake of verifying the synthesized ingredients, we fix a set {h i , i } of stuttering shapes, and check the requirements of Theorem 1 as discussed in Section 3.4. In particular, we check that p d is weakly monotone, i.e., that cond cannot change from true to false in any step of tr . Note that some of the properties may be used to discriminate some of the ingredients independent of the others. For example, the simulation requirement only depends on and p d .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Verification"
        },
        {
            "text": "Unbounded verification Once candidates pass a preliminary screening phase, they are verified by encoding the program and all the components r , p d , ,\u02c6 as first-order logic expressions, and using an SMT solver (Z3 [13] ) to verify that the requirements are fulfilled for all traces of the program. As mentioned in Section 3.4, all the checks are local and require observing a bounded set of steps starting from a given \u03c3. The only facet of the criteria that is difficult to encode is the fact they are required of the reachable states (and not any state). Of course, if we are able to ascertain that these are met for all \u03c3 \u2208 \u03a3, including unreachable states, then the result is sound. However, for some programs and squeezers, the required properties (esp., simulation) do not hold universally, but are violated by unreachable states. To cope with this situation without having to manually provide invariants that capture properties of the reachable states, we use a CHC solver, Spacer [23] , which is part of Z3, to check whether all the reachable states in the unbounded-state system induced by the input program satisfy these properties. This can be seen as a reduction from the problem of verifying the premises of Theorem 1 to that of verifying a safety property.",
            "cite_spans": [
                {
                    "start": 215,
                    "end": 219,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 987,
                    "end": 991,
                    "text": "[23]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Verification"
        },
        {
            "text": "We implemented our complexity analyzer as a publicly available tool, SqzComp, that receives a program in a subset of C and produces recurrence relations. SqzComp is written in C++, using the Z3 C++ API [13] , and using Spacer [23] via its SMTLIB2compatible interface. Since our squeezers may remove elements from arrays, we initially encoded arrays as SMT sequences. However, we found that it is beneficial to restrict squeezers to only remove the first or last elements of an array, resulting in a more efficient encoding with the theory of arrays. For the base case of generated recurrences, we use the symbolic execution engine KLEE [11] to bound the total number of iterations by a constant.",
            "cite_spans": [
                {
                    "start": 202,
                    "end": 206,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 226,
                    "end": 230,
                    "text": "[23]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 636,
                    "end": 640,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [],
            "section": "Empirical Evaluation"
        },
        {
            "text": "We evaluated our tool, SqzComp, on a variety of benchmark programs taken from [16] , as well as three additional programs: the binary counter example from Section 2, a subsets example, described in Section 5.2, and an example computing monotone sequences. These examples exhibit intricate time complexities. From the benchmark suite of [16] we filtered out non-deterministic programs, as well as programs that failed syntactic constraints that our frontend cannot currently handle. We compared SqzComp to CoFloCo [16] -the state of the art tool for complexity analysis of imperative programs. Table 1 summarizes the results of our experiments. The first column presents the name of the program, which describes its characteristics (each of the \"two-phase loop\" programs consists of a loop with an if statement, where the branch executed changes starting from some iteration). The second column specifies the real complexity, while the following two columns present the bounds inferred by SqzComp CoFloCo's analysis time is always in the order of magnitude of 0.1 second, whether it succeeds to find a complexity bound or not. Our analysis is considerably slower, mostly due to the na\u00efve implementation of the synthesizer. When both CoFloCo and SqzComp succeed, the bounds inferred by CoFloCo are sometimes tighter.",
            "cite_spans": [
                {
                    "start": 78,
                    "end": 82,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 336,
                    "end": 340,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 513,
                    "end": 517,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 988,
                    "end": 995,
                    "text": "SqzComp",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 593,
                    "end": 600,
                    "text": "Table 1",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Experiments"
        },
        {
            "text": "However, SqzComp manages to find tight complexity bounds for the new examples, which are not solved by CoFloCo, and to the best of our knowledge, are beyond reach of existing tools. (We also encoded the new examples as OCaml programs and ran the tool of [20] on them, and it failed to infer bounds.)",
            "cite_spans": [
                {
                    "start": 254,
                    "end": 258,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "This subsection presents one challenging example from our benchmarks, the subsets example, and the details of its complexity analysis. Notably, our method is able to infer a binomial bound, which is asymptotically tight.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case study: Subsets example"
        },
        {
            "text": "The code, shown in Fig. 4 , iterates over all the subsets of {m,...,n-1} of size k. The \"current\" subset is maintained in an array I whose length is k, and which is always sorted, thus avoiding generating the same set more than once. The first k iterations of the loop fill the array with values {m,m+1,...,m+k-1}, which represent the first subset generated. This is taken care of by the branches at lines 5, 6 that perform a \"right fill\" phase, filling in the array with an ascending sequence starting from m at I[0]. Once the first k iterations are done, j reaches the end of the array (j=k) and so the next iteration will execute line 4, turning off the flag f, signifying that the array should now be scanned leftwards. In each successive iteration, j is decreased, looking for the rightmost element that can be incremented. For example, if n = 8, I = [2, 6, 7] , this rightmost element is",
            "cite_spans": [
                {
                    "start": 856,
                    "end": 859,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 860,
                    "end": 862,
                    "text": "6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 863,
                    "end": 865,
                    "text": "7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 19,
                    "end": 25,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Case study: Subsets example"
        },
        {
            "text": "After that element is incremented, the flag f is turned on again, completing the \"left scan\" phase and starting a \"right fill\" phase. A univariate recurrence Consider the rank function r(I, n, k, m, j, f ) = n \u2212 m, defined with respect to (N, <), and the squeezer shown below the program in Fig. 4 . The squeezer observes the first element of the array: if it is equal to m (the lower bound of the range), it removes it from the array, shrinking its size (k) by one. It then adjusts the index j to keep pointing to the same element; unless j = 0, in which case that element is removed. This squeezer forms a 2-partitioned simulation, as illustrated by the traces in Fig. 5 . All states are (1, 1)-stuttering, except for \u03c3 0 , which is (2, 1)-stuttering, as caused by the removal of I[0] when j = 0. The rank bounding function is\u02c6 (i, \u03c1) = \u03c1 \u2212 1 for i \u2208 {1, 2}. We therefore obtain the following recurrence relation:",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 291,
                    "end": 297,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 666,
                    "end": 672,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Case study: Subsets example"
        },
        {
            "text": "The base of the recurrence is comp x (0) = 1, leading to the solution comp x (\u03c1) \u2264 2 \u03c1+1 \u2212 1. This means that for an initial state, comp s (I, n, k, m, 0, true) \u2264 comp x (n \u2212 m) \u2264 2 n\u2212m+1 \u2212 1.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case study: Subsets example"
        },
        {
            "text": "A multivariate recurrence Consider an alternative rank definition r(I, n, k, m, j, f ) = (n \u2212 m, k) defined with respect to (N \u00d7 N, <), where '<' denotes the lexicographic order, together with the same squeezer and partition as before. The rank bounding func-",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case study: Subsets example"
        },
        {
            "text": "The corresponding recurrence relation is:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case study: Subsets example"
        },
        {
            "text": "with base comp x (0, _) = 1, resulting in the solution comp x (\u03c1 1 , \u03c1 2 ) \u2264 \u03c11+2 \u03c12 . That is, for an initial state, comp s (I, n, k, m, 0, true) \u2264 comp x (n \u2212 m, k) \u2264 n\u2212m+2 k . Interestingly, this example demonstrates that the same squeezer may yield different recurrences, when different ranks (and rank bounding functions) are considered. It also demonstrates a case where different segments of a trace are mapped to mini-traces of a different rank.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Case study: Subsets example"
        },
        {
            "text": "This section focuses on exploring existing methods for static complexity analysis of imperative programs. Dynamic profiling and analysis [26] are a separate research area, more related to testing, and generally do not provide formal guarantees. We further focus on works that determine asymptotic complexity bounds, and use the number of iterations executed as their cost model; we refrain from thoroughly covering previous techniques that analyze complexity at the instruction level.",
            "cite_spans": [
                {
                    "start": 137,
                    "end": 141,
                    "text": "[26]",
                    "ref_id": "BIBREF26"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Static cost analysis The seminal work of [28] defined a two steps meta-framework where recurrence relations are extracted from the underlying program, and then analyzed to provide closed-form upper bounds. Broadly speaking, cost relations are a generalized framework that captures the essence of most of the works mentioned in this section.",
            "cite_spans": [
                {
                    "start": 41,
                    "end": 45,
                    "text": "[28]",
                    "ref_id": "BIBREF28"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "[4] and [16] infer cost relations of imperative programs written in Java and C respectively. Cost relations resemble somewhat limited C procedures: They are capable of recursive calls to other cost relations, and they can handle non-determinism that arises either as a consequence of direct nondet() in the program, or as a result of inherent imprecision of static analysis. They define for every basic block of the program its own cost relation function, and then form chains according to the control flow graph of the program. They use numerical abstract domains to support a context sensitive analysis of whether a chain of visits to specific basic blocks is feasible or not. Once all infeasible chains are removed, disjunctive analysis determines an overall approximation of the heaviest chain, representing the max number of iterations. [19] uses multiple counter instrumentation that are automatically inserted in various points in the code, initialized and incremented. These ghost counters enable to infer an overall complexity bound by applying appropriate abstract interpretation handling numeric domains. [18] and [17] apply code transformations to represent multi-path loops and nested loops in a canonical way. Then, paths connecting pairs of \"interesting\" code points \u03c0 1 , \u03c0 2 (loop headers etc.) are identified, in a way that satisfies some properties. For instance, \u03c0 1 is reached twice without reaching \u03c0 2 . The path property induces progress invariants, which are then analyzed to infer the overall complexity bound. [24] define an abstraction of the program to a size-change-graph, where transition edges of the control flow graph are annotated to capture sound over-approximation relations between integer variables. The graph is then searched for infinitely decreasing sequences, represented as words in an \u03c9-regular language. This representation concisely characterizes program termination. [29] then harnesses the size-change abstraction from [24] to analyze the complexity of imperative programs. First, they apply standard program transformations like pathwise analysis to summarize inner nested loops. Then, they heuristically define a set of scalar rank functions they call norms. These norms are somewhat similar to our rank function in the sense that they help to abstract away program parts that do not effect its complexity. The program is then represented as a size-change graph, and multi-path contextualization [25] prunes subsequent transitions which are infeasible. [8] introduces difference constraints in the context of termination, to bound variables x in current iteration with some y in previous iteration plus some constant c:",
            "cite_spans": [
                {
                    "start": 8,
                    "end": 12,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 842,
                    "end": 846,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1125,
                    "end": 1129,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 1537,
                    "end": 1541,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 1915,
                    "end": 1919,
                    "text": "[29]",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1968,
                    "end": 1972,
                    "text": "[24]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 2447,
                    "end": 2451,
                    "text": "[25]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 2504,
                    "end": 2507,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "x \u2264 y + c. [27] extends difference constraints to complexity analysis. Indeed, it is quite often the case that ideas from the area of program termination are assimilated in the context of complexity analysis and vice versa. They exploit the observation that typical operations on loop counters like increment, decrement and resets are essentially expressible as difference constraints. They design an abstraction based on the domain of difference constraints, and obtain relevant invariants which are then used in determining upper bounds. [10] is very similar, only that it represents a program as an integer transition system and allows nonlinear numerical constraints and ranking functions. As we mentioned earlier, all of these approaches are based on identifying the progress of executions over time, characterizing the progress between two given points in the program. In contrast, our approach allows to reason over state size and compares whole executions.",
            "cite_spans": [
                {
                    "start": 11,
                    "end": 15,
                    "text": "[27]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 540,
                    "end": 544,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "Squeezers. The notion of squeezers was introduced by [22] for the sake of safety verification. As discussed in Section 1, the challenges in complexity analysis are different, and require additional ingredients beyond squeezers. [15, 1, 2] introduce well structured transition systems, where a well-quasi order (wqo) on the set of states induces a simulation relation. This property ensures decidability of safety verification of such systems (via a backward reachability algorithm). Our use of squeezers that decrease the rank of a state and induce a sort of a simulation relation may resemble the wqo of a well structured transition system. However, there are several key differences: we do not require the order (which is defined on ranks) to be a wqo. Further, we do not require a simulation relation between any states whose ranks are ordered, only between a state and its squeezed counterpart. Notably, our work considers complexity analysis rather than safety verification.",
            "cite_spans": [
                {
                    "start": 53,
                    "end": 57,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 228,
                    "end": 232,
                    "text": "[15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 233,
                    "end": 235,
                    "text": "1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 236,
                    "end": 238,
                    "text": "2]",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Related Work"
        },
        {
            "text": "This work introduces a novel framework for run-time complexity analysis. The framework supports derivation of recurrence relations based on inductive reasoning, where the form of induction depends on the choice of a squeezer (and rank bounding function). The new approach thus offers more flexibility than the classical methods where induction is coupled with the time dimension. For example, when the rank captures the \"state size\", the approach mimics induction over the space dimension, reasoning about whole traces, and alleviating the need to describe the intricate development of states over time. We demonstrate that such squeezers and rank bounding functions, which we manage to synthesize automatically, facilitate complexity analysis for programs that are beyond reach for existing methods. Thanks to the simplicity and compactness of these ingredients, even a rather na\u00efve enumeration was able to find them efficiently.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "General decidability theorems for infinitestate systems",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "A"
                    ],
                    "last": "Abdulla",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Cerans",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Jonsson",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Tsay",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Proceedings, 11th Annual IEEE Symposium on Logic in Computer Science",
            "volume": "",
            "issn": "",
            "pages": "313--321",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Algorithmic analysis of programs with well quasi-ordered domains",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "A"
                    ],
                    "last": "Abdulla",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Cerans",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Jonsson",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Tsay",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Inf. Comput",
            "volume": "160",
            "issn": "1-2",
            "pages": "109--127",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Automatic inference of upper bounds for recurrence relations in cost analysis",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Albert",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Arenas",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Genaim",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Puebla",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Static Analysis",
            "volume": "",
            "issn": "",
            "pages": "221--237",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "COSTA: design and implementation of a cost and termination analyzer for java bytecode",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Albert",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Arenas",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Genaim",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Puebla",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Zanardini",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Formal Methods for Components and Objects, 6th International Symposium",
            "volume": "",
            "issn": "",
            "pages": "113--132",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Resource analysis driven by (conditional) termination proofs. Theory Pract",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Albert",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Bofill",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Borralleras",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Martin-Martin",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Rubio",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Log. Program",
            "volume": "19",
            "issn": "5-6",
            "pages": "722--739",
            "other_ids": {
                "DOI": [
                    "10.1017/S1471068419000152"
                ]
            }
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "On the limits of the classical approach to cost analysis",
            "authors": [
                {
                    "first": "D",
                    "middle": [
                        "E"
                    ],
                    "last": "Alonso-Blas",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Genaim",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Static Analysis",
            "volume": "",
            "issn": "",
            "pages": "405--421",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Syntax-guided synthesis",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Alur",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Bod\u00edk",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Dallal",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Fisman",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Garg",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Juniwal",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kress-Gazit",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Madhusudan",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "M K"
                    ],
                    "last": "Martin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Raghothaman",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Saha",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Seshia",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Singh",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Solar-Lezama",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Torlak",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Udupa",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Dependable Software Systems Engineering, NATO Science for Peace and Security Series, D: Information and Communication Security",
            "volume": "40",
            "issn": "",
            "pages": "1--25",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Size-change termination with difference constraints",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Ben-Amram",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "ACM Trans. Program. Lang. Syst",
            "volume": "30",
            "issn": "3",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Templates and recurrences: Better together",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Breck",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Cyphert",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Kincaid",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Reps",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation",
            "volume": "2020",
            "issn": "",
            "pages": "688--702",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Alternating runtime and size complexity analysis of integer programs",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Brockschmidt",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Emmes",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Falke",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Fuhs",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Giesl",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Tools and Algorithms for the Construction and Analysis of Systems -20th International Conference, TACAS 2014, Held as Part of the European Joint Conferences on Theory and Practice of Software",
            "volume": "8413",
            "issn": "",
            "pages": "140--155",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Klee: Unassisted and automatic generation of highcoverage tests for complex systems programs",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Cadar",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Dunbar",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Engler",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation",
            "volume": "",
            "issn": "",
            "pages": "209--224",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Automatic discovery of linear restraints among variables of a program",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cousot",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Halbwachs",
                    "suffix": ""
                }
            ],
            "year": 1978,
            "venue": "Proceedings of the 5th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages",
            "volume": "",
            "issn": "",
            "pages": "84--96",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Z3: An efficient SMT solver",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "De Moura",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Bj\u00f8rner",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "Proceedings of the Theory and Practice of Software, 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems",
            "volume": "",
            "issn": "",
            "pages": "337--340",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Cost analysis of logic programs",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "K"
                    ],
                    "last": "Debray",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "W"
                    ],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "ACM Trans. Program. Lang. Syst",
            "volume": "15",
            "issn": "5",
            "pages": "826--875",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Well-structured transition systems everywhere! THEORETI",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Finkel",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Schnoebelen",
                    "suffix": ""
                }
            ],
            "year": 1998,
            "venue": "CAL COMPUTER SCIENCE",
            "volume": "256",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Upper and lower amortized cost bounds of programs expressed as cost relations",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Flores-Montoya",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "9995",
            "issn": "",
            "pages": "254--273",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "The reachability-bound problem",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gulwani",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Control-flow refinement and progress invariants for bound analysis",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gulwani",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Jain",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Koskinen",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation",
            "volume": "",
            "issn": "",
            "pages": "375--385",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Speed: precise and efficient static estimation of program computational complexity",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gulwani",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "K"
                    ],
                    "last": "Mehra",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "M"
                    ],
                    "last": "Chilimbi",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "127--139",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Resource aware ML",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hoffmann",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Aehlig",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hofmann",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Computer Aided Verification -24th International Conference, CAV 2012",
            "volume": "7358",
            "issn": "",
            "pages": "781--786",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Amortized resource analysis with polynomial potential: A static inference of polynomial bounds for functional programs",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Hoffmann",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hofmann",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Putting the squeeze on array programs: Loop verification via inductive rank reduction",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Ish-Shalom",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Itzhaky",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Rinetzky",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Shoham",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Verification, Model Checking, and Abstract Interpretation -21st International Conference",
            "authors": [],
            "year": 2020,
            "venue": "",
            "volume": "2020",
            "issn": "",
            "pages": "112--135",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Smt-based model checking for recursive programs",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Komuravelli",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gurfinkel",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chaki",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "The size-change principle for program termination",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "S"
                    ],
                    "last": "Lee",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "D"
                    ],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Ben-Amram",
                    "suffix": ""
                }
            ],
            "year": 2001,
            "venue": "Proceedings of the 28th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
            "volume": "",
            "issn": "",
            "pages": "81--92",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Termination analysis with calling context graphs",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Manolios",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Vroon",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "Computer Aided Verification",
            "volume": "",
            "issn": "",
            "pages": "401--414",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Combining static analysis and profiling for estimating execution times",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Mera",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "L\u00f3pez-Garc\u00eda",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Puebla",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Carro",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "V"
                    ],
                    "last": "Hermenegildo",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "International Symposium on Practical Aspects of Declarative Languages",
            "volume": "",
            "issn": "",
            "pages": "140--154",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Complexity and resource bound analysis of imperative programs using difference constraints",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sinn",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zuleger",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Veith",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "J. Autom. Reasoning",
            "volume": "59",
            "issn": "1",
            "pages": "3--45",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Mechanical program analysis",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wegbreit",
                    "suffix": ""
                }
            ],
            "year": 1975,
            "venue": "Commun. ACM",
            "volume": "18",
            "issn": "9",
            "pages": "528--539",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Bound analysis of imperative programs with the size-change abstraction",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zuleger",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Gulwani",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sinn",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Veith",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "Static Analysis",
            "volume": "",
            "issn": "",
            "pages": "280--297",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "A program that produces all combinations of n bits.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "i, c = n\u2212 1, i\u2212 1, c[1:] \u02c6 (n) = n\u2212 1x\u2212 y = max{0, x \u2212 y} Correspondence between two traces of the binary counter program. Squeezer removes the leftmost array entry, that represents the least significant bit. The rank is the array size, i.e., four on the upper trace and three on the lower one. The simulation includes only 1-,2and 3-steps, so the length of the upper trace is at most three times that of the lower trace, yielding an overall complexity bound of O(3 n ).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "An execution trace of the binary counter program that corresponds to two mini-traces of lower rank.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "(i) upper bound: r (\u03c3 s ) \u02c6 (\u03c1, i) and (ii) rank decrease:\u02c6 (\u03c1, i) \u227a \u03c1",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Let r, and p 1 : \u03a3 \u2192 {1} such that \u223c PS p1 . Then\u02c6 : X \u00d7 {1} \u2192 X satisfies the upper-bound requirement of a rank bounding function if and only if r( (\u03c3 0 )) \u02c6 (r(\u03c3 0 ), 1) for every \u03c3 0 \u2208 init such that r(\u03c3 0 ) \u2208 X \u2212 B.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Let r, and p d : \u03a3 \u2192 {1, . . . , d} such that \u223c PS p d . Suppose that r : \u03a3 \u2192 X extends r and is preserved by tr . Then\u02c6 : X \u00d7 {1, . . . , d} \u2192 X satisfies the upper-bound requirement of a rank bounding function if and only ifr( (\u03c3 s )) \u02c6 (r(\u03c3 s ), i) for every reachable switch state \u03c3 s such thatr(\u03c3 s ) \u2208 X \u2212 B and for every i \u2208 {1, . . . , d}.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "whereas the rank, partition, and\u02c6 are r (n, i, c) = n (n, i, c) = n \u2212 1, (i \u2265 n) ? i \u2212 1 : i, c[: n \u2212 1] \u02c6 (\u03c1) = \u03c1 \u2212 1 p d (n, i, c) = (i \u2265 n || c[n \u2212 1]) ? 2 : 1",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "An example program that produces all subsets of {m, . . . , n \u2212 1} of size k; below is the synthesized squeezer. output by the tool.) The fourth and fifth columns present the analysis running time, respectively the number of segments used in the analysis, of SqzComp.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "An illustration of the 2-partitioned simulation for the subsets example. In the univariate case, the rank of the upper trace is n \u2212 m and that of the lower traces is n \u2212 m \u2212 1. In the multivariate case, the upper trace is of rank (n \u2212 m, k), lower traces of ranks (n \u2212 m \u2212 1, k \u2212 1), (n \u2212 m \u2212 1, k).",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "maintains (some variant of) a simulation relation, thus ensuring a bona fide correspondence between higher-rank traces and lower-rank traces through correspondence between states. -A trace partition p d : \u03a3 \u2192 [1..d] that maps each state to a segment-identifier i \u2208 [1..d], and induces a decomposition of a trace into segments, allowing to map each of them to a separate, lower-rank mini-trace. -A rank-bounding function\u02c6 : X \u00d7[1.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Definition 2 (Complexity over states). For a state \u03c3 \u2208 \u03a3, we denote by comp s (\u03c3) the number of transitions from \u03c3 to a terminal state along \u03c4 (\u03c3) (the trace that starts from \u03c3). Formally, if \u03c4 (\u03c3) does not include a terminal state, i.e., the procedure does not terminate from \u03c3, then comp s (\u03c3) = \u221e. Otherwise:",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Experimental results. In array programs, A denotes an array.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "and by CoFloCo, respectively. (For SqzComp, the reported bounds are the solutions of the recurrences void subsets(uint n, uint k, uint m) { uint I[k]; int j = 0; bool f = true; while (j >= 0) { if (j >= k) / * start left scan * /{f=false; j--;} else if (j==0 && f) / * init * /{f=true;I[0]=m;j++;} else if (f) / * right fill * /{f=true;I[j]=I[j-1]+1;j++;} else if (I[j]>=n-k+j)/ * left scan * /{f=false; j--;} else / * start right fill * /{f=true; I[j]=I[j]+1;j++;}",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}