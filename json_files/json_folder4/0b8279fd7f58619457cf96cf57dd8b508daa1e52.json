{
    "paper_id": "0b8279fd7f58619457cf96cf57dd8b508daa1e52",
    "metadata": {
        "title": "Cough Recognition Based on Mel-Spectrogram and Convolutional Neural Network",
        "authors": [
            {
                "first": "Quan",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Anhui Province Key Laboratory of Special Heavy Load Robot",
                    "institution": "Anhui University of Technology",
                    "location": {
                        "addrLine": "Ma'anshan",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Jianhua",
                "middle": [],
                "last": "Shan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Anhui Province Key Laboratory of Special Heavy Load Robot",
                    "institution": "Anhui University of Technology",
                    "location": {
                        "addrLine": "Ma'anshan",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Wenlong",
                "middle": [],
                "last": "Ding",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Anhui Province Key Laboratory of Special Heavy Load Robot",
                    "institution": "Anhui University of Technology",
                    "location": {
                        "addrLine": "Ma'anshan",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Chengyin",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Anhui Province Key Laboratory of Special Heavy Load Robot",
                    "institution": "Anhui University of Technology",
                    "location": {
                        "addrLine": "Ma'anshan",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Shi",
                "middle": [],
                "last": "Yuan",
                "suffix": "",
                "affiliation": {
                    "laboratory": "Anhui Province Key Laboratory of Special Heavy Load Robot",
                    "institution": "Anhui University of Technology",
                    "location": {
                        "addrLine": "Ma'anshan",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Fuchun",
                "middle": [],
                "last": "Sun",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Tsinghua University",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Haiyuan",
                "middle": [],
                "last": "Li",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Beijing University of Posts and Telecommunications",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            },
            {
                "first": "Bin",
                "middle": [],
                "last": "Fang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Tsinghua University",
                    "location": {
                        "settlement": "Beijing",
                        "country": "China"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "In daily life, there are a variety of complex sound sources. It is important to effectively detect certain sounds in some situations. With the outbreak of COVID-19, it is necessary to distinguish the sound of coughing, to estimate suspected patients in the population. In this paper, we propose a method for cough recognition based on a Mel-spectrogram and a Convolutional Neural Network called the Cough Recognition Network (CRN), which can effectively distinguish cough sounds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "As a disease with a long incubation period and high infection rate, COVID-19 has caused millions of people to be infected and hundreds of thousands of people to died. How to avoid the rapid spread of the epidemic and effectively control the number of infected people has become an urgent issue. Asif et al. found that data from 10,172 COVID-19 laboratory-confirmed cases have shown a correlation with coughing in 54.08% (Sattar Hashmi and Asif, 2020) . Therefore, coughing, as a typical symptom of pneumonia, is of great significance in controlling the potential infectious source if it can be quickly and accurately monitored in the population.",
            "cite_spans": [
                {
                    "start": 420,
                    "end": 450,
                    "text": "(Sattar Hashmi and Asif, 2020)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "Many scholars have studied how to extract features of sound and recognize the sound. Mel Frequency Cepstrum Coefficient (MFCC), as a method of extracting audio features (Shintri and Bhatia, 2015) , is widely used in various audio recognition tasks. Xie et al. used MFCC to recognize abnormal voice (Xie et al., 2012) . Wang et al. proposed to recognize speech emotion based on improved MFCC (Wang and Hu, 2018) . Suksri described a method that used MFCC extracted from the speech signals of spoken words for speech recognition (Ittichaichareon et al., 2012) . The Fourier transform (FT) is also widely used in audio processing. Jozef et al. presented a new procedure for the frequency analysis of audio signals (Pucik et al., 2014) .",
            "cite_spans": [
                {
                    "start": 169,
                    "end": 195,
                    "text": "(Shintri and Bhatia, 2015)",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 249,
                    "end": 264,
                    "text": "Xie et al. used",
                    "ref_id": null
                },
                {
                    "start": 298,
                    "end": 316,
                    "text": "(Xie et al., 2012)",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 319,
                    "end": 339,
                    "text": "Wang et al. proposed",
                    "ref_id": null
                },
                {
                    "start": 391,
                    "end": 410,
                    "text": "(Wang and Hu, 2018)",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 527,
                    "end": 557,
                    "text": "(Ittichaichareon et al., 2012)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 711,
                    "end": 731,
                    "text": "(Pucik et al., 2014)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "Although these traditional methods are very effective for the extraction of audio features, considering the complexity of the real scene, the method of deep learning may achieve better results. With the development of deep learning, the neural network has played an important role in audio recognition. Oren et al. proposed spectral representations for convolutional neural networks (Rippel et al., 2015) . Some LSTM-based networks for speech recognition are also presented (Pundak and Sainath, 2017; Trianto et al., 2018) . Compared with traditional methods, deep learning can extract more complex and robust features.",
            "cite_spans": [
                {
                    "start": 383,
                    "end": 404,
                    "text": "(Rippel et al., 2015)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 474,
                    "end": 500,
                    "text": "(Pundak and Sainath, 2017;",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 501,
                    "end": 522,
                    "text": "Trianto et al., 2018)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "For cough recognition, various methods are proposed. Cough signals are usually obtained by audio or inertial sensors, which can detect the vibration caused by coughing. These sensors include a microphone that can be worn or placed near the user, or a piezoelectric transducer and a high-sensitivity accelerator that can be placed in the throat or chest area (Drugman et al., 2013; Amoh and Odame, 2016; Elfaramawy et al., 2018) .",
            "cite_spans": [
                {
                    "start": 358,
                    "end": 380,
                    "text": "(Drugman et al., 2013;",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 381,
                    "end": 402,
                    "text": "Amoh and Odame, 2016;",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 403,
                    "end": 427,
                    "text": "Elfaramawy et al., 2018)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "Infante et al. used a machine learning method to recognize dry/wet cough (Infante et al., 2017) . Semi-supervized Tree Support Vector Machine is proposed for cough recognition and detection (Hoa et al., 2011) . K-NN is also an efficient tool that is often used for cough recognition (Hoyos Barcelo et al., 2017; Vhaduri et al., 2019) .",
            "cite_spans": [
                {
                    "start": 73,
                    "end": 95,
                    "text": "(Infante et al., 2017)",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 190,
                    "end": 208,
                    "text": "(Hoa et al., 2011)",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 283,
                    "end": 311,
                    "text": "(Hoyos Barcelo et al., 2017;",
                    "ref_id": null
                },
                {
                    "start": 312,
                    "end": 333,
                    "text": "Vhaduri et al., 2019)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "In addition, the Artificial Neural Network (ANN), Gaussian Mixture Model (GMM), Support Vector Machine (SVM), and other methods are also used for cough recognition (Drugman et al., 2011) .",
            "cite_spans": [
                {
                    "start": 164,
                    "end": 186,
                    "text": "(Drugman et al., 2011)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "The difficulty of cough recognition mainly lies in the distinction of background noise. There are many kinds of sound mixed together in daily scenes. How to effectively distinguish between coughing and other sounds has become a difficult problem to be solved.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "In this paper, we propose a cough recognition method based on a Mel-spectrogram and a Convolutional Neural Network (CNN). First, we enhance the audio data and mix the voice in various complex scenes. Then, we preprocess the data to ensure the consistency of data length and convert it into a Melspectrogram. At last, we build a CNN-based model to classify the cough using the Mel-spectrogram. At the same time, we make comparisons with some other common methods. After the experiment result comparison, it can be seen that this method can effectively identify and detect coughing in complex scenes. It can be seen that the cough recognition model based on a Melspectrogram and a CNN can achieve good results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "INTRODUCTION"
        },
        {
            "text": "As shown in Figure 1 , the work-flow of our cough classification model is presented.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 20,
                    "text": "Figure 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "MATERIALS AND METHODOLOGY"
        },
        {
            "text": "Considering the natural environment, sound is not produced by a single sound source and the received sound is often the mix of multiple sounds. In order to improve the recognition effect and robustness, we enhance the data, using noise and human voice to mix the cough data.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Augmentation"
        },
        {
            "text": "We selected several audio datasets to make data augmentation, such as the ESC-50 dataset (Piczak, 2015) and the Speech Commands Data Set (Warden, 2018) . All cough data comes from the ESC-50 dataset.",
            "cite_spans": [
                {
                    "start": 89,
                    "end": 103,
                    "text": "(Piczak, 2015)",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 137,
                    "end": 151,
                    "text": "(Warden, 2018)",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Data Augmentation"
        },
        {
            "text": "Positive Samples 1: Cough. After audio segmentation, we select all cough audio samples as positive samples. We also obtain more cough audio samples by increasing and decreasing the volume.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Augmentation"
        },
        {
            "text": "Positive Samples 2 and 3: Cough + Human Sound and Cough + Natural Sound. In order to enhance the robustness of the model, we also mix cough audio with natural sound (wind, rain, door-clock, footsteps, and other common noises) and human sound (mainly including commonly spoken words such as \"go,\" \"up,\" \"right,\" and so on) respectively as positive samples 2 and 3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Augmentation"
        },
        {
            "text": "In all the mixed audio, the volume of the coughing sound is adjusted to produce more mixed outcomes of different cough sounds and other sounds.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Augmentation"
        },
        {
            "text": "All of the original and processed cough audio data are labeled as \"cough.\"",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Augmentation"
        },
        {
            "text": "Negative Samples 1: Human Sound. We choose human sounds (mainly include \"go,\" \"up\" and, some other common human noises, and all sounds come from different samples which are unused for cough augmentation) from the datasets above as one of the negative samples. So our model can distinguish between cough sounds and human sounds. And all human sounds were mixed with white noise, pink noise, and so on.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Augmentation"
        },
        {
            "text": "Negative Samples 2: Natural Sound: We choose natural noise (wind, rain, pouring-water, footsteps, and other common sounds. All sounds come from different samples which are unused for cough augmentation) from the datasets above as other negative samples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Augmentation"
        },
        {
            "text": "All human sound and natural sound data are labeled as \"others.\"",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Augmentation"
        },
        {
            "text": "In the end, we have cough sounds, mixed cough audio with natural noise, and mixed cough audio with human sounds as positive samples. At the same time, human sounds and natural sounds are taken as negative samples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Augmentation"
        },
        {
            "text": "Considering that audio with a too short length of time may make it difficult to recognize the sound, and that audio with a too long length of time may cause the superposition of a variety of uncorrelated sounds, we choose the length of 1 s as the input. And the duration of cough samples in the original dataset is different, so we select the audio containing coughing and divide it into seconds. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Data Preprocess"
        },
        {
            "text": "The Mel spectrum contains a short-time Fourier transform (STFT) for each frame of the spectrum (energy/amplitude spectrum), from the linear frequency scale to the logarithmic Mel-scale, and then goes through the filter bank to get the eigenvector, these eigenvalues can be roughly expressed as the distribution of signal energy on the Mel-scale frequency.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mel-Spectrogram"
        },
        {
            "text": "After the audio data are processed into 1 s-long data, we transform all the data into Mel-spectrograms so that we can train the convolutional neural networks for recognition.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mel-Spectrogram"
        },
        {
            "text": "Audio data usually have complex features, so it is necessary to extract useful features to recognize the audio. The Melspectrogram is one of the efficient methods for audio processing and 8 kHz sampling is used for each audio sample.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mel-Spectrogram"
        },
        {
            "text": "In the experiment, we employ the Python package called librosa for data processing and all parameters are as follows: (n fft 1024, hop length 512, n mels 128). Then we call the power_to_db function to convert the power spectrum (amplitude square) to decibel (DB) units.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Mel-Spectrogram"
        },
        {
            "text": "In Figure 2 , we show some examples of Mel-spectrograms. As can be seen from the figure, there are some differences in different types of voices. But after mixing noise, some details will be covered, which is helpful for us to test the cough recognition effect of the model for the real scene. And we extract the features of the audio and transform them into feature images, so there are three channels like traditional color images.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 11,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Mel-Spectrogram"
        },
        {
            "text": "For image input, we normalize them to make the model converge faster. For the Mel-spectrogram, we calculate the mean and standard deviation of the three channels respectively and then normalize them. The normalization formula is as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Normalization"
        },
        {
            "text": "where x denotes the values in different channels and x norm denotes normalized values.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Normalization"
        },
        {
            "text": "The recognition loss function of the model L rec represents the cross-entropy loss:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Loss Function"
        },
        {
            "text": "where y is the model output, y is the true label, and n is the number of samples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Loss Function"
        },
        {
            "text": "With the development of deep learning, more and more deep learning methods are applied to various scenarios, such as image recognition, image classification, speech recognition, machine translation, etc. As a kind of deep learning method, Convolutional Neural Networks (CNN) are widely used in the field of computer vision. In this section, we introduce the components of the proposed CNN-based network. The convolutional layer is the key of a CNN model, it can effectively reduce the parameters of the model and make it possible for the model to optimize. The calculation formula for the convolutional layer is as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convolutional Neural Network"
        },
        {
            "text": "where x n j is the output feature map, x n\u22121 i is the input feature map, M j is the selected area in the n \u2212 1 layer, k n ij is weight parameter, b n j is bias, and f is the activation function.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convolutional Neural Network"
        },
        {
            "text": "After each convolutional layer, we conduct batch normalization to make the outputs of the convolutional layer stay identically distributed, which can improve the performance of the model. The batch normalization formula is as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convolutional Neural Network"
        },
        {
            "text": "where x i is the output of convolutional layer without activation, u is the mean of x, \u03c3 2 is the variance of x, and \u03b3 and \u03b2 are parameters to learn. After feature extraction of the convolution layer, although the number of connections between layers has been significantly reduced, the number of neurons in the feature map group has not been significantly reduced. Therefore, like other common models, we add maximum pooling layers to solve this problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convolutional Neural Network"
        },
        {
            "text": "In the end, we use the fully connected layer as the output layer of the model. The calculation for the fully connected layer is:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convolutional Neural Network"
        },
        {
            "text": "where x is the input layer, N is the number of input layer nodes, w ij is the weight between the links x i and y j , b j is the bias, and f is the activation function. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Convolutional Neural Network"
        },
        {
            "text": "The CRN was trained by an Adam optimizer, whose learning rate is 0.0001. The max epoch and batch size were 20 and 64, respectively. The CRN was implemented by Pytorch and trained and tested on a computer with an Intel Core i7-8750H, two 8 GB memory chips (DDR4), and a GPU (Nvidia Geforce GTX 1060 6G).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiment Approach"
        },
        {
            "text": "Before training, we need to preprocess the audio data. As mentioned in the second part, we obtained 34,320 cough samples augmented by different audio data, including 17,160 cough + human sound samples, 17,160 cough + natural sound samples, 17,050 human sounds, and 17,919 different noises. As shown in Figure 3 , data components have been provided. In order to evaluate the model better, we use two ways to divide the processed dataset.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 302,
                    "end": 310,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Dataset Description"
        },
        {
            "text": "After all data are processed, 80% are randomly selected as the training set, 10% as the verification set, and 10% as the test set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random Division Dataset"
        },
        {
            "text": "Considering that due to data augmentation, some data may leak the features of coughing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Random Division Dataset"
        },
        {
            "text": "After all data are processed, we select almost 80% which we augment as the training set and 10% is augmented from completely different cough audio as the test set. In this way, the cough sounds of the training and test sets come from different original data, so that we can evaluate the generalization ability of the model. After all data are split, the mean and variance of each channel are calculated. They are normalized to make the model converge better.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "No-Leakage Division Dataset"
        },
        {
            "text": "In order to better evaluate the performance of the model, we list several indicators used to evaluate the model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance Measurements"
        },
        {
            "text": "The indicator that the samples with a correct reaction classification account for the total samples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Accuracy"
        },
        {
            "text": "The ratio of the number of samples recognized correctly to the total number of samples recognized.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Recall"
        },
        {
            "text": "The ratio of the number of samples recognized correctly to the number of samples that should be recognized.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Precision"
        },
        {
            "text": "It is an index used to measure the accuracy of the binary classification model.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F1 Score"
        },
        {
            "text": "Precision ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F1 Score"
        },
        {
            "text": "The Mel-spectrogram is an effective tool to extract hidden features from audio and visualize them as an image. A CNN model can effectively extract features from images, and then complete tasks such as classification and recognition. Therefore, we use the CNN model to effectively classify the audio and to realize the accurate recognition and detection of coughing. In Figure 4 , the architecture of this model has been illustrated.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 369,
                    "end": 377,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Experiment Based on Mel-Spectrogram + CNN"
        },
        {
            "text": "Considering the different positions of coughing in audio, the relative positions of coughing are also different. Before we feed the image into the network, we first unify the image size into 256 \u00d7 256, and then randomly select 224 \u00d7 224 size parts for the recognition of different cough positions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiment Based on Mel-Spectrogram + CNN"
        },
        {
            "text": "After two methods of dataset division and training, we get the performance of the cough recognition task.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "RESULTS"
        },
        {
            "text": "As shown in Table 1 , we can find that Mel-Spectrogram + CNN can achieve the best performance in cough recognition than other methods. For randomly divided datasets, the correct recognition rate is 98%. It can be seen that the model can still achieve good recognition performance even if a variety of different sounds are mixed. The train/test loss curves are presented in Figure 5 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 12,
                    "end": 19,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 373,
                    "end": 381,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Experiment on Random Division Dataset"
        },
        {
            "text": "Considering that the model needs to cope with the cough sounds of different people, we add an experiment to estimate the generalization ability of the model. In this experiment, all the cough data are augmented, but the cough sound in the training set and the test set come from totally different collection objects. In this way, it can detect whether the model has the ability to recognize the cough sound produced by strange sound sources effectively.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiment on No-Leakage Division Dataset"
        },
        {
            "text": "The train/ test loss curves of no-leakage experiment are presented in Figure 6 and the experiment result is shown in ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 70,
                    "end": 78,
                    "text": "Figure 6",
                    "ref_id": null
                }
            ],
            "section": "Experiment on No-Leakage Division Dataset"
        },
        {
            "text": "In order to prove the effectiveness of this method, we use several other methods for comparison.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiment Based On Other Traditional Methods"
        },
        {
            "text": "MFCC is an effective method to extract audio features. We use this method to preprocess the original audio data and then pass it to the different model. In order to make it suitable for the linear model, in the experiment, we take the average value on each dimension.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "MFCC"
        },
        {
            "text": "BP is a multilayer feedforward network which has a strong nonlinear mapping ability. In our experiment, we build a fourlayer BP neural network and the activation is ReLU.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Back Propagation Network"
        },
        {
            "text": "A Support Vector Machine (SVM) is a kind of generalized linear classifier that classifies data according to supervised learning.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Support Vector Machine"
        },
        {
            "text": "The K-means algorithm is an iterative clustering algorithm. Firstly, it randomly selects K objects as the initial clustering center. Then it calculates the distance between each object and each seed cluster center and assigns each object to the nearest cluster center.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "K-Means"
        },
        {
            "text": "Naive Bayes is a classification method based on Bayes theorem and the independent hypothesis of characteristic conditions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Naive-Bayes"
        },
        {
            "text": "LighGBM is one of the boosting set models. It is an efficient implementation of the Gradient Boosting Decision Tree (GBDT) as XGBoost. In principle, it is similar to GBDT and XGBoost. It uses the negative gradient of loss function as the residual approximation of the current decision tree to fit the new decision tree. All results based on these methods are shown in Table 1 , and we can find that the CNN model is better than these methods in recognition accuracy and other indicators.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 368,
                    "end": 375,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "LightGBM"
        },
        {
            "text": "In this work, we proposed a cough recognition network (CRN) based on the CNN model and a Mel-spectrogram. From the experiments result based on random division and no-leakage division datasets, we can find that the proposed CRN can achieve excellent performance in cough recognition. Compared to other methods, the accuracy of CRN is highest and most of the indexes are the best. In order to estimate the generalization ability of the model, we have collected some cough sounds that were not included in training. We find that the CRN can also recognize them efficiency. Experiments show that the model can recognize coughing in complex scenes effectively, and can recognize coughing with various other sounds correctly, which is good for cough monitoring in daily life. Cough recognition is a potential solution for disease management during the COVID-19 pandemic and reduces epidemic prevention workers' exposure possibility.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CONCLUSION"
        },
        {
            "text": "Although the model has achieved good recognition results, there are still some problems that need to be further solved. For example, the audio length is now limited to 1 s. When the intercept position is not right, it may be misjudged.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "CONCLUSION"
        },
        {
            "text": "Publicly available datasets were analyzed in this study. This data can be found here: https://github.com/karolpiczak/ESC-50 ESC-50",
            "cite_spans": [],
            "ref_spans": [],
            "section": "DATA AVAILABILITY STATEMENT"
        },
        {
            "text": "Dataset http://download.tensorflow.org/data/speech_ commands_v0.02.tar.gz Speech Commands Dataset.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "DATA AVAILABILITY STATEMENT"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Deep Neural Networks for Identifying Cough Sounds",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Amoh",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Odame",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Biomed. Circuits Syst",
            "volume": "10",
            "issn": "",
            "pages": "1003--1011",
            "other_ids": {
                "DOI": [
                    "10.1109/TBCAS.2016.2598794"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Assessment of Audio Features for Automatic Cough Detection",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Drugman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Urbain",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Dutoit",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "2011 19th European Signal Processing Conference",
            "volume": "",
            "issn": "",
            "pages": "1289--1293",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Objective Study of Sensor Relevance for Automatic Cough Detection",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Drugman",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Urbain",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Bauwens",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Chessini",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Valderrama",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Lebecque",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE J. Biomed. Health Inform",
            "volume": "17",
            "issn": "",
            "pages": "699--707",
            "other_ids": {
                "DOI": [
                    "10.1109/jbhi.2013.2239303"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "A Wireless Respiratory Monitoring System Using a Wearable Patch Sensor Network",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Elfaramawy",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "L"
                    ],
                    "last": "Fall",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Arab",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Morissette",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Lellouche",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Gosselin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Sensors J",
            "volume": "19",
            "issn": "",
            "pages": "650--657",
            "other_ids": {
                "DOI": [
                    "10.1109/JSEN.2018.2877617"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Semi-supervised Tree Support Vector Machine for Online Cough Recognition",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Hoa",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Tran",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Dat",
                    "suffix": ""
                }
            ],
            "year": 2011,
            "venue": "12th Annual Conference of the Frontiers in Robotics and AI | www.frontiersin.org",
            "volume": "8",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Efficient K-NN Implementation for Real-Time Detection of Cough Events in Smartphones",
            "authors": [
                {
                    "first": "|",
                    "middle": [],
                    "last": "Hoyos-Barcelo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Monge-Alvarez",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zeeshan Shakir",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Alcaraz-Calero",
                    "suffix": ""
                },
                {
                    "first": "J.-M",
                    "middle": [],
                    "last": "Casaseca-De-La-Higuera",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "International SpeechCommunication Association",
            "volume": "22",
            "issn": "",
            "pages": "1662--1671",
            "other_ids": {
                "DOI": [
                    "10.1109/JBHI.2017.2768162"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Classification of Voluntary Coughs Applied to the Screening of Respiratory Disease",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Infante",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "B"
                    ],
                    "last": "Chamberlain",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kodgule",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "R"
                    ],
                    "last": "Fletcher",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Annu Int. Conf. IEEE Eng. Med. Biol. Soc",
            "volume": "",
            "issn": "",
            "pages": "1413--1416",
            "other_ids": {
                "DOI": [
                    "10.1109/EMBC.2017.8037098"
                ]
            }
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Speech Recognition Using Mfcc. Int. Conf. Comp. Grap. Simula. Model",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Ittichaichareon",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Suksri",
                    "suffix": ""
                },
                {
                    "first": "Yingthawornsuk",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "135--138",
            "other_ids": {
                "DOI": [
                    "10.13140/RG.2.1.2598.3208"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Esc: Dataset for Environmental Sound Classification",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "J"
                    ],
                    "last": "Piczak",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "1015--1018",
            "other_ids": {
                "DOI": [
                    "10.1145/2733373.2806390"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Fft with Modified Frequency Scale for Audio Signal Analysis",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Pucik",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Kubinec",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Ondracek",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "International Conference Radioelektronika",
            "volume": "",
            "issn": "",
            "pages": "1--4",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Highway-LSTM and Recurrent Highway Networks for Speech Recognition",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Pundak",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Sainath",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Proceedings of Interspeech 2017",
            "volume": "",
            "issn": "",
            "pages": "1303--1307",
            "other_ids": {
                "DOI": [
                    "10.21437/Interspeech.2017-429"
                ]
            }
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Spectral Representations for Convolutional Neural Networks. arXiv",
            "authors": [
                {
                    "first": "O",
                    "middle": [],
                    "last": "Rippel",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Snoek",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "P"
                    ],
                    "last": "Adams",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Early Detection and Assessment of Covid-19",
            "authors": [
                {
                    "first": "H",
                    "middle": [
                        "A"
                    ],
                    "last": "Sattar Hashmi",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "M"
                    ],
                    "last": "Asif",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Front. Med",
            "volume": "131",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.3389/fmed.2020.00311"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Analysis of Mfcc and Multitaper Mfcc Feature Extraction Methods",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "G"
                    ],
                    "last": "Shintri",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "K"
                    ],
                    "last": "Bhatia",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Int. J. Comput. Appl",
            "volume": "131",
            "issn": "",
            "pages": "7--10",
            "other_ids": {
                "DOI": [
                    "10.5120/ijca2015906883"
                ]
            }
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Fast-lstm Acoustic Model for Distant Speech Recognition",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Trianto",
                    "suffix": ""
                },
                {
                    "first": "T.-C",
                    "middle": [],
                    "last": "Tai",
                    "suffix": ""
                },
                {
                    "first": "J.-C",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Inter. Confer. Consu. Electro. (ICCE)",
            "volume": "2018",
            "issn": "",
            "pages": "1--4",
            "other_ids": {
                "DOI": [
                    "10.1109/ICCE.2018.8326195"
                ]
            }
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Nocturnal Cough and Snore Detection in Noisy Environments Using Smartphone-Microphones",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Vhaduri",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "V"
                    ],
                    "last": "Kessel",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Ko",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Wood",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Brunschwiler",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Inter. Conf. Health. Infor. (ICHI)",
            "volume": "2019",
            "issn": "",
            "pages": "1--7",
            "other_ids": {
                "DOI": [
                    "10.1109/ICHI.2019.8904563"
                ]
            }
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Speech Emotion Recognition Based on Improved Mfcc",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Inter. Confe. Compu. Sci. Appli. Engin",
            "volume": "88",
            "issn": "",
            "pages": "1--7",
            "other_ids": {
                "DOI": [
                    "10.1145/3207677.3278037"
                ]
            }
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition. arXiv",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Warden",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.2172/1635786"
                ]
            }
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Algorithm of Abnormal Audio Recognition Based on Improved Mfcc",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Proced. Eng",
            "volume": "29",
            "issn": "",
            "pages": "731--737",
            "other_ids": {
                "DOI": [
                    "10.1016/j.proeng.2012.01.032"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "The work-flow diagram. Frontiers in Robotics and AI | www.frontiersin.org May 2021 | Volume 8 | Article 580080",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Mel-spectrograms of different voices. Frontiers in Robotics and AI | www.frontiersin.org May 2021 | Volume 8 | Article 580080",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Data components. Frontiers in Robotics and AI | www.frontiersin.org May 2021 | Volume 8 | Article 580080",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "The Architecture of the Mel-spectrogram and CNN model.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "The loss of the random division experiment.FIGURE 6 | The loss of the no-leakage division experiment. Frontiers in Robotics and AI | www.frontiersin.org May 2021 | Volume 8 | Article 580080",
            "latex": null,
            "type": "figure"
        },
        "TABREF1": {
            "text": "The comparison results of different methods.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "The no-leakage recognition accuracy is 95.18% and the F1 score is the highest of all methods. It can be seen that the model performs well during generalization cough recognition tasks.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "BF proposed the idea of the paper. QZ and JS designed the network and wrote the manuscript. WD, CW, and SY wrote the code and analyzed the results. FS and HL helped improve the paper. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "AUTHOR CONTRIBUTIONS"
        },
        {
            "text": "The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.Copyright \u00a9 2021 Zhou, Shan, Ding, Wang, Yuan, Sun, Li and Fang. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conflict of Interest:"
        }
    ]
}