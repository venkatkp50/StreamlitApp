{
    "paper_id": "0b96a7078e0c3a23db8038369e182fffc96ffcfb",
    "metadata": {
        "title": "Detection of COVID-19 from speech signal using bio-inspired based cepstral features",
        "authors": [
            {
                "first": "Tusar",
                "middle": [
                    "Kanti"
                ],
                "last": "Dash",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "C V Raman Global University",
                    "location": {
                        "settlement": "Bhubaneswar",
                        "country": "India"
                    }
                },
                "email": ""
            },
            {
                "first": "Soumya",
                "middle": [],
                "last": "Mishra",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "C V Raman Global University",
                    "location": {
                        "settlement": "Bhubaneswar",
                        "country": "India"
                    }
                },
                "email": ""
            },
            {
                "first": "Ganapati",
                "middle": [],
                "last": "Panda",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "C V Raman Global University",
                    "location": {
                        "settlement": "Bhubaneswar",
                        "country": "India"
                    }
                },
                "email": ""
            },
            {
                "first": "Suresh",
                "middle": [],
                "last": "Chandra Satapathy",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "KIIT Deemed to be University",
                    "location": {
                        "settlement": "Bhubaneswar",
                        "country": "India"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "The early detection of COVID-19 is a challenging task due to its deadly spreading nature and existing fear in minds of people. Speech-based detection can be one of the safest tools for this purpose as the voice of the suspected can be easily recorded. The Mel Frequency Cepstral Coefficient (MFCC) analysis of speech signal is one of the oldest but potential analysis tools. The performance of this analysis mainly depends on the use of conversion between normal frequency scale to perceptual frequency scale and the frequency range of the filters used. Traditionally, in speech recognition, these values are fixed. But the characteristics of speech signals vary from disease to disease. In the case of detection of COVID-19, mainly the coughing sounds are used whose bandwidth and properties are quite different from the complete speech signal. By exploiting these properties the efficiency of the COVID-19 detection can be improved. To achieve this objective the frequency range and the conversion scale of frequencies have been suitably optimized. Further to enhance the accuracy of detection performance, speech enhancement has been carried out before extraction of features. By implementing these two concepts a new feature called COVID-19 Coefficient (C-19CC) is developed in this paper. Finally, the performance of these features has been compared.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Coronavirus disease which exhibits acute respiratory syndrome is a deadly viral infection. As reported, it has started in Wuhan, China in 2019 and has affected the whole world [1] . As per the report of the World Health Organization, more than a hundred million people have suffered till 7th March 2021 out of which more than 2.5 million deaths have been reported [2] . The social distancing of 1.6 m to 3 m is recommended to control the rapid spreading of COVID-19 cases [3] . It is observed from the experiences of the medical practitioners that rather than the deadly nature of the virus, its fear of stigma is stopping people from going to medical laboratories for testing purposes [4] . Under such circumstances, it has become a huge challenge for developing an appropriate method for the early detection of this disease. It is a fact that the speech-based detection of COVID-19 is a simpler and safer approach for this purpose [5] . In this section, a review of related literature is carried out in two parts: speech based COVID-19 detection and speech recognition using MFCC features. approach using audio, text, and image for achieving better detection results. The log Mel spectra of a speech signal are mapped with the respiratory sensors to train the neural network-based models. A sensitivity of 91.2% for breathing based detection and a mean absolute error of 1.01 breaths per minute have been reported using the proposed methods. In another paper, the health condition of COVID-19 patients is categorized into four types with respect to the severity of illness, sleep quality, fatigue, and anxiety [10] . Audio dataset has been collected from twenty females and thirty-two males COVID19 patients from two hospitals in Wuhan, China during March 20, -26, 2020 . Two acoustic feature sets from the computational paralinguistics challenge and extended Geneva minimalistic acoustic parameter sets have been used as inputs to SVM to achieve an average classification accuracy of 69%.",
            "cite_spans": [
                {
                    "start": 176,
                    "end": 179,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 364,
                    "end": 367,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 472,
                    "end": 475,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 686,
                    "end": 689,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 933,
                    "end": 936,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 1612,
                    "end": 1616,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 1752,
                    "end": 1771,
                    "text": "March 20, -26, 2020",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The cepstral analysis is one of the oldest and popular signal analysis methods which finds applications in the speech signal and mechanical systems [11] . The MFCC based features are very popular and effective for speech recognition, music information retrieval, speech evaluation parameters, etc. An optimization of MFCC features is achieved by reducing the feature space using Linear Discriminant Analysis Fisher's F-ratio [12] . The use of these features achieves faster convergence of the ANN model and improvement in recognition accuracy at different SNR levels. The source recognition of Cell-Phone is carried out using the optimization of different cepstral coefficients such as Mel, linear, and Bark frequency [13] . The use of minimum and maximum frequencies of MFCC and cepstral variance normalization has enhanced the identification rate to 96.85%. With an objective to minimize the dissimilarity between the perceptual and feature domain distortions, modified MFCC based features are proposed in [14] . These simple feature vectors provide improved speech recognition performance in noisy as well as clean conditions. In another paper, the mean and standard deviation of the feature space including MFCC are optimized using genetic algorithm, and differential evolution [15] . These improved features are used for the punjabi language speaker recognition.",
            "cite_spans": [
                {
                    "start": 148,
                    "end": 152,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 425,
                    "end": 429,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 718,
                    "end": 722,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 1008,
                    "end": 1012,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [],
            "section": "Review on optimization in MFCC features"
        },
        {
            "text": "An analysis of different frequency bands has been carried out using the F-ratio method for speech unit classification and it is observed that 1 kHz to 3 kHz frequency range to be provided more emphasis and accordingly an optimization of features using the F-ratio scale is proposed in [16] . A significant reduction in the sentence error rate is reported by using the proposed feature optimization technique. The central and side frequency parameters of MFCC filter banks have been optimized using particle swarm optimization and genetic algorithm [17] . These optimized features are then applied in the Hindi vowel recognition using the Hidden Markov model and Multilayer perceptrons under different noise conditions. A hybrid approach is proposed by taking gammatone and mel frequency cepstral coefficients with PCA, and multi tapered method using differential evolution, and Hidden Markov model (HMM) based classifiers for robust recognition of Punjabi speech under different noise conditions [18] . The frequency range of the filter banks of MFCC is optimized for emotion recognition using two databases [19] . This method has improved the speaker-independent emotion recognition accuracy by 15% for the Assamese database and 25% for the Berlin database.",
            "cite_spans": [
                {
                    "start": 285,
                    "end": 289,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 548,
                    "end": 552,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1108,
                    "end": 1112,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Review on optimization in MFCC features"
        },
        {
            "text": "It is observed from the literature review that early detection of COVID-19 from speech data is a challenging and timely research area [20] [21] [22] . For the remote online based COVID-19 detection from the speech signal, the patients have to use mobile applications in the real-life noisy environment. Investigation in this direction has not been fully explored particularly in the selection of proper audio features of COVID-19 patients for detection purposes. The nature of the speech signals used for the analysis of COVID-19 are mainly the breathing and cough sounds and hence it is quite different from the speech signal comprising complete sentences. The MFCC features have also been used in COVID-19 detection in [23] , but these features are directly used without any modification or improvement in the features and hence the detection performance is poor. Thus, in the present COVID-19 scenario, there is a huge requirement to develop a better effective tool for improved detection from a safe distance and remotely recorded speech. Hence, there is a need to find and identify improved features which are expected to improve the detection accuracy of the classifier. The focus of the current investigation is to develop potential features from the speech data for facilitating the classifier to yield higher accuracy of detection. In addition, speech enhancement is required to be carried out for extraction of the proper audio features [24] . It is further observed that there exists a huge class imbalance present in the speech data available on the online platforms [8, 23] . This class imbalance affects the overall training and testing performance of classifiers and hence this issue needs to be addressed and resolved. These problems have been identified during the literature review and taken up in this paper.",
            "cite_spans": [
                {
                    "start": 134,
                    "end": 138,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 139,
                    "end": 143,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 144,
                    "end": 148,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 721,
                    "end": 725,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1447,
                    "end": 1451,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1579,
                    "end": 1582,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 1583,
                    "end": 1586,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Motivation"
        },
        {
            "text": "There are different techniques used for achieving higher accuracy in classification and prediction tasks out of which the deep learning-based techniques are preferred if the data size is high and features extraction and selection are difficult [25, 26] . On the other hand, ML based methods use extracted features whereas CNN extracts the appropriate features through a series of convolution operations. Further DL methods involve more time for classification [27] [28] [29] . In the present problem, two speech datasets [8, 23] with categorically less data are available and the feature optimization is the target. Though Cepstral analysis is an old method, still it is quite popular, effective and a lot of recent articles still employ this feature [30, 31] . Therefore, for the present problem, Cepstral optimised features are obtained by a bio-inspired technique and used for classification purposes.",
            "cite_spans": [
                {
                    "start": 244,
                    "end": 248,
                    "text": "[25,",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 249,
                    "end": 252,
                    "text": "26]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 460,
                    "end": 464,
                    "text": "[27]",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 465,
                    "end": 469,
                    "text": "[28]",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 470,
                    "end": 474,
                    "text": "[29]",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 521,
                    "end": 524,
                    "text": "[8,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 525,
                    "end": 528,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 751,
                    "end": 755,
                    "text": "[30,",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 756,
                    "end": 759,
                    "text": "31]",
                    "ref_id": "BIBREF30"
                }
            ],
            "ref_spans": [],
            "section": "Motivation"
        },
        {
            "text": "Based on the motivation of research arising out of the literature review, the problem has been formulated with the following research objectives. Thus, the research objectives of the paper are:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Research objectives"
        },
        {
            "text": "1. To analyze the cepstral features used in speech recognition and to suitably optimizing the conversion scale in the frequency domain, and frequency range of filter banks using the bio-inspired technique to achieve better COVID-19 detection. 2. To identify the best possible sound patterns during coughing, breathing, and voiced sounds to detect COVID-19. 3. To employ the adaptive synthetic sampling approach for achieving efficient training for class imbalance in the database and to facilitate proper classification using SVM. 4. To compare and analyze the detection performance of the proposed cepstral feature-based classifier with that obtained from other reported results.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Research objectives"
        },
        {
            "text": "Based on the objectives of the research the organization of the paper proceeds as follows. The introduction, literature review, motivation, research objectives are presented in Section I. In section II, a detailed review of the related work on the theme of the problem is presented. The salient characteristics of data obtained from the standard database of COVID-19 are provided in Section III. In Section IV, the proposed methodology is presented in detail. The simulation-based experiment using two standard data sets and discussion of results as well as the contribution of the paper is presented in Section V. Finally, the conclusion of the paper and scope for future work are dealt with in Section VI.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Organization of the paper"
        },
        {
            "text": "The cepstral analysis is one the oldest and popular signal analysis methods used in various applications like speech signal processing, mechanical engineering problems, and analysis of multiple inputs, multiple output systems, etc [11] . The MFCCs are very popular and effective features in speech recognition, music information retrieval, speech evaluation parameters. It is normally calculated using the following steps [32] . \u2022 Step-1 -Apply the pre-processing like windowing, framing to the input signal \u2022 Step-2 -Calculate the energy of the frame \u2022 Step-3 -Find the Discrete Fourier transform using the FFT method \u2022 Step-4 -Apply the Mel filter bank by mapping the power spectrum into the mel scale and by using triangular overlapping windows. These steps are also shown in Fig. 1 .",
            "cite_spans": [
                {
                    "start": 231,
                    "end": 235,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 422,
                    "end": 426,
                    "text": "[32]",
                    "ref_id": "BIBREF31"
                },
                {
                    "start": 429,
                    "end": 430,
                    "text": "\u2022",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 779,
                    "end": 785,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Related works"
        },
        {
            "text": "Step-4 deals with the use of the Mel filter bank. There are several variations of these filter banks reported in the speech processing such as triangular filter bank using mel-scale, human factor scale, Bark-scale, ERB-scale, and Gammatone filter bank. Depending on these filter banks the cepstral coefficients are named accordingly. The basic steps used for the filter bank design are shown in Fig. 2 [33] . In this case, the Perceptual scale means frequency scale based on the human perception of sound like the mel scale. The details of the four types of features are discussed in the sequel.",
            "cite_spans": [
                {
                    "start": 402,
                    "end": 406,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [
                {
                    "start": 395,
                    "end": 401,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Related works"
        },
        {
            "text": "The steps involved in the design of the filter bank for extraction of TFBCC-M features [33] are explained in this section. The steps are as follows: f m = 2595 \u00b7 log 10 1 + f 700",
            "cite_spans": [
                {
                    "start": 87,
                    "end": 91,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Triangular filter bank using mel-scale (TFBCC-M features)"
        },
        {
            "text": "(1)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Triangular filter bank using mel-scale (TFBCC-M features)"
        },
        {
            "text": "\u2022 The linear scale lower and higher cut-off frequencies ( f l and f h ) are converted into melscale ( fm l and fm h ) respectively. Now, the center frequencies f m c p of each filter are calculated using Equation (2) .",
            "cite_spans": [
                {
                    "start": 213,
                    "end": 216,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Triangular filter bank using mel-scale (TFBCC-M features)"
        },
        {
            "text": "where p = 1, 2, 3,..., P -1. P is the number of Mel filters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Triangular filter bank using mel-scale (TFBCC-M features)"
        },
        {
            "text": "\u2022 The center frequencies f m c p of p th filter band is to be converted to linear scale using f c p given in Equation (3) . ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Triangular filter bank using mel-scale (TFBCC-M features)"
        },
        {
            "text": "Where, k is the frequency domain index of FFT, f r (p) is the rounded center frequencies ( f c p ) and it is calculated using Equation (5) and fs is the sampling frequency of the speech signal.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Triangular filter bank using mel-scale (TFBCC-M features)"
        },
        {
            "text": "The calculation of TFBCC-B features is similar to the TFBCC-M features with only one difference [33] . Instead of conversion between linear scale frequency of DFT and mel scale (as mentioned in Equations (1) and (3) ), the conversion is between linear scale frequency of DFT ( f ) and Bark scale ( f b )as mentioned in Equations (6) and (7) .",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 100,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                }
            ],
            "ref_spans": [],
            "section": "Triangular filter bank using bark scale (TFBCC-B features)"
        },
        {
            "text": "where f b c p is the center frequency in the Bark scale.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Triangular filter bank using bark scale (TFBCC-B features)"
        },
        {
            "text": "The difference between the designing of filter bank for TFBCC-M and TFBCC-H features lies in the calculation of the critical bandwidth using the ERB approximation [34] .",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 167,
                    "text": "[34]",
                    "ref_id": "BIBREF33"
                }
            ],
            "ref_spans": [],
            "section": "Triangular filter bank using human factor scale (TFBCC-H features)"
        },
        {
            "text": "\u2022 The center frequencies f c p of the first ( p = 1 ) and last ( p = P ) filters are computed as mentioned in Equation (8) ",
            "cite_spans": [
                {
                    "start": 119,
                    "end": 122,
                    "text": "(8)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Triangular filter bank using human factor scale (TFBCC-H features)"
        },
        {
            "text": "where p is the index of filter and P is the maximum number of filters. For the first filter The f c p is obtained usingEquation (9) and Table 1 . ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 136,
                    "end": 143,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Triangular filter bank using human factor scale (TFBCC-H features)"
        },
        {
            "text": "linear-scale to mel-scale as mentioned in Equation (11) , where f c 1 and f cP are the center frequencies of the first and last filter in the mel-scale computed using Equation (1) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Triangular filter bank using human factor scale (TFBCC-H features)"
        },
        {
            "text": "\u2022 Convert the computed center frequencies of all the filters from mel-scale to linear-scale using Equation (3) . \u2022 Calculate the lower and upper limit of the frequency of each of the filters in the filter bank using Equations (12) , (13) and (14) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Triangular filter bank using human factor scale (TFBCC-H features)"
        },
        {
            "text": "\u2022 Round off the center frequencies and magnitude response calculation of each filter as is done in case of TFBCC-M features using Equations (4) and (5) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Triangular filter bank using human factor scale (TFBCC-H features)"
        },
        {
            "text": "The calculation of TFBCC-E features is similar to that of TFBCC-H features with only one change [33] . Instead of conversion between linear scale frequency of DFT and mel scale (as mentioned in Equations (11) , (1) and (3) ), the conversion is made between linear scale frequency of DFT ( f ) and ERB scale ( f e )as mentioned in Equations (15) and (16) .",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 100,
                    "text": "[33]",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 349,
                    "end": 353,
                    "text": "(16)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Triangular filter bank using ERB scale (TFBCC-E features)"
        },
        {
            "text": "It is observed from the detailed review of the existing Cepstral coefficients that the two parameters such as the conversion of frequency from linear scale to perceptual scale and the selection of cut-off frequencies are the determining factor for the performance of the cepstral analysis. This issue is addressed in the literature [13] [14] [15] [16] [17] [18] [19] . This problem has been further investigated in Section 5.2 .",
            "cite_spans": [
                {
                    "start": 332,
                    "end": 336,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 337,
                    "end": 341,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 342,
                    "end": 346,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 347,
                    "end": 351,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 352,
                    "end": 356,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 357,
                    "end": 361,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 362,
                    "end": 366,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Parameters for optimization"
        },
        {
            "text": "Two Speech databases are used in the simulation-based experiments carried out in Section 5 . The brief details of the databases used are discussed in this section.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Database preparation"
        },
        {
            "text": "The Coswara database (DB-1) has been developed by the Indian Institute of Science, India in the year 2020 [23] . A web application is used for collecting audio samples for diagnosing COVID-19 prevalence using breath, cough, and voice sounds. The audio files are arranged in groupings of different respiratory indicators such as shallow and deep breathing, shallow and heavy cough, continuous vowel pronunciation /a/, /e/, and /o/, counting normal and fast. This database also contains additional information in terms of age, gender, demography, existing health history, and the existence of chronic health preconditions. The volunteering members have individually contributed to multiple sound clips for multiple segments. The maximum duration of the sound clips for the individual segment is approximately 15 seconds duration with the sampling frequency as 41 kHz or 48 kHz. This data set covers 570 participants and each participant has contributed 9 audio files pertaining to various categories. Cumulatively, this data set comprises 3470 clean, 1055 noisy, and the remaining are highly degraded sound samples.",
            "cite_spans": [
                {
                    "start": 106,
                    "end": 110,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Coswara database"
        },
        {
            "text": "The Crowdsourced Respiratory Sound Data (DB-2) has been developed by Cambridge University, the UK in the year 2020 [8] . The Android and web-based sound application is used for capturing speech/audio samples for detecting Corona Virus disease prevalence using breath, and cough sounds. For capturing the required sounds, the volunteers are prompted to follow instructions for coughing and breathing a couple of times along with reading phases. Finally, the users are asked whether they have clinically tested COVID positive so far. Since the project employs two applications to collect the data, the indicating words 'web' and 'android' are frequently used to distinguish between the recorded audio samples. Also, while naming the audio files, 'no cough' and 'with cough' designate a volunteer's report of a condition to dry or wet cough, while 'nosymp' means the volunteer showed no signs at that time. The selected audio.wav files have been arranged in groupings under categories of cough, breath, and asthma. The maximum duration of the sound clips for the individual segment recorded by the android application varies from 8 seconds to 20 seconds duration approximately with a sampling frequency of a maximum 48 kHz. Similarly, the maximum duration of the sound clips for the individual segment recorded by Web application varied from 11 seconds to 24 seconds duration approximately with the sampling frequency of maximum of 48 kHz samples. This dataset consists of 4352 unique users from the web app and 2261 unique users from the Android app. Out of these, total of 235 users are declared COVID-19 positive.",
            "cite_spans": [
                {
                    "start": 115,
                    "end": 118,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Crowdsourced respiratory sound data"
        },
        {
            "text": "In the two databases, the signals are recorded at 44.1 kHz and 48.1 kHz sampling rates. It is observed from the literature [35] the most of the latent features are within 8 kHz bandwidth and the Hence pre-processing is done by downsampling the speech signal to 16 kHz.",
            "cite_spans": [
                {
                    "start": 123,
                    "end": 127,
                    "text": "[35]",
                    "ref_id": "BIBREF34"
                }
            ],
            "ref_spans": [],
            "section": "Crowdsourced respiratory sound data"
        },
        {
            "text": "The details of the proposed method are discussed in this Section. The methodology is depicted in the block diagram form ( Fig. 3 ) and the need and operation of each block are explained. The algorithm is divided into two parts: configuration and application [36] . The configuration part deals with the preparation of a clean balanced dataset, and the application part explains the extraction and use of the proposed C-19CC features. In the configuration stage, the two available datasets are analysed and converted into a labeled balanced dataset by using the Adaptive Synthetic Sampling Approach [37] which is applied to deal with meant for Imbalanced Learning method. To transform the dataset with a uniform single sampling rate, the sampling rate conversion strategy is applied. Subsequently, the Speech Enhancement algorithm is employed for the reduction of noise present in the data. In the configuration stage, the calculation of the best possible values of the Cepstral features is carried out from the pre-processed dataset. Finally, the previously configured set of C-19CC features are extracted from speech databases. The classifier is then trained using these extracted features for the identification of the appropriate class.",
            "cite_spans": [
                {
                    "start": 258,
                    "end": 262,
                    "text": "[36]",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 598,
                    "end": 602,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [
                {
                    "start": 120,
                    "end": 128,
                    "text": "( Fig. 3",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Proposed methodology"
        },
        {
            "text": "Based on the flashing pattern of fireflies, an efficient optimization algorithm known as FF Algorithm has been developed in the last decade. It has the advantages of swarm intelligence and also it has other advantages as compared to the standard swarm intelligence based algorithms due to its automatic subdivision and the ability of dealing with multi-modality [38] . This section deals with a brief discussion on the operation and implementation of the FF algorithm.",
            "cite_spans": [
                {
                    "start": 362,
                    "end": 366,
                    "text": "[38]",
                    "ref_id": "BIBREF37"
                }
            ],
            "ref_spans": [],
            "section": "Fire-Fly optimization algorithm"
        },
        {
            "text": "The basic concept of the FF algorithm is based on the attraction and attacking principle of the firefly species. They produce short and rhythmic flashes and the attractiveness of a firefly is calculated by its brightness (light intensity). This principle is modeled as the objective function. The attractiveness is governed by light intensity variation with distance and the absorption coefficient and expressed in Equation (17) .",
            "cite_spans": [
                {
                    "start": 424,
                    "end": 428,
                    "text": "(17)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Fire-Fly optimization algorithm"
        },
        {
            "text": "The explanations of the parameters used in the FF Algorithm are listed in Table 2 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 74,
                    "end": 81,
                    "text": "Table 2",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Fire-Fly optimization algorithm"
        },
        {
            "text": "By combining the two factors of Equation (17) the instantaneous intensity can be expressed asEquation (18) ",
            "cite_spans": [
                {
                    "start": 102,
                    "end": 106,
                    "text": "(18)",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Fire-Fly optimization algorithm"
        },
        {
            "text": "Similarly, the attractiveness ( \u03b2) is represented as Equation (19) .",
            "cite_spans": [
                {
                    "start": 62,
                    "end": 66,
                    "text": "(19)",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [],
            "section": "Fire-Fly optimization algorithm"
        },
        {
            "text": "The distance between any two fireflies i and j at position x i and x j is computed as the Cartesian distance given in Equation (20) .",
            "cite_spans": [
                {
                    "start": 127,
                    "end": 131,
                    "text": "(20)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Fire-Fly optimization algorithm"
        },
        {
            "text": "Where x i,k is the k th component of the spatial coordinate x i of the i th firefly. The movement of a firefly i attracted to another brighter firefly j is expressed as Equation (21) , where the second term denotes the attraction and the third term is for inserting randomization. For most cases the value of \u03b2 o = 1 and \u03b1 \u2208 [0, 1].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fire-Fly optimization algorithm"
        },
        {
            "text": "The speed of the convergence and the overall effectiveness of the FF algorithm depends upon the parameter \u03b3 which denotes the variation of the attractiveness. The value of \u03b3 varies from 0.1 to 10. When any i th firefly is attracted by a brighter (more attractive) firefly j, then its movement is expressed as in Equation (21) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fire-Fly optimization algorithm"
        },
        {
            "text": "The flow chart of the FF algorithm is shown in Fig. 4 . In the FF algorithm, the whole population can be easily subdivided into subgroups based on the attraction principle. These subgroups help to find the local optimum solutions and correspondingly the global optimum. This concept of subdivision allows the fireflies to get the optima simultaneously if the population size is sufficiently [39] . Also, it has been proved that the FF algorithm works better than the traditional Particle Swarm Optimization and Genetic Algorithm in terms of both efficiency and success rate [39] . Recently several speech processing based optimizations have been implemented successfully using FF algorithms [40] . Hence, the FF algorithm has been chosen for finding the best possible cepstral features to be used for the COVID-19 classification.",
            "cite_spans": [
                {
                    "start": 391,
                    "end": 395,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 574,
                    "end": 578,
                    "text": "[39]",
                    "ref_id": "BIBREF38"
                },
                {
                    "start": 691,
                    "end": 695,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [
                {
                    "start": 47,
                    "end": 53,
                    "text": "Fig. 4",
                    "ref_id": null
                }
            ],
            "section": "Fire-Fly optimization algorithm"
        },
        {
            "text": "The classification is an important task for the detection of COVID-19. Support Vector Machines (SVM) are simple but potential classifiers having lower computational complexity providing higher classification accuracy as compared to other non-linear classifiers [41] . It is further observed from the literature that for the several speech signal based classifications, SVM with the Gaussian kernel is one of the effective classifiers due to its overall good per- formance and requirement of the optimization of fewer parameters associated with penalty and kernel parameters [42] . The crossvalidation process is used for the calculation of the accuracy of the classifier. In this case, the input data is divided into two parts such as training and testing. The validation accuracy is calculated from the unknown testing part of the data set. In the present case, 80% of the data is used for training and the remaining 20% is used for testing and a five-fold cross-validation scheme is also used. The simulation study is carried out in the MATLAB Platform and the validation accuracy is calculated using Equation (22) .",
            "cite_spans": [
                {
                    "start": 261,
                    "end": 265,
                    "text": "[41]",
                    "ref_id": "BIBREF40"
                },
                {
                    "start": 574,
                    "end": 578,
                    "text": "[42]",
                    "ref_id": "BIBREF41"
                },
                {
                    "start": 1112,
                    "end": 1116,
                    "text": "(22)",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Classification"
        },
        {
            "text": "In the optimization process, the cost function is considered as a minimization problem, the k-fold loss is taken as the objective function. The range of k-foldloss is from 0 to 1 and the lower the value the better is the overall performance of the classifier.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Classification"
        },
        {
            "text": "The problem dealt with in this paper is to calculate the optimum values of conversion between linear to mel scale and higher and lower cut-off frequencies in of the filters related Equations (1) , (2) and (3) . These equations are re-written here with the proper variables that needs to be optimized. The conversion of the linear scale frequency of DFT ( f ) to Mel scale ( f m ) is written with a variable factor \u03b4 in theEquation (23) . This idea of optimizing \u03b4for classification is inspired a similar implementation of the Optimization in Automatic Speech Recognition [14] . In normal MFCC calculations, the value of \u03b4 is taken as 7.",
            "cite_spans": [
                {
                    "start": 197,
                    "end": 200,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 431,
                    "end": 435,
                    "text": "(23)",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 571,
                    "end": 575,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Problem formulation"
        },
        {
            "text": "f m = 2595 \u00b7 log 10 ",
            "cite_spans": [
                {
                    "start": 17,
                    "end": 19,
                    "text": "10",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Problem formulation"
        },
        {
            "text": "Correspondingly, the conversion of mel scale to linear scale is also written with the variable \u03b4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem formulation"
        },
        {
            "text": "It is also observed from the literature that the linear scale lower and higher cut-off frequencies ( f l and f h ) and correspondingly the melscale lower and higher cut-off frequencies ( fm l and fm h ) in Equation ( 2) can also be optimally chosen for improvement in the overall classification accuracy [13] .",
            "cite_spans": [
                {
                    "start": 215,
                    "end": 216,
                    "text": "(",
                    "ref_id": null
                },
                {
                    "start": 304,
                    "end": 308,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "Problem formulation"
        },
        {
            "text": "The effect of the change in magnitude of the parameters on the accuracy of classification is dealt in this section. In Fig. 5 , the effects of the change in the value of \u03b4.and maximum frequency of the speech signal on the validation accuracy are shown. It is noticed from these two figures that the selection of the best possible value of the \u03b4.to achieve the highest validation accuracy is not straight forward and hence can not be selected using the empirical calculations. Due to the random nature in the pattern of these graphs, to select the best parameters to achieve the maximum validation accuracy, the bio-inspired optimization technique is employed. To fulfill this requirement, the FF algorithm is chosen in this paper.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 119,
                    "end": 125,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Need for finding optimum values of \u03b4., fl and fh using FF algorithm"
        },
        {
            "text": "The objective cost function is to minimize the k-fold loss. It is defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Need for finding optimum values of \u03b4., fl and fh using FF algorithm"
        },
        {
            "text": "The k-fold loss of the classifier depends on the values of \u03b4, fl and fh. Hence, these three variables ( \u03b4, fl and fh) are chosen for optimization. The k-fold loss needs to be minimized by suitably optimizing by using nature inspired technique. The range of k-fold loss lies between 0 and 1. The goal is to calculate the best possible values of these variables to obtain the lowest possible value of the k-fold loss at the output of the SVM classifier.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Need for finding optimum values of \u03b4., fl and fh using FF algorithm"
        },
        {
            "text": "Speech enhancement is a process used to denoise the noisy speech signal and to improve the overall quality and intelligibility of the denoised speech. It is widely used in hearing aids, speech communications, and speech recognition tasks. Recently, the phase spectrum compensation based speech enhancement has been proposed [43] and its performance has been shown to be improved using the bio-inspired and ANN techniques [40] . This algorithm has been employed in this paper in the speech enhancement part. This algorithm is based on the concept of the use of proper scaling factor in the phase spectrum compensation. The Flow chart of this algorithm is shown in Fig. 6 .",
            "cite_spans": [
                {
                    "start": 324,
                    "end": 328,
                    "text": "[43]",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 421,
                    "end": 425,
                    "text": "[40]",
                    "ref_id": "BIBREF39"
                }
            ],
            "ref_spans": [
                {
                    "start": 663,
                    "end": 669,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "Speech enhancement"
        },
        {
            "text": "Classification based on Imbalanced learning is a challenging task in the fields of machine learning and data mining. One of the effective approaches to handle such a problem is called Adaptive Synthetic Sampling Approach for Imbalanced Learning (ADASYN). It solves the problem of the imbalanced classification problem by generating new data from the minority class (synthetic data). This is achieved by reducing the bias of the class imbalance, and gradually changing the classification decision boundary [37] .",
            "cite_spans": [
                {
                    "start": 505,
                    "end": 509,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Adaptive synthetic sampling approach for imbalanced learning"
        },
        {
            "text": "The motivation behind using the ADASYN algorithm in the proposed COVID-19 detection problem due to its encouraging performance in speech recognition (vowel) in [37] , where the ratio between the number of the minority to majority samples is 90:900. Similarly, in the present case, the ratio of minority to majority class in Database-1 and Databse-2 are above 30% to 70% which are unbalanced case.",
            "cite_spans": [
                {
                    "start": 160,
                    "end": 164,
                    "text": "[37]",
                    "ref_id": "BIBREF36"
                }
            ],
            "ref_spans": [],
            "section": "Adaptive synthetic sampling approach for imbalanced learning"
        },
        {
            "text": "The steps of the or the ADASYN algorithm are expressed below. Step-4 -Calculate the number of synthetic data samples that need to be generated for each minority sample \u2022 Step-5 -Generate the synthetic data using Equation (26) for the loop varying from 1 to g i",
            "cite_spans": [
                {
                    "start": 221,
                    "end": 225,
                    "text": "(26)",
                    "ref_id": "BIBREF25"
                }
            ],
            "ref_spans": [],
            "section": "Adaptive synthetic sampling approach for imbalanced learning"
        },
        {
            "text": "where g i is the number of synthetic data that need to be generated for each minority sample, \u03bb is a random number between 0 and 1, and x zi is the random selection of one minority data sample from the K-nearest neighbors. The important parameter which makes ADASYN algorithm better than other similar technique [44] is the use of a density distribution which automatically calculates the number of synthetic samples that need to be generated for each minority data sample.",
            "cite_spans": [
                {
                    "start": 312,
                    "end": 316,
                    "text": "[44]",
                    "ref_id": "BIBREF43"
                }
            ],
            "ref_spans": [],
            "section": "Adaptive synthetic sampling approach for imbalanced learning"
        },
        {
            "text": "In this section the details of the simulation based experiments carried out and various results are obtained. The different steps in simulation study snd the corresponding flowchart is shown in Fig. 3 Step-1 -A balanced labeled data set is prepared with 200 speech samples.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 194,
                    "end": 200,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Experiment"
        },
        {
            "text": "Step-2 -The sampling rate is converted to 16 kHz for all the speech samples.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiment"
        },
        {
            "text": "Step-3 -Speech Enhancement principle is applied to remove the unwanted noise components from the data set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiment"
        },
        {
            "text": "Step-4 -The best possible values of \u03b4, fl and fh in the MFCC implementation are obtained which helps to yield the lowest possible value of k-fold loss of the SVM classifier using the enhanced speech data set.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiment"
        },
        {
            "text": "Step-5 -The optimized values of \u03b4, fl and fh obtained from the",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiment"
        },
        {
            "text": "Step-4 are applied and the corresponding modified cepstral features of the remaining speech samples of database-1 and 2 are obtained. Step-6 -The various performance measures of classification using the optimized features are obtained from the SVM classifier.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiment"
        },
        {
            "text": "The performance of any classifiers is commonly evaluated by using either a numeric metric (accuracy), or a graphical representation of performance (receiver operating characteristic (ROC) curve). A commonly used classification metric, the Confusion matrix is calculated from the four values such as: TP (True Positive), FP (False Positive), FN (False Negative), and TN (True Negative) [45] . For the medical diagnosis, the FN plays a crucial role because it shows the class of patients who suffer from the COVID-19 disease but the classifier has falsely predicted them to be healthy [46] . But the FP has less significance in COVID-19 as the patient can go for the second round of tests to confirm. But the COVID-19 positive patients should not have false interpretation as if they are negative. To effectively find the FNs, the Recall (Sensitivity) value is used. It is calculated as",
            "cite_spans": [
                {
                    "start": 385,
                    "end": 389,
                    "text": "[45]",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 583,
                    "end": 587,
                    "text": "[46]",
                    "ref_id": "BIBREF45"
                }
            ],
            "ref_spans": [],
            "section": "Sensitivity"
        },
        {
            "text": "Traditionally, F-\u03b2Score is another performance measure of classification accuracy. The \u03b2value indicates whether the evaluation importance would be to be given to FP or FN. It is known from the medical field that identification of FN is more significant than FP and \u03b2value is taken as 2 for evaluation of classification accuracy [47] . The F-\u03b2Score and F-2 Score are computed as",
            "cite_spans": [
                {
                    "start": 328,
                    "end": 332,
                    "text": "[47]",
                    "ref_id": "BIBREF46"
                }
            ],
            "ref_spans": [],
            "section": "F-\u03b2Score"
        },
        {
            "text": "T P T P + F P and Recall = T P T P + F N (28)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F-\u03b2Score"
        },
        {
            "text": "Kruskal-Wallis tests are widely used to check the suitability of the input features to be used in the classification task. It is a nonparametric evaluation and no assumption is made about any prior distribution of the input data [6] . The test is based on statistical parameter H defined in Equation (29) , where the total number of samples including all the classes is M, the number of samples in the jth class is m j , and the sum of ranks of the jth class is R j , and N is the number samples in the independent group.",
            "cite_spans": [
                {
                    "start": 229,
                    "end": 232,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [],
            "section": "Kruskal-Wallis tests"
        },
        {
            "text": "In this Section, the performance analysis using the existing MFCC features (TFBCC-M features) is carried out. For this purpose, 50 uniformly distributed random are selected from the COVID-19 positive and negative subjects in each of cough (C-1), breathing (B-1), and voiced (V-1) from database-1 and cough (C-2), breathing (B-2) from database-2. The 13 MFCC feature vectors are extracted and classified using the SVM classifier using five-fold cross validation. The performance is evaluated in terms of the validation accuracy, sensitivity, F-2 score, and Kruskal-Wallis tests using two databases. The results are plotted in Fig. 7 . The Receiver Operating characteristics (ROC) and Area Under Curves (AUC) are obtained and plotted in Fig. 8 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 625,
                    "end": 631,
                    "text": "Fig. 7",
                    "ref_id": "FIGREF9"
                },
                {
                    "start": 735,
                    "end": 741,
                    "text": "Fig. 8",
                    "ref_id": "FIGREF10"
                }
            ],
            "section": "Performance evaluation using TFBCC-M features"
        },
        {
            "text": "It has been observed that the overall performance of the TFBCC-M features are not satisfactory in the detection of COVID-19 and also it is noticed that the cough sounds are providing consistent performance compared to other categories of sounds. Therefore, to improve the performance of the TFBCC-M features the cough sounds are used for determining the optimum values of \u03b4, f l and f h by using FF Algorithm.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Performance evaluation using TFBCC-M features"
        },
        {
            "text": "The relationship between the best cost (k fold loss) and the number of iterations obtained from the simulation study is shown in Fig. 9 . The associated parameters of the FF algorithm used in the simulation study are \u03b3 = 0 . 2 , \u03b2 0 = 1 , \u03d1 = 0 . 98 and the population size is assumed to be 50. These parameters have been chosen based on trial and error which provides the least k-fold loss at the output of the classifier. The objective function to be minimized is given in Equation (25) . The three attributes \u03b4, f l and f h are associated as a member of the population of the FF algorithm. The light intensity value affects the k-fold loss for a given range of \u03b4, f l and f h . The attractiveness is governed by light intensity variation with distance and the absorption coefficient. It is expressed in Equation (18) . The minimization of k-fold loss continues using the FF algorithm until it attains the best possible minimum value. After obtaining the satisfactory convergence, the best member of the population provides the optimized values of \u03b4, f l and f h .",
            "cite_spans": [
                {
                    "start": 484,
                    "end": 488,
                    "text": "(25)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 815,
                    "end": 819,
                    "text": "(18)",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [
                {
                    "start": 129,
                    "end": 135,
                    "text": "Fig. 9",
                    "ref_id": "FIGREF11"
                }
            ],
            "section": "finding the optimum values of \u03b4, f l and f h using FF algorithm"
        },
        {
            "text": "In this section, the detection performance using the proposed Cepstral Coefficients 11 . Comparison of the F-2 score and average p-value using different feature sets. [23] for cough sounds. The effectiveness of the proposed C-19CC features is illustrated using different measures such as validation accuracy, sensitivity, F-2 score and Kruskal-Wallis tests for the two databases.",
            "cite_spans": [
                {
                    "start": 167,
                    "end": 171,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [
                {
                    "start": 84,
                    "end": 86,
                    "text": "11",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "The better classification performance demonstrates the efficacy of the proposed method. The optimization of MFCC features has been previously used in the area of spoken language recognition [13] speaker-independent emotion recognition [18] , source recognition of Cell-Phones [19] . In this current research work, betteroptimized MFCC features are obtained and used for efficient detection of COVID-19. An important advantage of using C-19CC features as the problem of feature selection or reduction is avoided. In traditional speech recognition tasks, several spectral, cepstral, temporal, and wavelet features are obtained and combined to form the desired feature vector which is a tedious and time-consuming process. On the other hand, the proposed C-19CC features are easy to generate and potential in performance. In Figs. 10 and 11 , the comparison of various performance measures using different feature sets is made for the combined cough category of sounds using the relevant data of the two databases. It is observed that the sensitivity is high because of the lower FN values. This is very important in the case of medical data classification. The same is also evident from the plot of the F-2 Score and validation accuracy. To further justify the effectiveness of the proposed technique, the p-value comparative analysis is carried out for standard MFCC and C-19 CC features as inputs. The average comparative performance plots are presented in Figure 10 . The average p-value is observed to be quite low. The lower the p-value, the better is the effectiveness of that feature. The average p-values obtained for all the 13 coefficients of C-19CC are quite low and thus are suitable features for the COVID-19 classification. The validation accuracy of different audio features is plotted in Fig. 10 . It is noticed that the C-19CC features-based model outperforms the other input feature-based models. This justifies the effectiveness of the suggested features. The F-2 scores of the individual categories of both databases are listed in Table 3 using the two databases.",
            "cite_spans": [
                {
                    "start": 190,
                    "end": 194,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 235,
                    "end": 239,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 276,
                    "end": 280,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                }
            ],
            "ref_spans": [
                {
                    "start": 822,
                    "end": 837,
                    "text": "Figs. 10 and 11",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1457,
                    "end": 1466,
                    "text": "Figure 10",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 1802,
                    "end": 1809,
                    "text": "Fig. 10",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 2049,
                    "end": 2056,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "It is observed from Table 3 that in the category of cough sounds, the proposed C-19 CC provides the highest F-2 scores of 0.851 and 0.741 for the database-2 and database-1 respectively. While in the Vowel category, the E sound TQ-M features perform the best and the /e/ vowel is providing superior performance compred to /o/ and /a/. Similarly, for the counting case, the T-S features exhibit the best performance and in the breathing category, the T-M and D-M features are better than the others. The validation accuracy is also found to be the highest for the category of cough sound using the proposed C-19 CC features. Considering all the categories and databases, the cough sound is found to be the best one which provides the highest accuracy of detection as well as the highest F-2 score employing the C-19CC features. The acoustic properties of the coughing signal are different from the speech signal in terms of bandwidth and the way of perception of sound. The application of the additional speech enhancement block has further improved the detection performance of the proposed model. Additionally, the use of the ADASYN tool for removing the class imbalance in both the databases has improved the classification performance because it helps in identifying the better features. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 20,
                    "end": 27,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Discussion"
        },
        {
            "text": "The detection of COVID-19 using speech signal can serve as an important cost-effective tool as it does involve any complicated medical test. This approach can easily diagnose the preliminary condition of a patient even without visiting a hospital and without the help of any medical staff as it serves as an automatic detection tool. In this paper, a new audio feature called C-19CC is proposed and used for detection of COVID-19 in this paper and the performance of the method is tested using two standard speech databases. The proposed model has been demonstrated to be superior to other existing speech based COVID-19 detection model reported in the literature. However, it is suggested that the detection accuracy need to be ascertained by appropriate medical experts. The performance can be further be increased by combining the new C-19CC with other temporal and statistical features. The proposed method and the combination of features can also be applied for detection of other speech related diseases.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "The proposed C-19CC features are based on the selection of the best possible conversion scale and frequency range of the Cepstral filter bank by using the bio-inspired technique. This is achieved by Identification of the appropriate sound patterns to efficiently detect COVID-19 and application of the speech enhancement schemes for the improvement of the classification performance. In this paper, a simple SVM based classifier is used for detection purpose. However, the classification accuracy can further be improved by using deep learning-based techniques.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "The attributes given in the dataset are breath, cough and voiced vowel sounds. Moreover, the analysis can be extended to study the phonetic relevance and identification of phonemic grouping of speech based COVID-19 detection. For this study there is a requirement of preparation of the phonetically balanced dataset of COVID-19. The optimization method of the filter bank parameters can also be extended to different mechanical applications of cepstral analysis, where the properties of the input signal is quite different from that of standard human speech signals. There is a scope for further work to reduce computational complexities associated with this method so that it may be suitable for the real-life application using FPGA [8] .",
            "cite_spans": [
                {
                    "start": 734,
                    "end": 737,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "All authors declare that there is no conflict of interest in this work.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Declaration of Competing Interest"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "COVID-19 Infection: origin, transmission, and characteristics of human coronaviruses",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Shereen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Khan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kazmi",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Bashir",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Siddique",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "J. Adv. Res",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "WHO Coronavirus Disease (COVID-19) Dashboard Data",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "The efficacy of social distance and ventilation effectiveness in preventing COVID-19 transmission",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhai",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Sustainable cities and society",
            "volume": "62",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "More than virus, fear of stigma is stopping people from getting tested: Doctors, 2020",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "An early study on intelligent analysis of speech under COVID-19: severity",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Qian",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Koike",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "sleep quality, fatigue, and anxiety",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Hilbert spectrum analysis for automatic detection and evaluation of Parkinson's speech",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Karan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Sahu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Orozco-Arroyave",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mahto",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Biomed. Signal Process. Control",
            "volume": "61",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Automatic speech analysis for the assessment of patients with predementia and Alzheimer's disease, Alzheimer's & Dementia: Diagnosis",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "K\u00f6nig",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Satt",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sorin",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Hoory",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Toledo-Ronen",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Derreumaux",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Manera",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Verhey",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Aalten",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "H"
                    ],
                    "last": "Robert",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Assessment & Disease Monitoring",
            "volume": "1",
            "issn": "1",
            "pages": "112--124",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Exploring automatic diagnosis of COVID-19 from crowdsourced respiratory sound data",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Brown",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chauhan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Grammenos",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hasthanasombat",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Spathis",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Xia",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Cicuta",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Mascolo",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2006.05919"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "An overview on audio, signal, speech, & language processing for COVID-19",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Deshpande",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Schuller",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.08579"
                ]
            }
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "An early study on intelligent analysis of speech under COVID-19: severity",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Han",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Qian",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Song",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Koike",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "sleep quality, fatigue, and anxiety",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "From frequency to quefrency: a history of the cepstrum",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "V"
                    ],
                    "last": "Oppenheim",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "W"
                    ],
                    "last": "Schafer",
                    "suffix": ""
                }
            ],
            "year": 2004,
            "venue": "IEEE Signal Process. Mag",
            "volume": "21",
            "issn": "5",
            "pages": "95--106",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Linear discriminant analysis F-Ratio for optimization of TESPAR & MFCC features for speaker recognition",
            "authors": [
                {
                    "first": "K",
                    "middle": [
                        "A"
                    ],
                    "last": "Sheela",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "S"
                    ],
                    "last": "Prasad",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "J. Multimed",
            "volume": "2",
            "issn": "6",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Optimizing acoustic features for source cell-phone recognition using speech signals",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hanil\u00e7i",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Ertas",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "Proceedings of the first ACM workshop on Information hiding and multimedia security",
            "volume": "",
            "issn": "",
            "pages": "141--148",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Auditory model-based design and optimization of feature vectors for automatic speech recognition",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Chatterjee",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "B"
                    ],
                    "last": "Kleijn",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "IEEE Trans. Audio Speech Lang. Process",
            "volume": "19",
            "issn": "6",
            "pages": "1813--1825",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "A heterogeneous speech feature vectors generation approach with hybrid hmm classifiers",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Kadyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mantri",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "K"
                    ],
                    "last": "Aggarwal",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Int. J. Speech Technol",
            "volume": "20",
            "issn": "4",
            "pages": "761--769",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Acoustic feature optimization based on F-ratio for robust speech recognition",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "IEICE Trans. Inf. Syst",
            "volume": "93",
            "issn": "9",
            "pages": "2417--2430",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Filterbank optimization for robust ASR using GA and PSO",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "K"
                    ],
                    "last": "Aggarwal",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dave",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Int. J. Speech Technol",
            "volume": "15",
            "issn": "2",
            "pages": "191--201",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Improved filter bank on multitaper framework for robust punjabi-ASR system",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Kadyan",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Mantri",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "K"
                    ],
                    "last": "Aggarwal",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Int. J. Speech Technol",
            "volume": "23",
            "issn": "1",
            "pages": "87--100",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kou",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Shang",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Lane",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Chong",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "7130--7134",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Automatically discriminating and localizing COVID-19 from community-acquired pneumonia on chest X-rays",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Xiao",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Hou",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Pattern Recognit",
            "volume": "110",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Automatic COVID-19 lung infected region segmentation and measurement using CT-scans images",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Oulefki",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Agaian",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Trongtirakul",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "K"
                    ],
                    "last": "Laouar",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Pattern Recognit",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Social group optimization-assisted Kapur's entropy and morphological segmentation for automated detection of COVID-19 infection from computed tomography images",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Dey",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Rajinikanth",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "J"
                    ],
                    "last": "Fong",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "S"
                    ],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Mahmud",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Cognit. Comput",
            "volume": "12",
            "issn": "5",
            "pages": "1011--1023",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Coswara-A database of breathing, cough, and voice sounds for COVID-19 diagnosis",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sharma",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Krishnan",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ramoji",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "R"
                    ],
                    "last": "Chetupalli",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "K"
                    ],
                    "last": "Ghosh",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ganapathy",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:2005.10548"
                ]
            }
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Spectral-domain speech enhancement for speech recognition",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "H"
                    ],
                    "last": "You",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Bin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Speech Commun",
            "volume": "94",
            "issn": "",
            "pages": "30--41",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "2020 IEEE International Conference on Systems, Man, and Cybernetics",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "3667--3672",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Deep learning for image-based cancer detection and diagnosis-A survey",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Tang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Pattern Recognit",
            "volume": "83",
            "issn": "",
            "pages": "134--149",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "An overview on data representation learning: from traditional feature learning to recent deep learning",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Zhong",
                    "suffix": ""
                },
                {
                    "first": "L.-N",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Ling",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "The Journal of Finance and Data Science",
            "volume": "2",
            "issn": "4",
            "pages": "265--278",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "A five-layer deep convolutional neural network with stochastic pooling for chest CT-based COVID-19 diagnosis",
            "authors": [
                {
                    "first": "Y.-D",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "C"
                    ],
                    "last": "Satapathy",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "G.-R",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Mach. Vis. Appl",
            "volume": "32",
            "issn": "1",
            "pages": "1--13",
            "other_ids": {}
        },
        "BIBREF28": {
            "ref_id": "b28",
            "title": "Deep transfer learning-based automated detection of COVID-19 from lung CT scan slices",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ahuja",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "K"
                    ],
                    "last": "Panigrahi",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Dey",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Rajinikanth",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "K"
                    ],
                    "last": "Gandhi",
                    "suffix": ""
                }
            ],
            "year": 2021,
            "venue": "Applied Intelligence",
            "volume": "51",
            "issn": "1",
            "pages": "571--585",
            "other_ids": {}
        },
        "BIBREF29": {
            "ref_id": "b29",
            "title": "Cepstral analysis of voice in young adults",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "S"
                    ],
                    "last": "Sujitha",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [
                        "K"
                    ],
                    "last": "Pebbili",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Journal of Voice",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF30": {
            "ref_id": "b30",
            "title": "Multiclass classification of Parkinson's disease using cepstral analysis",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Benmalek",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Elmhamdi",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Jilbab",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Int. J. Speech Technol",
            "volume": "21",
            "issn": "1",
            "pages": "39--49",
            "other_ids": {}
        },
        "BIBREF31": {
            "ref_id": "b31",
            "title": "Speech processing, transmission and quality aspects (STQ); distributed speech recognition; advanced front-end feature extraction algorithm; compression algorithms",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "S"
                    ],
                    "last": "Doc",
                    "suffix": ""
                }
            ],
            "year": 2002,
            "venue": "ETSI ES",
            "volume": "202",
            "issn": "050",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF32": {
            "ref_id": "b32",
            "title": "Speech emotion recognition using cepstral features extracted with novel triangular filter banks based on bark and ERB frequency scales",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sugan",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "S S"
                    ],
                    "last": "Srinivas",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "S"
                    ],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "K"
                    ],
                    "last": "Nath",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kanhe",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Digit. Signal Process",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF33": {
            "ref_id": "b33",
            "title": "Performance comparison of different cepstral features for speech emotion recognition",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Sugan",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "S"
                    ],
                    "last": "Srinivas",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Kar",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "S"
                    ],
                    "last": "Kumar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "K"
                    ],
                    "last": "Nath",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kanhe",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "2018 International CET Conference on Control, Communication, and Computing (IC4)",
            "volume": "",
            "issn": "",
            "pages": "266--271",
            "other_ids": {}
        },
        "BIBREF34": {
            "ref_id": "b34",
            "title": "Parkinson disease prediction using intrinsic mode function based features from speech signal",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Karan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Sahu",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mahto",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Biocybernetics and Biomedical Engineering",
            "volume": "40",
            "issn": "1",
            "pages": "249--264",
            "other_ids": {}
        },
        "BIBREF35": {
            "ref_id": "b35",
            "title": "Learning representations of sound using trainable COPE feature extractors",
            "authors": [
                {
                    "first": "N",
                    "middle": [],
                    "last": "Strisciuglio",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Vento",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Petkov",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Pattern Recognit",
            "volume": "92",
            "issn": "",
            "pages": "25--36",
            "other_ids": {}
        },
        "BIBREF36": {
            "ref_id": "b36",
            "title": "ADASYN: Adaptive synthetic sampling approach for imbalanced learning",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bai",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "A"
                    ],
                    "last": "Garcia",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "IEEE international joint conference on neural networks",
            "volume": "",
            "issn": "",
            "pages": "1322--1328",
            "other_ids": {}
        },
        "BIBREF37": {
            "ref_id": "b37",
            "title": "Firefly algorithms for multimodal optimization",
            "authors": [
                {
                    "first": "X.-S",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "International symposium on stochastic algorithms",
            "volume": "",
            "issn": "",
            "pages": "169--178",
            "other_ids": {}
        },
        "BIBREF38": {
            "ref_id": "b38",
            "title": "Firefly algorithm: recent advances and applications",
            "authors": [
                {
                    "first": "X.-S",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "1",
            "issn": "",
            "pages": "36--50",
            "other_ids": {}
        },
        "BIBREF39": {
            "ref_id": "b39",
            "title": "Improved phase aware speech enhancement using bio-inspired and ANN techniques",
            "authors": [
                {
                    "first": "T",
                    "middle": [
                        "K"
                    ],
                    "last": "Dash",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Solanki",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Panda",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Analog Integr. Circuits Signal Process",
            "volume": "",
            "issn": "",
            "pages": "1--13",
            "other_ids": {}
        },
        "BIBREF40": {
            "ref_id": "b40",
            "title": "Support vector machines (SVM) as a technique for solvency analysis",
            "authors": [
                {
                    "first": "L",
                    "middle": [],
                    "last": "Auria",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Moro",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF41": {
            "ref_id": "b41",
            "title": "Hilbert spectrum analysis for automatic detection and evaluation of Parkinson's speech",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Karan",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "S"
                    ],
                    "last": "Sahu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "R"
                    ],
                    "last": "Orozco-Arroyave",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mahto",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Biomed. Signal Process Control",
            "volume": "61",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF42": {
            "ref_id": "b42",
            "title": "Noise driven short-time phase spectrum compensation procedure for speech enhancement, Ninth annual conference of the international speech communication association",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "P"
                    ],
                    "last": "Stark",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "K"
                    ],
                    "last": "W\u00f3jcicki",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "G"
                    ],
                    "last": "Lyons",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [
                        "K"
                    ],
                    "last": "Paliwal",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF43": {
            "ref_id": "b43",
            "title": "Learning from imbalanced data",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "A"
                    ],
                    "last": "Garcia",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "IEEE Trans. Knowl. Data Eng",
            "volume": "21",
            "issn": "9",
            "pages": "1263--1284",
            "other_ids": {}
        },
        "BIBREF44": {
            "ref_id": "b44",
            "title": "Points of significance: classification evaluation",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lever",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Krzywinski",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Altman",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF45": {
            "ref_id": "b45",
            "title": "Reference standards for next-generation sequencing",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "A"
                    ],
                    "last": "Hardwick",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "W"
                    ],
                    "last": "Deveson",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "R"
                    ],
                    "last": "Mercer",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Nat. Rev. Genet",
            "volume": "18",
            "issn": "8",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF46": {
            "ref_id": "b46",
            "title": "Unbalanced breast cancer data classification using novel fitness functions in genetic programming",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Devarriya",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Gulati",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Mansharamani",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Sakalle",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Bhardwaj",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Expert Syst. Appl",
            "volume": "140",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF47": {
            "ref_id": "b47",
            "title": "Diagnosis of Parkinson disease using the wavelet transform and MFCC and SVM classifier",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Soumaya",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "D"
                    ],
                    "last": "Taoufiq",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Nsiri",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Abdelkrim",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "2019 4th World Conference on Complex Systems (WCCS)",
            "volume": "",
            "issn": "",
            "pages": "1--6",
            "other_ids": {}
        },
        "BIBREF48": {
            "ref_id": "b48",
            "title": "A comparative analysis of speech signal processing algorithms for Parkinson's disease classification and the use of the tunable Q-factor wavelet transform",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "O"
                    ],
                    "last": "Sakar",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Serbes",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Gunduz",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "C"
                    ],
                    "last": "Tunc",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Nizam",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "E"
                    ],
                    "last": "Sakar",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Tutuncu",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Aydin",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "E"
                    ],
                    "last": "Isenkul",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Apaydin",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Appl. Soft Comput",
            "volume": "74",
            "issn": "",
            "pages": "255--263",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Steps for the MFCC feature extraction.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Step-5 -FInd the logarithm of Step 4 \u2022 Step-6 -Apply the Discrete cosine transform \u2022 Step-7 -Combine Energy and other features from step-6 to get MFCC features",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Convert linear scale frequency of DFT ( f ) to Mel scale ( f m ) using Equation (1) .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Steps for the Filter bank computation. The magnitude response of each of the filters H p (k ) in the mel filter bank is calculated using Equation(4) .",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Block Diagram of Proposed detection model of COVID-19.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "uniformly distributed zero mean random numbers in the range of -0.5 to 0.Flow Chart of Fire Fly Algorithm.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Effect of change in the Delta value and maximum frequency on the Validation Accuracy.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "Step-1 -Calculate the degree of class imbalance ( d ) from the given training data\u2022 Step-2 -Generate the number of new data samples (synthetic data = G ) for the minority class \u2022 Step-3 -Calculate the r i from the K nearest neighbors calculation and normalize it. (where r i = i K , where i is the number of examples in the K nearest neighbors of x i (feature vector of the i th sample) that belong to the majority class) \u2022",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Flow Chart of Improved phase aware speech enhancement Scheme.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF9": {
            "text": "Performance analysis of the TFBCC-M features.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF10": {
            "text": "Comparison of the performance of standard MFCC features for databse-1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF11": {
            "text": "The learning characteristics obtained during the optimization of FF algorithm.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF12": {
            "text": "(C-19CC) is compared with another seven types of audio features such as TFBCC-M features (T-M), TFBCC-B (T-B) features, TFBCC-H (T-H) features, TFBCC-E features (T-E) [33] , DWT based MFCC Features (D-M) [48] , TQWT based MFCC Features (T-M) [49] , and Temporal and Spectral acoustic features (T-S) Fig. 10. Comparison of the validation accuracy and sensitivity using different feature sets.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF13": {
            "text": "Fig. 11. Comparison of the F-2 score and average p-value using different feature sets.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Parameters used in the FF Algorithm.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "Performance Comparison of the different categories of sounds using 8 different feature sets.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The authors express their gratitude to Professor Cecilia Mascolo, Department of Computer Science and Technology and Chancellor, Master and Scholar of the University of Cambridge of the Old Schools Trinity Lane, Cambridge CB2 1TN, UK for sharing the speech database of COVID19 sound App of the paper published in ACM KDD [8] .",
            "cite_spans": [
                {
                    "start": 320,
                    "end": 323,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": "Acknowledgment"
        }
    ]
}