INTRODUCTION

Keeping up with scientific developments on COVID-19 highlights the perennial problem of information overload in a high-stakes domain. At the time of writing, hundreds of thousands of research papers have been published concerning COVID-19 and the SARS-CoV-2 virus. For biomedicine more generally, the PubMed 1 service adds 4,000 papers every day and over a million papers every year. While progress in general search has been made using sophisticated machine learning methods, such as neural retrieval models, vertical search is often limited to comparatively simple keyword search augmented by domain-specific ontologies (e.g., entity acronyms). The PubMed search engine exemplifies this experience. Direct supervision, while available for general search in the form of relevance labels from click logs, is typically scarce in specialized domains, especially for emerging areas such as COVID-related biomedical search.Self-supervised learning has emerged as a promising direction to overcome the annotation bottleneck, based on automatically creating noisy labeled data from unlabeled text. In particular, neural language model pretraining, such as BERT [8] , has demonstrated superb performance gains for general-domain information retrieval [21, 27, 45, 46] and natural language processing (NLP) [39, 40] . Additionally, for specialized domains, domain-specific pretraining has proven to be effective for in-domain applications [1, 3, 11, 12, 15, 20, 34] .We propose a general methodology for developing vertical search systems for specialized domains. As a case study, we focus on biomedical search. We find evidence that the methods have significant impact in the target domain, and, likely generalize to other vertical search domains. We demonstrate how advances described in earlier and related work [11, 44, 48] can be brought together to provide new capabilities. We also provide data supporting the feasibility of a large-scale deployment through detailed system analysis, stress-testing of the system, and acquisition of expert relevance evaluations. 2 In section 2, we explore the key idea of initializing a neural ranking model with domain-specific pretraining and fine-tuning the model on a self-supervised domain-specific dataset generated from general query-document pairs (e.g., from MS MARCO [26] ). Then, we introduce the biomedical domain as a case study. In section 3, we evaluate the method on the TREC-COVID dataset [30, 38] . We find that the method performs comparably or better than the best systems in the official TREC-COVID evaluation, despite its generality and simplicity, and despite using zero COVID-related relevance labels for direct supervision. In section 4, we discuss how our system design leverages distributed computing and modern cloud infrastructure for scalability and ease of use. This approach can be reused for other domains. In the biomedical domain, our system can scale to tens of millions of PubMed articles and attain a high query-per-second (QPS) throughput. We have deployed the resulting system for preview as Microsoft Biomedical Search, which provides a new search experience over biomedical literature: https://aka.ms/biomedsearch.

DOMAIN-SPECIFIC PRETRAINING FOR VERTICAL SEARCH

In this section, we present a general approach for vertical search based on domain-specific pretraining and self-supervised learning ( Figure 1 ). We first review neural language models and show how domain-specific pretraining can serve as the foundation for a domain-specific document neural ranker. We then present a general method of fine-tuning the ranker by using self-supervised, domainspecific relevance labels from a broad-coverage query-document dataset using the domain ontology. Finally, we show how this approach can be applied in biomedical literature search.

Domain-Specific Pretraining

Language model pretraining can be considered a form of taskagnostic self-supervision that generates training examples by hiding words from unlabeled text and tasks the model with predicting the hidden words. In our work on vertical search, we adopt the popular Bidirectional Encoder Representations from Transformers (BERT) [8] , which has become a standard building block for NLP applications. Instead of predicting the next token based on the preceding tokens, as in traditional generative models, BERT employs a Masked Language Model (MLM), which randomly replaces a subset of tokens by a special token [ ], and tries to predict them from the rest of the words. The training objective is the cross-entropy loss between the original tokens and the predicted ones. BERT builds on the transformer model [37] with its multi-head self-attention mechanism, which has demonstrated high performance in parallel computation and modeling long-range dependencies, as compared to recurrent neural networks such as LSTM [13] . The input consists of text spans, such as sentences, separated by a special token [ ]. To address out-of-vocabulary words, tokens are divided into subword units using Byte-Pair Encoding (BPE) [33] or its variants [18] , which generates a fixed-size subword vocabulary to compactly represent the training text corpora. The input is first passed to a lexical encoder, which combines the token embedding, position embedding, and segment embedding by element-wise summation. The embedding layer is then passed to multiple layers of transformer modules to generate a contextual representation [37] .Prior pretraining efforts have focused frequently on the newswire and web domains. For example, the BERT model was trained on Wikipedia 3 and BookCorpus [49] , and subsequent efforts have focused on crawling additional web text to conduct increasingly large-scale pretraining [6, 23, 29] . For domain-specific applications, pretraining on in-domain text has been shown to provide additional gains, but the prevalent assumption is that out-domain text is still helpful and pretraining typically adopts a mixed-domain approach [12, 20] . Gu et al. [11] changes this assumption and shows that, for domains with ample text, a pure domain-specific pretraining approach is advantageous and leads to substantial gains in downstream in-domain applications. We adopt this approach by generating domain-specific vocabulary and performing language model pretraining from scratch on in-domain text [11] .

Self-Supervised Fine-Tuning

As a first-order approximation, the search problem can be abstracted as learning a relevance function for query and text span : ( , ) → {0, 1}. Here, may refer to a document or arbitrary text span such as a passage.Traditional search methods adopt a sparse retrieval approach by essentially treating the query as a bag of words and matching each word against the candidate text, which can be done efficiently using an inverted index. Individual words are weighted (e.g., by TF-IDF) to downweight the effect of stop words or function words, as exemplified by BM25 and its variants [31] .Variations abound in natural language expressions, which can cause significant challenges in sparse retrieval. To address this problem, dense retrieval maps query and text each to a vector in a continuous representation space and estimates relevance by computing the similarity between the two vectors (e.g., via dot product) [16, 17, 45] . Dense retrieval can be made highly scalable by pre-computing text vectors, and can potentially replace or combine with sparse retrieval.Neither sparse retrieval nor dense retrieval attempts to model complex interdependencies between the query and text. In contrast, sophisticated neural approaches concatenate query and text as input for a BERT model to leverage cross-attention among query and text tokens [47] . Specifically, query and text are combined] is a special token to be used for final prediction [8] . This could produce significant performance gains but requires a large amount of labeled data for fine-tuning the BERT model. Such a cross-attention neural model will not be scalable enough for the retrieval step, as we must compute, from scratch, for each candidate text with a new query. The standard practice thus adopts a two-stage approach, by using a fast L1 retrieval method to select top text candidates, and applying the neural ranker on these candidates as L2 reranking.In our proposed approach, we use BM25 for L1 retrieval, and initialize our L2 neural ranker with a domain-specific BERT model. To fine-tune the neural ranker, we use the Microsoft Machine Reading Comprehension dataset, MS MARCO [26] , and a domainspecific lexicon to generate noisy relevance labels at scale using self-supervision ( Figure 1 ). MS MARCO was created by identifying pairs of anonymized queries and relevant passages from Bing's search query logs, and crowd-sourcing potential answers from passages. The dataset contains about one million questions spanning a wide range of topics, each with corresponding relevant answer passages from Bing question answering systems. For self-supervised fine-tuning labels, we use the MS MARCO subset [24] whose queries contain at least one domain-specific term from the domain ontology.

Application to Biomedical Literature Search

Biomedicine is a representative case study that illustrates the challenges of vertical search. It is a high-value domain with a vast and rapidly growing research literature, as evident in PubMed (30+ million articles; adding over a million a year). However, existing biomedical search tools are typically limited to sparse retrieval methods, as exemplified by PubMed. This search is primarily limited to keyword matching, though it is augmented with limited query expansion using domain ontologies (e.g., MeSH terms [22] ). This method is suboptimal for long queries expressing complex intent.We use biomedicine as a running example to illustrate our approach for vertical search. We leverage PubMed articles for domainspecific pretraining and use the publicly-available PubMedBERT [11] to initialize our L2 neural ranker. For self-supervised fine-tuning, we use the Unified Medical Language System (UMLS) [5] as our domain ontology and filter MS MARCO queries using the disease or syndrome terms in UMLS, similar to MacAvaney et al. [24, 25] but focusing on the broad biomedical literature rather than COVID-19. This medical subset of MS MARCO contains about 78 thousand annotated queries. We used these queries and their relevant passages in MS MARCO as positive relevance labels. To generate negative labels, we ran BM25 for each query over all non-relevant passages in MS MARCO, and selected the top 100 results. This forces the neural ranker to work harder in separating truly relevant passages from ones with mere overlap in keywords. For balanced training, we down-sampled negative instances to equal the number of positive instances (i.e., 1:1 ratio). This resulted in about 640 thousand (query, passage, label) examples.Based on preliminary experiments, we chose a learning rate of 2 − 5 and ran fine-tuning for one epoch in all subsequent experiments. We found that the results are not sensitive to hyperparameters, as long as the learning rate is of the same order of magnitude and at least one epoch is run over all the examples. At retrieval time, we used = 60 in the L1 ranker by default (i.e., we used BM25 to select top 60 text candidates).

CASE STUDY EVALUATION ON COVID-19 SEARCH

The COVID-19 literature provides a realistic test ground for biomedical search. In a little over a year, the COVID-related biomedical literature has grown to include over 440 thousand papers that mention COVID-19 or the SARS-CoV-2 virus. This explosive growth sparked the creation of the COVID-19 Open Research Dataset (CORD-19) [43] and subsequently TREC-COVID [30, 38] , an evaluation resource for pandemic information retrieval.In this section, we describe our evaluation of the biomedical search system on TREC-COVID, focusing on two key questions. First, how does our system perform compared to the best systems participating in TREC-COVID? We note that many of these systems are expected to have complex designs and/or require COVID-related relevance labels for training and development. Second, what is the impact of domain-specific pretraining compared to general-domain or mixed-domain pretraining?

The TREC-COVID Dataset

To create TREC-COVID, organizers from the National Institute of Standards and Technology (NIST) used versions of CORD-19 from April 10 (Round 1), May 1 (Round 2), May 19 (Round 3), June 19 (Round 4), and July 16 (Round 5). These datasets spanned an initial set of 30 topics with five new topics planned for each additional round; the final set thus consists of 50 topics and cumulative judgements from previous rounds generated by domain experts [30] . Relevance labels were created by annotators using a customized platform and released in rounds. Round 1 contains 8,691 relevance labels for 30 topics, and was provided to participating teams for training and development. Subsequent rounds were hosted to introduce additional topics and relevance labels as a rolling evaluation for increased participation. We use Round 2, the round we participated in, to evaluate our system development. It contains 12,037 relevance labels for 35 topics.

Top Systems in TREC-COVID Leaderboard

The results of TREC-COVID Round 2 are organized into three groups: Manual, which used manual interventions, e.g., manual query rewriting, in any part of the system, Feedback, which used labels from Round 1, and Automatic, which does not use manual effort or Round 1 labels. 4 Note that the categorization of Feedback and Automatic is not always explicit so their grouping might be mixed. Overall, 136 systems participated in the official evaluation. NDCG@10 was used as the main evaluation metric, with Precision@5 (P@5) reported as an additional metric. The best performing systems typically adopted a sophisticated neural ranking pipeline and performed extensive training and development on TREC-COVID labeled data from Round 1. Some systems also use very large pretrained language models. For example, covidex.t5 used T5 Large [29] , a general-domain transformer-based model with 770 million parameters pretrained on the Colossal Clean Crawled (C4) web corpus (26 TB). 5 The best performing non-manual system for Round 2 is CMT (CMU-Microsoft-Tsinghua) [44] , which adopted a two-stage ranking approach. For L1, CMT used standard BM25 sparse retrieval as well as dense retrieval by fusing top ranking results from the two methods. The dense retrieval method computed the dot product of query and passage embeddings based on a BERT model [17] . For L2, CMT used a neural ranker with cross-attention over query and candidate passage.For training, CMT started with the same biomedical MS MARCO data (by selecting MS MARCO queries with biomedical terms) [24] , but then applied additional processing to generate synthetic labeled data. Briefly, it first trained a query generation system using query generation (QG) [27] on the query-passage pairs from biomedical MS MARCO, initialized by GPT-2 [28] . Given this trained QG system, for each COVID-related document , it generated a pseudo query = ( ), and then applied BM25 to retrieve a pair of documents with high and low ranking, ′ + , ′ − . Finally, it called on ContrastQG [44] to generate a query that would best differentiate the two documents ′ = ( ′ + , ′ − ). For the neural ranker, CMT started with SciBERT [3] with continual pretraining on CORD-19, and fine-tuned the model using both Med MARCO labels and synthetic labels from ContrastQG.To leverage the TREC-COVID data from Round 1, CMT incorporated data reweighting (ReinfoSelect) based on the REINFORCE algorithm [48] . It used performance on Round 1 data as a reward signal, and learned to denoise training labels by re-weighting them using policy gradient.

Our Approach on TREC-COVID

TREC-COVID offers an excellent benchmark for assessing the general applicability of our proposed approach for vertical search. We evaluated our systems on the test set (Round 2) and compared them with the best systems in the official TREC-COVID evaluation. We Table 2 : Comparison of domain-specific (PubMedBERT and PubMedBERT-COVID) pretraining with out-domain (BERT, RoBERTa, UniLM) or mixed-domain pretraining (SciBERT) in TREC-COVID test results (Round 2). All results were averaged from ten runs (standard deviation in parentheses). Domain-specific pretraining is essential for attaining good performance in our general approach for vertical search.essentially took the biomedical search system from subsection 2.3 as is (PubMedBERT). Although COVID-related text may differ somewhat from general biomedical text, we expect that a biomedical model should offer strong performance for this subset of biomedical literature. To further assess the impact from domain-specific pretraining, we also conducted continual pretraining using CORD-19 for 100K BERT steps and evaluated it in our biomedical search system (PubMedBERT-COVID). Table 1 shows the results. Surprisingly, without using any relevance labels, our systems (top panel) performs competitively against the best systems in TREC-COVID evaluation. E.g., PubMedBERT-COVID outperforms covidex.t5 by over three absolute points in NDCG@10, even though the latter used a much larger language model pretrained on three orders of magnitude more data (26TB vs 21GB). Our systems were trained using one epoch with a fixed learning rate (2e-5). By exploring longer training (up to five epochs) and multiple learning rates (1e-5, 2e-5, 5e-5) and using Round 1 as dev set, our best system (middle panel) performs on par in NDCG@10 with CMT, the top system in TREC-COVID, while requiring no additional sophisticated learning components such as dense retrieval, QG, ContrastQG, and ReinfoSelect. The success of our systems can be attributed primarily to our in-domain language models (PubMedBERT, PubMedBERT-COVID). To further assess the impact of domain-specific pretraining, we also evaluated our system using out-domain and mixed-domain models. See Table 2 for the results. Out-domain language models all perform relatively poorly in this evaluation of biomedical search, and exhibit little difference in search relevance despite significant difference in the size of vocabulary, pretraining corpus, and model (e.g., RoBERTa [23] used a larger vocabulary and both RoBERTa and UniLM [9] were pretrained on much larger text corpus). Pretraining on PubMed text helps SciBERT, but its mixeddomain approach (including compute science literature) inhibits its performance compared to domain-specific pretraining. Continual pretraining on covid-specific literature helps substantially, with PubMedBERT-COVID outperforming PubMedBERT by over four absolute points in NDCG@10. Overall, domain-specific pretraining is essential for the performance gain, with PubMedBERT-COVID outperforming general-domain BERT models by over ten absolute points in NDCG@10.In sum, the TREC-COVID results provide strong evidence that, by leveraging domain-specific pretraining, our approach for vertical search is general and can attain high accuracy in a new domain without significant manual effort.

PUBMED-SCALE BIOMEDICAL SEARCH

The canonical tool for biomedical search is the PubMed search itself. Recently, COVID-19 has spawned a plethora of new prototype biomedical search tools. See Table 3 for a list of representative 6 https://pubmed.ncbi.nlm.nih.gov/ 7 https://covid19search.azurewebsites.net/home/index?q= 8 https://cord-19.apps.allenai.org/ 9 https://covid19-research-explorer.appspot.com/ 10 https://covidex.ai/ 11 https://sfr-med.com/search 12 https://aka.ms/biomedsearch systems. PubMed covers essentially the entire biomedical literature, but its aforementioned search engine is based on relatively simplistic sparse retrieval methods, which generally perform less well, especially in the presence of long queries with complex intent. By contrast, while some new search tools feature advanced neural ranking methods, their search scope was typically limited to CORD-19, which considers only a tiny fraction of biomedical literature. In this section, we describe our effort in developing and deploying Microsoft Biomedical Search, a new biomedical search engine that combines PubMed-scale coverage and state-of-the-art neural ranking, based on our general approach for vertical search, as described in subsection 2.3 and validated in section 3. Creating the system required addressing significant challenges with system design and engineering. Employing a modern cloud infrastructure helped with the fielding of the system. The fielded system can serve as a reference architecture for vertical search in general; many components are directly reusable for other high-value domains.

System Challenges

The key challenge in the system design is to scale to tens of millions of biomedical articles, while enabling affordable and fast computation in sophisticated neural ranking methods, based on large language models with hundreds of millions of parameters.Specifically, the CORD-19 dataset initially covered about 29,000 documents (abstracts or full-text articles) when it was first launched in March 2020. It quickly grew to about 60,000 documents when it was adopted by TREC-COVID (Round 2, May 2020), which is the version used by many COVID-search tools. Even in its latest version (as of early Feb. 2021), CORD-19 only contains about 440,000 documents (with about 150,000 full-text articles). By contrast, PubMed covers over 30 million biomedical publications, with about 20 million abstracts and over 3 million full-text articles, which is two orders of magnitude larger than CORD-19.Given early feedback from a range of biomedical practitioners, in addition to document-level retrieval, we decided to enable passagelevel retrieval to enhance granularity and precision. This further exacerbates our scalability challenge, as the retrieval candidates now include over 216 million paragraphs (passages).Neural ranking methods can greatly improve search relevance compared to standard keyword-based and sparse retrieval methods. However, they present additional challenges as these methods often build upon large pretrained language models, which are computational intensive and generally require expensive graphic processing units (GPUs).

Our Solution

As described in subsection 2.3, we adopt a two-stage ranking model, with an L1 ranker based on BM25 and an L2 reranker based on PubMedBERT. As shown in Figure 2 (left), the system comprises a web front end, web back end API, cache, L1 ranking, and L2 ranking. Query requests are passed on from web front end to back end API, which coordinates L1 and L2 ranking. The system first consults the cache and returns results directly if the query is cached. Otherwise, it calls on L1 to retrieve top candidates and then calls on L2 to conduct neural reranking. Finally, it combines the results and returns them to the front end for display.To address the scalability challenges, we develop our system on top of modern cloud infrastructures to leverage their native capabilities of distributed computing, cache, and load balancing, which drastically simplifies our system design and engineering. We choose to use Microsoft Azure as the cloud infrastructure, but our design is general and can be easily adapted to other cloud infrastructures.In early experiments, we found that the Web front end, back end and cache components are sufficiently fast. So, in what follows, we will focus on discussing how to address scalability challenges in L1 and L2 ranking.For L1, we use BM25, which can be supported by standard inverted index methods. We adopt Elastic Search, an open-source distributed search engine built on Apache Lucene [10] . Given our PubMed-scale coverage, the index size of Elastic Search is over 160GB and is growing as new papers arrive. The index size further multiplies with the number of replications added to ensure system availability (we use two replications). As such, we need to use machines with enough memory and processing power.For L2, although we only run on limited number of candidate passages from L1 (we used top 60 in our system), the neural ranking model is based on large pretrained language models which are computationally intensive. Currently, we use the base model of Pub-MedBERT with 12 layers of transformer modules, containing over 300 million parameters. We thus use a distributed GPU cluster and make careful hardware and software choices to maximize overall cost-effectiveness while minimizing L2 latency.We use query-per-second (QPS) as our key workload metric for system design. To identify major bottlenecks and fine-tune design choices, we conducted focused experiments on L1 and L2 rankers separately to assess their impact on run-time latency.We use Locust [14] , a Python-based framework for load testing. To ensure head-to-head comparison among design choices, we adopted a fixed system setup as follows:• The back end API is developed with Flask [32] , using Gevent [4] with 8 workers to ensure the highest performance • To minimize variance due to network cost, the back end API and L1 or L2 rankers are deployed in the same data center, as well as machines used to send queries. • All the servers are deployed in the same virtual network.• We prepare a query set which contains 71 thousand anonymized queries sampled from Microsoft Academic Search. • We turned off the cache layer during all experiments.With this configuration, the latency of back end API per query is around 20 ms. We used the Locust client to simulate asynchronous requests from multiple users. Each simulated user would randomly wait for 15-60 seconds after each search request. Each experiments ran for 10 minutes.From preliminary experiments, we found that Elastic Search requires warm-up to reach maximum performance, so we ran the system with low QPS (0.5 per sec) for 10 • Each query is processed by a main node, which distributes its query to data nodes and then merges the results. • There are three main nodes and ten data nodes, each using a premium machine (D8s v3) with a 1TB SSD disk (P30). • The index is divided into 30 shards.For L2, we used Kubernetes to manage a GPU cluster. See Figure 2 (right) for a reference architecture. We used V100 GPUs in initial experiments. Since they are relatively expensive, we explored using low-cost GPUs in subsequent experiments to maximize cost effectiveness. For each query, to rerank the top 60 candidate paragraphs from L1, it takes about 0.9 second on a V100 GPU. The K80 GPU only costs a fraction of V100, but requires 3 second per query. We therefore used 4-K80 machines, which reduce the latency to 0.75 second but cost less than a third of the cost for V100. Table 4 and Table 5 shows simulated test results for L1 and L2 ranking, respectively. There were no failures in all the tests. For L1 ranking, our configuration can already support 10-20 QPS while keeping latency for most queries to less than a second. To support higher QPS, we can simply add more main and data nodes, which scale roughly linearly. For L2 ranking, our test used 32 4-K80 machines with a total of 128 K80s. It can support about 10 QPS while keeping latency for most queries to around or under a second. To support higher QPS, we can simply add more K80 machines.

Microsoft Biomedical Search

Our biomedical search system has been deployed as Microsoft Biomedical Search, which is publicly available. See Figure 3 for a sample screenshot.Before deployment, we conducted several user studies among the co-authors and our extended teams with a diverse set of selfconstructed and sampled queries. Overall, we verified that our Table 6 : Reference configuration and monthly cost estimate to support expected QPS while keeping median latency under two seconds (based on pricing from June 2021).system performed well for long queries with complex intent, generally returning more relevant results compared to PubMed and other search tools. However, for overly general short queries (e.g., "breast cancer"), our system can be under-selective among articles that all mention the terms. To improve user experience, we augmented L1 ranking by including results from Microsoft Academic, which uses a saliency score that takes into account temporal evolution and heterogeneity of the Microsoft Academic Graph to predict and up-weight influential papers [35, 41, 42] . Given a query, we retrieve top 30 results from Microsoft Academic as ranked by its saliency score and combine them with the top 30 results from BM25. L2 reranking is then conducted over the combined set of results. The saliency score helps elevate important papers when the query is underspecified, which generally leads to a better user experience.In addition to standard search capabilities, our system incorporates a state-of-the-art machine reading comprehension (MRC) method [7] trained on [19] as an optional component. Given a query and a top reranked candidate passage, the MRC component will treat it as a question-answering problem and return a text span in the passage as the answer, if the answer confidence is above the score of abstaining from answering. The MRC component uses the same cloud architecture as the L2 neural ranker Figure 2 (right), with similar latency performance.Our system can be deployed for public release at a rather affordable cost. Table 6 shows the reference configuration and cost estimate to support various expected loads (QPS).

DISCUSSION

Prior work on vertical search tends to focus on domain-specific crawling (focused crawling) and user interface [2] . We instead explore the orthogonal aspect of the underlying search algorithm. These tend to be simplistic in past systems, due to the scarcity of domain-specific relevance labels, as exemplified by the PubMed search engine. While easier to implement and scale, such systems often render subpar search experiences, which is particularly concerning for high-value verticals such as biomedicine. E.g., Soni and Roberts [36] studied the evaluation of commercial COVID-19 search systems and found that "commercial search engines sizably underperformed those evaluated under TREC-COVID. This has implications for trust in popular health search engines and developing biomedical search engines for future health crises."By leveraging domain-specific pretraining and self-supervision from broad-coverage query-passage dataset, we show that it is possible to train a sophisticated neural ranking system to attain high search relevance, without requiring any manual annotation effort. Although we focus on biomedical search as a running example in this paper, our reference system comprises general and reusable components that can be directly applied to other domains. Our approach may potentially help bridge the performance gap in conventional vertical search systems while keeping the design and engineering effort simple and affordable.There are many exciting directions to explore. For example, we can combine our approach with other search engines that take advantage of complementary signals not used in ours. Our hybrid L1 ranker combining BM25 with Microsoft Academic Search saliency scores is an example of such fusion opportunities. A particularly exciting prospect is applying our approach to help improve the PubMed search engine, which is an essential resource for millions of biomedical practitioners across the globe.In the long run, we can also envision applying our approach to other high-value domains such as finance, law, retail, etc. Our approach can also be applied to enterprise search scenarios, to facilitate search across proprietary document collections, which standard search engines are not optimized for. In principle, all it takes is gathering unlabeled text in the given domain to support domain-specific pretraining. If a comprehensive index is not available (as in PubMed for biomedicine), one could leverage focused crawling in traditional vertical search to identify such in-domain documents from the web. In practice, additional challenges may arise, e.g., in self-supervised fine-tuning. Currently, we generate the training dataset by selecting MARCO queries using a domain lexicon. If such a lexicon is not readily available (as in UMLS for biomedicine), additional work is required to identify words most pertinent to the given domain (e.g., by contrasting between general and domain-specific language models). We also rely on MARCO to have sufficient coverage for a given domain. We expect that highvalue domains are generally well represented in MARCO already. For an obscure domain with little representation in open-domain query log, we can fall back to using a general query-document relevance model as a start and invest additional effort for refinement.

CONCLUSION

We described a methodology for developing vertical search capabilities and demonstrate its effectiveness in the TREC-COVID evaluation for COVID-related biomedical search. The generality and efficacy of the approach rely on domain-specific pretraining and self-supervised fine-tuning, which require no annotation effort for applying to a new domain. Using biomedicine as a running example, we present a general reference system design that can scale to tens of millions of domain-specific documents by leveraging capabilities supplied in modern cloud infrastructure. Our system has been deployed as Microsoft Biomedical Search. Future directions include further improvement of self-supervised reranking, combining the core retrieval and ranking services with complementary search methods and resources, and validation of the generality of the methodology by testing the approach in building search systems for other vertical domains.

