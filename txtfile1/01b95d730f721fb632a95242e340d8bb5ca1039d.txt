

There is a fundamental contradiction between the realities of cluster randomized designs (few sites/clusters from which to sample) and the needs for balance in the resulting design (site pairings that lead to comparable groups of sites). For that reason, we used simulation methods as well described in the literature 1,2,3,4 also as "restricted randomization" or "covariate balanced randomization". This approach seeks to achieve balance for each of the specified characteristics of the clusters (schools).In each simulation we assigned the 20 schools at random into 2 groups. Then we tested the resulting groups for differences in group means of enrollment, facility score, and attendance. These were the pre-specified school characteristics needed to balance. We then compared these inter-group differences in means to pre-set constraints. In our case we chose mean difference constraint between schools group of 10 students for enrollment, 3 for facility percent, and 3 for attendance percent. These constraint we regarded as narrow given the range of the distributions (See Table AX1 ). With these constraints on means alone, however, we found in testing that the distributions could differ in range (and variance). For that reason, for both enrollment and for attendance, we added two more limits on the differences in ranges by school group: 50 for enrollment and 5 for attendance. All together, there were 5 constraints.In testing we found that the yield of balanced treatment allocations across schools was 3 in 1000. For that reason, we proceeded with 2000 simulations to be able to achieve at least one balanced allocation of schools.From the resulting set of balanced allocations, we randomly selected one allocation of 20 schools into two groups.We accomplished this simulation-based randomization with a custom-written program in Stata v 15.1 after finding that published alternatives were too slow and insufficiently flexible for our dual task of achieving balanced means and ranges. The final version of randomization proceeded with a program seed to ensure reproducibility. The second wave of public schools used the same approach as Wave #1. There were 8 public schools in this wave. Using the same program and criteria as in the first wave, we used the same program (modified for only 8 schools) to arrive at 4 schools assigned to intervention and 4 assigned to controls.

Wave #3 Charter School Randomization (February 2019):

Owing to the small number of schools, the lack of data on facility condition, and the need to balance on several factors, we paired the 6 candidate schools and then when the pairs were formed, we randomized one school in each pair to intervention and the other to control.The primary criterion for pairing was % of days with 95% or greater attendance. That criterion led to the pairing of two of the 6 schools (high attendance). Of the four remaining schools, they were pair on the basis of facility condition and enrollment.Randomization assignment then was by pair. We generated 6 random numbers via a single program from a random uniform generator (using Stata v 15.1), and assigned in order two numbers to each of 3 pairs. Once assigned a number, each pair then had a school with a higher random number and school with a lower random number. The school with the higher random number became the intervention school in the pair.

Randomization of children within Schools:

Randomization at the child level proceeded using randomly permuted blocks with varying block sizes, as implemented using the Stata program "ralloc". 5 This program used randomly permuted blocks with block sizes of 2 and 4. Randomization was stratified by school and by clinic within school. There were two waves of randomization. The first occurred with the first recruitment of schools and the second with the second wave of schools.

Randomization of schools to implementation interviews:

Of the 20 original public schools selected to participate -18 school principals agreed to participate and two declined. Two schools were ineligible to participate in staff surveys. In addition, 3 charter schools were selected for convenience for preliminary work and test.Across the remaining schools, the goal was to select 2 more charter schools and 18 conventional public schools. To this end, we first assigned a random uniform (0,1) number to each school. Next, we ranked these random numbers within the charter schools and within the public schools. Third, we selected the 2 (charter) and the 18 schools (public) with the highest ranks (lowest rank numbers) for inclusion in the sample. This process was done by one of the study statisticians, using a pre-selected seed for the random number generator and with no contact with any of the schools nor any information about the schools other than their names.

Allocation Concealment:

For the individual child level randomization, all randomization lists were generated by one of the statisticians and then delivered to the REDCap database designers. Upon each child recruitment, REDCap then produced the treatment assignment for the next child and for the school of the child's attendance. All patients had to consent to the study (and sign consent forms) before the research coordinator could elicit from REDCap the treatment assignment for that child. By this method no one on the staff other than the database manager was aware of the next treatment assignment for the next child in the stratum of school*clinic. 

Online Appendix D. Simulations for Sample Size and Power

The pre-specified contrasts from specific aims 1a, 1b, and 1c, appear in Table D1 . All power estimates represent changes over time in the primary outcome (asthma control) in units of standard deviation of change. For translation into clinical terms, the minimally important difference (MID) for children on the Juniper's Asthma Control Questionnaire is 0.5 standard deviation units, which equates to 0.4 units on the questionnaire scale (for children ages 6 to 17 years). Estimates assumed a modest degree of variation in outcomes across schools, which equates to clustering of children within schools. For an overall effect size of 0.5 standard deviation units, we assumed that individual schools would vary from no effect (0.0) to a large effect (1.0 sd units). This degree of variation corresponds to a random cluster (school) effect of 0.025 sd units, and an intraclass correlation coefficient (ICC) of 0.06. With an average of 28 children per school, this variability equates to a design effect (variance inflation factor) of about 2.5. As the table shows, the contrasts for the primary care intervention are powerful to show even small effects, because they are within-cluster comparisons and therefore are influenced if at all only slightly by clustering of children within schools. Across school contrasts, of the effects of the school intervention, remain powerful for MID changes. Cells e, f are not included in this table for simplicity and to be conservative.

