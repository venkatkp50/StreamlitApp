INTRODUCTION

Recent years have witnessed continuous successes of neural ranking models in information retrieval [6, 17, 19, 25] . Most notably, deep pretrained language models (LMs) achieve state-of-the-art performance on several web search benchmarks [4, 18, 27] . Their success relies on the learned semantic information from general domain corpus with the language model pretraining [4, 28] .However, ranking models in specific domains usually face the domain adaption problem, which comes from two generalization gaps between the general and the specific domain. The first gap derives from the discrepancy of vocabulary distributions in different domains. Taking the COVID domain as an example [23, 24] , the earliest related publication appeared at the end of 2019. Even pretrained LMs targeting the biomedical domain [2, 13] are unfamiliar with new medical terms like COVID-19 because their pretraining corpora have not contained such new terminologies. The other gap is the label scarcity. For the specific searching scenario, large-scale relevance labels are luxury, such as biomedical and scientific domains. * indicates equal contribution.In addition, most information retrieval (IR) systems usually use sparse ranking methods in the first-stage retrieval, such as BM25, which are based on term-matching signals to calculate the relevance between query and document. Nevertheless, these systems may fail when queries and documents use different terms to describe the same meaning, which is known as the vocabulary mismatch problem [5, 8] .The vocabulary mismatch problem of sparse retrieval has become an obstacle to existing IR systems, especially for specific domains that have lots of in-domain terminologies.This paper presents a solution to alleviate the specific domain adaption problem with three core technics. The first one conducts domain-adaptive pretraining (DAPT) [10] to help pretrained language models learn semantics of special domain terminologies to keep the language knowledge is the latest. The second one uses Contrast Query Generation (ContrastQG) and ReInfoSelect [29] to mitigate the label scarcity problem in the specific domain. ContrastQG and ReInfoSelect focus on generating and filtering pseudo relevance labels to further improve ranking performance, respectively. Finally, our system integrates dense retrieval to alleviate the sparse retrieval's vocabulary mismatch bottleneck. Dense retrieval can encode query and document to dense vectors to measure the relevance between query and document in the latent semantic space [3, 9, 12, 14, 25] .Using above technologies, our system achieves the best performance among non-manual groups in Round 2 of TREC-COVID [23] , which is a COVID-domain TREC task to evaluate information retrieval systems for searching COVID-19 related literature.The next section will analyze the generalization gaps and vocabulary mismatch faced by the COVID domain search. Sec.3 and Sec.4 describe in detail how our system alleviates these problems. Sec.5 shows the evaluation results and hyperparameter study. In the Sec. 6 and Sec.7, we discuss the negative attempts and our concerns of the residual collection evaluation [21] used in TREC-COVID.

DATA STUDY

This section studies the generalization gaps from web to COVID domain, and the vocabulary mismatch problem of sparse retrieval.Domain Discrepancy. Most existing pretrained language models divide uncommon words into subwords, which aims to alleviate the out-of-vocabulary problem [22] . As shown in Figure 1 , the subword ratio of TREC-COVID queries is dramatically higher than that of the web domain dataset, MS MARCO [1] . The results show that existing pretrained language models treat most COVID-domain arXiv:2011.01580v1 [cs.IR] 3 Nov 2020Pretraining Corpus Domain TREC-COVID MS MARCO Figure 1 : The proportion of query words that are decomposed into subwords by the pretrained language model's vocabulary.terminologies as unfamiliar words, indicating a considerable discrepancy between the existing pretraining and the COVID domain. Label Scarcity. The label scarcity in the COVID domain search is very prominent. Only 30 queries were judged in the second round of TREC-COVID. In contrast, medical MS MARCO contains more than 78,800 annotated queries, which is the medical subset of MS MARCO filtered by the previous work [16] .Vocabulary Mismatch. We observed that BM25 only covered 35% of relevant documents in the top 100 retrieved documents. The result reveals that retrieving relevant documents only according to term-matching signals will hinder the search system's effectiveness.

SYSTEM DESCRIPTION

Our system employs a two-stage retrieval architecture, which utilizes BM25 for base retrieval and SciBERT [2] for reranking. The domainadaptive pretraining and two few-shot learning techniques are used to mitigate the generalization gaps faced by SciBERT in the COVID domain. Dense retrieval is also incorporated into our system to alleviate BM25's vocabulary mismatch problem.

Domain-Adaptive Pretraining

SciBERT has been used in our system since it is pretrained with scientific texts and biomedical publications. However, COVID is a new concept that has not appeared in previous pretraining corpora. Therefore, we conduct domain-adaptive pretraining (DAPT) [10] for SciBERT. Our approach is straightforward to continuously train SciBERT with CORD-19 corpus [24] , which is a growing collection of scientific papers about COVID-19 and coronavirus.

Few-Shot Learning

We introduce two few/zero-shot learning methods named ContrastQG and ReInfoSelect [29] to alleviate the label scarcity challenge when fine-tuning the neural ranking model. Specifically, we first use Con-trastQG to generate weakly supervised data in a zero-shot manner and then utilize a weak supervision data selection method, ReInfoSelect, to recognize high quality training data.ContrastQG is a zero-shot data synthetic method aiming to generate queries for synthesizing weakly supervised relevance signals. Unlike the prior work [15] , ContrastQG synthesizes a query given a relevant text pair rather than a single related text, which can capture the specificity between two documents to generate more meaningful queries instead of keyword-style queries.The entire synthesis process uses two query generators named and , which aim to generate pseudo queries according to documents. Both and are implemented with standard GPT-2 [20] .is trained on medical MS MARCO's positive passage-query pairs ( + , ) following the previous method [15] .is directly trained on medical MS MARCO's triples by encoding the concatenated text of positive and negative passages ( + , − ) to generate query .At inference time, we first leverage to generate queries based on a single COVID domain document :Then we utilize BM25 to retrieve two related documents ( ′ + , ′ − ) that show different correlation according to the generated query . Finally, is used to generate another query ′ based on the two contrastive documents ( ′ + , ′ − ):is used as weakly supervised data to train the neural ranker.ReInfoSelect [29] uses reinforcement learning to select weak supervision data. ReInfoSelect evaluates the neural ranker's performance on the target data and regards the NDCG difference as the reward. Then the reward signal from target data is propagated to guide data selector via the policy gradient.In our system, we use ContrastQG and medical MARCO to construct the weakly supervised data. The annotated data of TREC-COVID Round 1 is used as the target data. The trial-and-error learning mechanism of ReInfoSelect can select proper weakly supervised data according to neural ranker's performance in the target domain, which helps to further mitigate the domain discrepancy.

Dense Retrieval

Dense retrieval maps queries and documents to the same distributed representation space and retrieves related documents based on the similarities between document vectors and query vectors [12, 25] .Let each training instance contain a query , relevant (positive) document + and irrelevant (negative) documents − = { − } =1 . Dense retrieval first encodes the query and all documents to dense vectors and . Then the similarity of and is calculated as ( , ). The training objective can be formulated as learning a distributed representation space that the positive document has a higher similarity to the query than all negative documents:where the similarity (·, ·) is the dot product between vectors.

IMPLEMENTATION DETAILS.

In this section, we describe the system's implementation details.Dataset. The testing data of TREC-COVID Round 2 contains the May 1, 2020 version of the CORD-19 document set [24] (59,851 COVID-related papers) and 35 queries written by biomedical professionals. Among these queries, the first 30 queries have been judged System Setup. For data preprocessing, we concatenated title and abstract to represent each document and deleted stop words for all queries. Our system utilized the BM25 constructed by Anserini [26] as the base retrieval and adopted the dense retrieval implementation provided by Gao, et al. [9] . The neural ranker based on SciBERT [2] was used in dense retrieval and reranking stages [16] with the learning rate of 2e-5 and the batch size of 32. We set the warm-up proportion as 0.1 and limited the maximum sequence length to 256. The NDCG@10 score on the development set is used to measure the convergence and is calculated every three training steps. Our system is based on PyTorch, and the training process it involves can be implemented on a GeForce RTX 2080 Ti.

EVALUATION RESULTS

This section presents evaluation results and hyperparameter studies. Table 1 shows the overall performance of different models in the TREC-COVID task. Three top systems during Round 2 evaluation and several variants of our systems are compared.

Overall Results

Our system achieved the best performance in Round 2 of TREC-COVID. From our detailed experimental results, our method significant improves the ranking performance of SciBERT in the COVID domain. The domain-adaptive pretraining (DAPT) helps to improve SciBERT, which illustrates that learning the semantics of these new terminologies is crucial for language models. Then the system's performance has been further improved with about 6.5% NDCG@10 gains by ContrastQG and ReInfoSelect. ContrastQG generates lots of pseudo relevance labels, which provides more training guidance for neural rankers in the specific domain. ReInfoSelect further boosts models with more fine-grained selected supervisions. The most significant improvement comes from the fusion of dense retrieval, where the P@5 score is increased by 11.8%. This result shows that dense retrieval can significantly improve retrieval effectiveness by alleviating sparse retrieval's vocabulary mismatch problem.

Hyperparameter Study

Among all hyperparameters, we found the reranking depth significantly impacts the neural ranking model's effectiveness. As shown in Table 2 , SciBERT's performance is significantly limited at the shallow reranking depth (≤20), mainly caused by the low ranking accuracy of BM25. With the increase the reranking depth to 50 and 100, the neural ranker shows stable performance and achieves the best. Nevertheless, the reranking accuracy begins to drop as the depth continues to increase. The possible reason is that the neural ranker is not good enough to distinguish truly relevant documents when more noisy documents are included. Figure 2 shows the testing results of each query. The first 30 queries have been judged in Round 1, and others are newly added in Round 2 (query 31-35). Our system outperforms baselines on most queries with previous annotations. Besides, our system is also comparable to the T5 Fusion system on new queries and avoids the sharp drop of the SciBERT Fusion system (such as 34th query), which shows our system's robustness.

FAILED ATTEMPTS

This section discusses some of our failed attempts and experience.Manual Labeling. A straightforward approach to mitigate the label scarcity is to annotate more data within this domain manually. We recruited three medical students who compiled 50 COVID-related queries and assigned the relevance label to the top 20 documents retrieved by BM25 for each query. However, our annotations were not able to get good agreement with TREC-COVID's annotations. 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Ours @2 @3 @4 @5 @6 @7 @8 @9 @10 Top@10 Feedback System Corpus Filtering. MacAvaney et al. [16] proposed to narrow the retrieval scale by filtering out the document published before 2020. Nevertheless, our analysis found that this method excluded more than 80% of documents from the second round of corpus, dropping a large amount of useful COVID-related literature, such as SARS and MERS. Thus, we did not adopt this method in our system.Neural Reranker. We also attempted two other neural ranking models besides SciBERT for document reranking, including BERT [7] and Conv-KNRM [6] . Our experimental results show that BERT-Large has no obvious advantage over SciBERT-Base and Conv-KNRM performs the worst. The main reason for the poor performance of Conv-KNRM is that we did not use its subword version [11] , which led to a severe out-of-vocabulary problem.Fusion Attempts. Two fusion methods have been tried to integrate dense retrieval into our system. One approach is to combine dense retrieval with BM25 in the base retrieval stage. The other is to fuse dense retrieval into SciBERT's reranking processing directly. The second method works better in our limited attempts.

CONCERNS ON RESIDUAL EVALUATION

This section discusses our observations about the residual collection evaluation used in the TREC-COVID task. In residual collection evaluation, test queries can be divided into old queries and new queries. The old queries have been annotated in previous rounds, but their annotated documents will be removed from the collection before scoring. TREC-COVID allows IR systems to use old queries' relevance judgments and classify such systems as feedback types. Figure 3 shows the evaluation results of the top 10 feedback systems in Round 2 of TREC-COVID. Although these systems performed closely in overall scores, they showed significant differences in the old and new queries. E.g., the 2nd system's performance in the new query is greatly better than that in the old query. In contrast, some systems' ranking accuracy for the new query is considerably lower than in the old query, even worse than the base retrieval BM25 Fusion system, such as the 3rd-5th and 9th systems. A powerful search system is desirable to achieve balanced performance on known and unknown queries. However, this result shows that the residual collection evaluation may bias towards seen queries, which are much easier in real production scenarios.

