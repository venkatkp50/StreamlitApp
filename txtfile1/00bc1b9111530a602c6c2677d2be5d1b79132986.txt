

I also have a number of clarifying questions and comments that I would like the authors to consider.

Methods

The PadiaTrac tool: PediaTrac included 220-340 items per sampling period. Each assessment takes about 1-2 hours. This is a large burden on parents (which is acknowledged in the ethics). The burden makes me wonder whether the burden of answering this about of questions can compromise the validity of responses due to fatigue and boredom? It seems to be that the development of a much more condensed assessment that takes closre to 20 minutes would be much preferable as much more feasible.Study population: It seems that the first (NB) assessment will take place at 39 weeks post-menstrual age for both term and preterm infants. All subsequent data collection time points will similarly be corrected age for preterm infants. This is a minor point, but I expect this to matter less as children age to I wonder if they will stop this adjustment after a certain point.A subset of 100 toddlers participate in a clinic based developmental evaluation (Bayley's Scales of Infant Development) at 24 months. I don't see a justification for this sample size in the power calculation. How was this number determined?The protocol does not mention anything about the sex ratio of either the full sample or this subsample. Will you attached to recruit 50% male / 50% female?Data collection and tracking The value of the of paper-based legacy measures at 2 and 12 months is unclear. Are these paper froms the source of information used to compare the "three sources of distortion" described later in the protocol. Are these this the only time point where you "triangulate data"? And why are these paper-based? What is the benefit of paper? Wouldn't this be completed online like in most pediatric practices? Table 3 provides a very helpful outline of the content of PediaTrac but it does not explain the selection process which I had expected given that the key aim of the protocol is to "to describe the methods used to refine the item bank and scales of PediaTrac (now PediaTrac v3.0)". I expected an explanation of how PediaTrac had been refined previously to arrive at this content. Then the next analytic step, which is indeed outlined in this protocol, is to examine the validity of the tool that has already been developed.The study draws on a sample of dyads recruited from three sites, each site has a different target sample size. How were these targets derived?15% attrition from 600 would give a sample of 510 not 500 (page 7).The description of PediaTrac is not very easy to follow. The domains in the tool need to be described/defined either in the methods or in the introduction. Why were these domains chosen? Content from the discussion could be helpful here to help explain the system and how it was developed.PAGE 7 -"To more effectively model the latent trait……...reflecting older development could be included" -this paragraph does not make sense. It's the first mention of latent trait development -what do you mean by this? What do you mean by more difficult item (are you referring to item difficulty as determined through IRT)?The legacy measures -how were they selected over other measures?With regards to missing data, each of the legacy measures has established rules on how to tackle item level missing data and the minimum number of items that must be completed. Are you following those rules? It would be useful to explain more fully your strategies for dealing with item and domain-level missing data and loss to follow-up.See point above about the lack of consistency in description of the aims in the objectives section and the planned analyses. Aim 2 explores sub group differences these are not described in aims and objectives section and there is no information in the introduction providing a rationale for this analysis. Aim 3 refers to analyses exploring developmental/longitudinal pathways but again this is not consistent with stated aims and objectives of the study.Aim 4 -surveys of multiple domains -do you mean the total overall score? Confusing not clear what you mean.Sample size, power attrition 15% attrition rate is described as plausible. Citation is needed.If the final sample size is at your lower estimate of 420 what are the implications for your analysis, particularly analyses that involve dividing the group into smaller subgroups? Particularly as you state 500 as an adequate sample size (this needs a citation).Please specify what software you plan to use for data analyses.Page 10 please specify the amount paid to participants in USD or GBP.Throughout you refer to validity items/validity indices/response bias items interchangeably -please use a consistent term throughout. 

Data analyses

In general, I find. That more information about e.g. assumptions (e.g. normality), how missing data will be handled, and what covarites will be considered in each analysis would be helpful. As mentioned below, more detailed power analyses would also be helpful.Aim 1: The IRT analysis of validity of PediaTrac Tool. This aims looks good overall. The IRT framework seems very suited for this aim.Aim 2: To test if pediatric discriminate between term and preterm. This aim was not explained in the abstract or in the introduction. While very meaningful, more context would be helpful. Was PediaTrac designed to make this distinction? Is it considered a screening tool? Is preterm vs. term the most relevant distinction? What about identification of early stages of developmental disorders like ASD? Statistical models will be multivariate regression. Will you use logistic regression? What co-variates will be included? Will you control for severe neurodevelopmental conditions associated with preterm birth that very likely will affect outcomes? Aim 3: Temporal change in the measure of the construct [the PediaTrac developmental IRT score?] and to assess subgroup differences in the trends over time. Statistical approach is longitudinal growth curve models. What subgroups will be compared? Would this approach be used to address the preterm vs. term birth? This seems a more natural Aim 2 rather than 3.Aim 4: The predictive validity of PediaTrac will be assessed by examining associations of domain scores at each sampling period and of changes in these scores across assessments to scores on the Bayley-4 at 24 months. This is very interesting and important.Statistical power: No calculations to support power for aims 2-4. The general reference to a sample size of 500 for IRT graded response models seem very general.Embedded validity indices I very much appreciate the attempt to unpack responder bias through the use of three sources of distortion. I understand that these are methods used to compare responds from the same respondent, but caregiver may very well see through this and replicate the bias across responses. It would have been useful to have a subsample of respondents were e.g. two caregivers or even better, a caregiver and a non-parental care taker or other professional responded on the same child. The comparison with the Bayley Scales of Early Learning in Aim 4 is also important in this regard and it would be good to do some concurrent validity check against the Bayley's, but only predictive validity. As mentioned above, it was unclear to be whether this will be done with the paperbased assessments and at ages 2 and 12 months? Ethics Thoughtful ethics overall, but the section does not cover what happens if a child if found to be developmentally very off track?

REVIEWER

Blower, Sarah University of York, Health Sciences REVIEW RETURNED 20-May-2021Nüssli, Stephan Berne University of Applied Sciences, Engineering and Information Technology REVIEW RETURNED 02-Sep-2021

GENERAL COMMENTS

Previous systematic reviews have highlighted large numbers of different measures of child development and related variables and inconsistency in the choice of measures across studies investigating the outcomes (Wolpert et al, 2016) . The psychometric properties of commonly used measures to assess early childhood development have also been questioned. This protocol proposes a study to evaluate a new measure and online system for capturing longitudinal data on child development -PediaTrac v3.0. The authors intend PediaTrac to be a low cost, efficient measure that can be implemented longitudinally and routinely to identify children who may require additional support.The protocol needs further refinement to ensure clarity and consistency of message throughout and additional information is needed in order to judge the merit of the methods proposed and whether there are flaws that could jeopardise interpretation of the data generated. The protocol is a difficult read -some information is repeated and yet there is also not enough context in other places for readers unfamiliar with this work. I have separated my comments on the protocol into major issues and more minor issues. I absolutely sympathize with the authors with regards to the challenge of communicating a complex longitudinal study in an outline protocol and I hope my comments help them present a clearer vision of their very important work.Major issues:Thank you for the opportunity to revise this interesting protocol. The manuscript is well written and detailed. The structure is solid and explains complex concepts in a clear and comprehensive way. Please find below a few minor comments: -Abstract, page 5 (Lines 14-18) : "PediaTrac, a web-based measure, has the potential to become a standard for monitoring development beginning at birth for early detection in the primary care setting.". This sentence is unclear to me. Please rephrase.-Please report the name of organizations/institutions/etcetera in full the first time you mention them within the text before reporting the acronym. For instance amend "the NICHD..." (page 7,line 5) to" The National Institute of Child Health and Human Development (NICHD)... ".-Please explain what you mean with "real-time" (see page 5-line 12/page 7-line 18 and 52/page 8-line 14): at first impression, it may seem to imply that developmental assessment would take place on a daily basis, which however is not what emerges from the protocol. Consider changing the term or further explain it, as it may be confusing.-Ethics: page 20 (Lines 26 and 31): "Caregivers will be informed of the limitations of confidentiality..." and "Caregivers are informed that...".. I would suggest to use the same tense (will be informed or are informed) in these two sentences.- Figure 1 : consider removing (or amend) the 3rd bullet point in the first box on the top, as it is redundant with 1st bullet point in the first box on the bottom. Second box on the top, 3rd bullet point: "Families not approached until 34 weeks post conception al age". This sentence is not clear to me. Do you mean that you did not enroll infants who were born before 34 weeks of gestational age? Consider specifying this criteria in the "study population" section.Thank you for the dedicated and valuable study of the development of infants and toddlers Comments: 1) Power of the study -> not comprehensibly calculated 2) "Recruitment from multiple sites with demographically diverse populations" -> includes at most the risk for demographic bias on site #3 and #4 with the smallest number of participants in the outpatient area comparing to site #1 and #2 3) I could not find the items from pediaTrac in the current study design or in the validation study of PediaTrac™ (2018)

Introduction

There is some useful contextual information in the introduction, however the rationale for both the development of PediaTrac and the proposed study needs strengthening. For example, the authors describe a "critical need to develop a comprehensive assessment tool to track infant/toddler status that can be used to support behavioural correlates of brain structure and function" (page 4).There are several existing measures of infant and toddler development (you refer to them as legacy measures). The introduction would benefit from a brief overview of these existing tools and a clearer explanation of the flaws of these measures, and the reasons why a new measure is needed. The authors mention CHADIS, PROMIS, Ages and Stages but assume prior knowledge of these tools -a brief description of each would be useful. Isn't there an argument for improving existing measures such as these, rather than creating new ones that further add to inconsistency in the field? The authors argue that there is no single measure covering all domains of development, this may be true, however I'm still not sure why you couldn't combine existing measures into a larger battery. Why create a completely new tool? I am confident that you have several counter-arguments to these points, however they are scattered about the introduction as it currently stands and need to be pulled together to support your central argument. With reference to the sentence highlighted above on page 4 of your protocol, it is also not clear from the protocol how PediaTrac be used to derive behavioural correlates of brain structure and function data?Can you explain what you mean by this? It's possible that this might be an area of future development that would sit better in the discussion section? If PediaTrac collects information on brain structure and function this is not clear.The introduction feels a little jumbled. It was difficult to figure out what the primary functions of PediaTrac were. I think that they were three-fold: 1) to support screening and identification of those at risk of developmental disorders; 2) to support consistent collection of quality routine data; and 3) to support further research on developmental trajectories and pathways. Could you find a simple way of summarizing these?You describe PediaTrac as low-cost throughout the manuscript but the protocol does not describe an economic evaluation. Low-cost compared to what? What does low-cost mean and what will you charge for access to PediaTrac in the future? This could be something for the discussion and is important to explain in more detail if developing PediaTrac is a response to the barriers posed by tools and measurement systems managed by commercial publishers?Objectives (page 6) The aims and objectives of the study are not very clear and there are significant inconsistencies between the stated aims and the planned analyses. It would be helpful if you could number the aims in this section and mirror the number and sequence of these aims later when you present hypotheses and also in the methods section. Hypotheses 1, 3 and 4 are listed but there isn't a second? These also don't map onto the numbered aims in the methods/analysis section.For example you state that one aim of the study is to demonstrate the feasibility of PediaTrac -in what ways are you assessing feasibility?Another aim was to establish validity of PediaTrac but perhaps you could be more precise here about the psychometric properties under investigation?Another aim was to describe the methods used to refine item bank and scales. However, on page 7 the authors state that this information has been described in a previous publication -there are no further details in this protocol.A gold standard assessment at 24 months is highlighted, but it is not clear at any point in the protocol what the gold standard assessment is? I'm not aware of a gold standard broad measure of child development.You have inserted a quote very early in the introduction referring to five behavioural or cognitive disorders. Do you need this quote? If you decide to retain it you need to explain to the reader what five disorders are being referred to here. You may also need to comment on how the items/domains in PediaTrac map onto the symptoms of those disorders.

Embedded validity indices

Great to see your design take into account potential biases, this is really critical when using self-report data. The study is mitigating against random responding but respondent fatigue is not listed as one of the potential threats to internal validity at the start of the section. Surely given the time needed at each timepoint and number of items to be completed this is a major risk for your study?Ethics Does the total time per sampling period include the time taken to complete the PediaTrac questions? Participant burden seems very high -particularly for respondents parenting small children? Would be interested to know how that was justified to ethics -was it really necessary to collect so much data?Stress arising from time taken to complete data collection is acknowledged as a risk for participants but not addressed in your mitigation strategy.Also broader point here about the sheer amount of data being collected -is it fully justified? Difficult to tell without additional detail about the rationale for the selection of legacy measures and why those measures cannot be used within PediaTrac -why new items?

Discussion

Useful context is provided here re the development of the tool that should come much earlier in the protocol, possibly in background or methods section.This section should include an acknowledgement of the limitations of the study -as these will influence the conclusions that may be drawn and the contribution the study is likely to make. For example, the bias arising from a sample that does not include more disadvantaged groups such as those without digital devices and who are not fluent in English.*** MINOR COMMENTS:Spell out acronyms when first used -e.g. NICHD on page 4. CHADIS page 5.Use of the phrase 'Midwest' on p.9 -please specify this is the US for international readers.Tense throughout needs to be consistent -probably future tense -sympathise as it's difficult if the study has already commenced.The phrase "legacy measure" needs to be explained/defined when first used. I don't think it will be a familiar term for readers who have no prior knowledge of NIH funded PROMIS such as those based outside the US.

Abstract

According to the abstract "response validity indices are being developed", it's not clear whether they are being developed in this study or for this study?It would be useful to clearly state in the abstract exactly which psychometric properties are tested in the study.The phrase "undergo direct developmental assessment at 24 months of age". What does this mean? Is this a clinical assessment? Please clarify.Article summary (page 3) PediaTrac is efficient and low-cost. You don't know the outcomes of your study yet, so this should be rephrased to "is designed to be a" or something similar. See also my comment above about the meaning of "low-cost".Please also take another look at the second bullet point. Same as above -I think you need to emphasise the potential rather than state that it will become "data common elements".

VERSION 1 -AUTHOR RESPONSE

Reviewer 1 1. Four hypotheses are presented. These seem to address only aims 1 and 4. Better alignment between hypotheses and aims would be helpful.• We have substantially revised the Objectives section so that the Aims and Hypotheses are aligned. The study actually has 3 main Aims. The hypotheses that were noted were exclusively for Aim 1, since this Aim has multiple hypotheses. We had originally attempted to present the protocol in a slightly modified format to enhance clarity, but it is apparent that this led to confusion. We modified both the Objective and the Data Analysis section so there is clear parallel structure. We believe the revision is much improved.2. The PediaTrac tool: PediaTrac included 220-340 items per sampling period. Each assessment takes about 1-2 hours. This is a large burden on parents (which is acknowledged in the ethics). The burden makes me wonder whether the burden of answering this about of questions can compromise the validity of responses due to fatigue and boredom? It seems to be that the development of a much more condensed assessment that takes closer to 20 minutes would be much preferable as much more feasible.We agree with the reviewer, and the ultimate goal is to have a much shorter version of PediaTrac. This is an important aim of this investigation --to identify the items which are providing the most psychometric information so the item bank can be reduced. We have clarified that the PediaTrac tool itself will require only 20-30 minutes. The time to complete legacy instruments required the additional 30-90 minutes. However, these tools were necessary to validate the multiple domains of PediaTrac. We also have added clarification that the participants are not required to complete the assessments in one sitting.3. It seems that the first (NB) assessment will take place at 39 weeks post-menstrual age for both term and preterm infants. All subsequent data collection time points will similarly be corrected age for preterm infants. This is a minor point, but I expect this to matter less as children age to I wonder if they will stop this adjustment after a certain point.• Adjusted age or age correction for preterm birth in developmental assessment is recommended up to 24 months, and it has recently been suggested up to 36 months of age (Aylward 2020). 4. A subset of 100 toddlers participates in a clinic based developmental evaluation (Bayley's Scales of Infant Development) at 24 months. I don't see a justification for this sample size in the power calculation. How was this number determined?• We have corrected this omission by adding the power analysis for the developmental assessment with the Bayley-4. 5. The protocol does not mention anything about the sex ratio of either the full sample or this subsample. Will you attempt to recruit 50% male / 50% female? • Yes, we will attempt to recruit equal numbers of males and females. This has been added to the "Patient and public involvement" section. 6. The value of the of paper-based legacy measures at 2 and 12 months is unclear. a. Are these paper forms the source of information used to compare the "three sources of distortion" described later in the protocol.• The instruments administered at 2 and 12 months are the IOP and modified PAI which are specifically designed to examine response bias.• We have moved the "Embedded response bias" section so that this content is with the other legacy measures and modified the text to clarify that these are external criterion measures for response bias. We only administered them near the beginning and end of the study period to decrease further burden on the participants and so that we could examine trends over the course of the investigation.b. Are these the only time points where you "triangulate data"? • We may be misunderstanding the phrase, but this is not qualitative research, per se. 7. And why are these paper-based? What is the benefit of paper? Wouldn't this be completed online like in most pediatric practices? • Most of the legacy instruments but not all (i.e., ASQ, M-CHAT) are not digitally available and have publication/copy restrictions. 8. Table 3 provides a very helpful outline of the content of PediaTrac but it does not explain the selection process which I had expected given that the key aim of the protocol is to "to describe the methods used to refine the item bank and scales of PediaTrac (now PediaTrac v3.0)". I expected an explanation of how PediaTrac had been refined previously to arrive at this content. Then the next analytic step, which is indeed outlined in this protocol, is to examine the validity of the tool that has already been developed.• In Study Design and Overview section, we have added information that summarizes the previous approach to item selection and refinement fully described in Lajiness-O'Neill et al (2018). We have information about how we have extended this approach to develop the item content in the current version. 9. In general, I find that more information about e.g. assumptions (e.g. normality), how missing data will be handled, and what covariates will be considered in each analysis would be helpful.• Please refer to the Descriptive Analyses section for text regarding missing data. In that section, we also note which covariates will be examined for Aims 2 and 3. Aim 1 will not include covariates as this aim is specifically focused on the item and domain level IRT analyses. 10. As mentioned below, more detailed power analyses would also be helpful.• We have added text and analyses to the power analyses section. 11. Aim 2: To test if pediatric discriminates between term and preterm. This aim was not explained in the abstract or in the introduction. While very meaningful, more context would be helpful. a. Was PediaTrac designed to make this distinction?• We have rewritten the Objectives section and added text to the Introduction to address this important critique. b. Is it considered a screening tool? • Yesearly identification of developmental deviations would be expected to lead to more comprehensive and targeted direct assessments. c. Is preterm vs. term the most relevant distinction? • Yes, it is one of the most relevant due to the strong evidence of developmental risks associated with preterm birth and the prevalence of this condition. d. What about identification of early stages of developmental disorders like ASD?• That is anticipated to be an important application in future research with the instrument. We have added text to the Introduction in this regard. 12. Statistical models will be multivariate regression. a. Will you use logistic regression? • Yes, we intend to complete a series of regression models as well logistic regression (when outcomes are dichotomous), and we have added text in this regard to the analyses section. b. What covariates will be included?• We have added text in this regard. c. Will you control for severe neurodevelopmental conditions associated with preterm birth that very likely will affect outcomes?• Apart from Down Syndrome and neonatal abstinence syndrome (NAS), we are not excluding or controlling for severe neurodevelopment conditions for a number of reasons including the fact that many conditions are not diagnosed at birth, and we are trying to recruit representative samples of newborns. 13. Aim 3: Temporal change in the measure of the construct [the PediaTrac developmental IRT score?] and to assess subgroup differences in the trends over time. Statistical approach is longitudinal growth curve models. a. What subgroups will be compared?• We have clarified that subgroups are term and preterm. b. Would this approach be used to address the preterm vs. term birth? This seems a more natural Aim 2 rather than 3.• Yes, you are correctwe have clarified that for Aim 2 we are: a) Initially assessing whether our theta scores can discriminate between term and preterm infants at each time point, and b) whether we can detect group differences in the developmental trajectories of specific domains over the first 18 months of development. 14. Statistical power: No calculations to support power for aims 2-4. The general reference to a sample size of 500 for IRT graded response models seem very general.• As noted in responses to items 4 and 10 above, we have added text regarding power analyses. 15. Embedded validity indices: I very much appreciate the attempt to unpack responder bias through the use of three sources of distortion. I understand that these are methods used to compare responses from the same respondent, but caregiver may very well see through this and replicate the bias across responses. a. It would have been useful to have a subsample of respondents where e.g. two caregivers or even better, a caregiver and a non-parental care taker or other professional responded on the same child.• This is a very interesting comment but beyond the scope of this phase of refinement. b. The comparison with the Bayley Scales of Early Learning in Aim 4 is also important in this regard and it would be good to do some concurrent validity check against the Bayley's, but only predictive validity. • Yes, we agree but we do not have a 24-month PediaTrac (yet). c. As mentioned above, it was unclear to me whether this will be done with the paper-based assessments and at ages 2 and 12 months?• We have addressed this in item 6 above. 1. There is some useful contextual information in the introduction, however the rationale for both the development of PediaTrac and the proposed study needs strengthening. For example, the authors describe a "critical need to develop a comprehensive assessment tool to track infant/toddler status that can be used to support behavioural correlates of brain structure and function" (page 4). a. There are several existing measures of infant and toddler development (you refer to them as legacy measures). The introduction would benefit from a brief overview of these existing tools and a clearer explanation of the flaws of these measures, and the reasons why a new measure is needed. The authors mention CHADIS, PROMIS, Ages and Stages but assume prior knowledge of these tools -a brief description of each would be useful.• We agree that a more compelling rationale would help strengthen the manuscript, and we have added this to the Introduction. We now briefly describe the existing measures and how they fail to address what we envision PediaTrac being able to accomplish. b. Isn't there an argument for improving existing measures such as these, rather than creating new ones that further add to inconsistency in the field? The authors argue that there is no single measure covering all domains of development, this may be true, however I'm still not sure why you couldn't combine existing measures into a larger battery. Why create a completely new tool? I am confident that you have several counter-arguments to these points, however they are scattered about the introduction as it currently stands and need to be pulled together to support your central argument.• We have now addressed this in the Introduction. Two of the most important potential benefits of PediaTrac are derived from how it is being developed. First, given that it is being developed using Item Response Theory methods, we will be able to examine outcomes at the item level and at the individual level (unlike the vast majority of current tools that were developed with Classical Test Theory methods), supporting precision medicine. This method also will allow us to prepare for a computerized adaptive testing (CAT) version of PediaTrac, which will significantly decrease burden. Second, there are no existing measures that are administered in a systematic way over time allowing for an examination of developmental trajectories across multiple domains of development concurrently.2. The authors explain that there is no equivalent to the growth chart for other developmental outcomes, however many existing measures such as the Ages and Stages Questionnaire (ASQ) and the Social-emotional (ASQ:SE) version do have well established cut-offs and norm data is available for infants and toddlers in the US? Similarly, when you state that existing/legacy measures do not indicate the degree to which children's development varies from age-related expectations. Isn't this exactly what ASQ does?• We have added text that further describes the limitations of legacy measures including the ASQ. Risk cut-off scores cannot provide growth trajectories, per se, nor can they identify precursors to pathology before deviations are in the pathological range. Furthermore, the complexity of precursor risk profiles is likely to require more comprehensive profile (multivariate) approaches for early identification, beyond the basic ASQ domains. 3. With reference to the sentence highlighted above on page 4 of your protocol, it is also not clear from the protocol how PediaTrac be used to derive behavioural correlates of brain structure and function data? a. Can you explain what you mean by this? • Yes, we have added text in this regard to note that developmental neuroscience studies in particular will be able to correlate behavioral data from PediaTrac with molecular, cellular and brain-systems level data. b. It's possible that this might be an area of future development that would sit better in the discussion section?• To expand on this, we have added pertinent text to the Discussion section. c. If PediaTrac collects information on brain structure and function this is not clear.• No, this project is focused on behavioral data only. 4. The introduction feels a little jumbled. It was difficult to figure out what the primary functions of PediaTrac were. I think that they were three-fold: 1) to support screening and identification of those at risk of developmental disorders; 2) to support consistent collection of quality routine data; and 3) to support further research on developmental trajectories and pathways. Could you find a simple way of summarizing these?• We have substantially revised the Introduction in a manner that we hope provides extensive clarifications that fully address your comment. 5. You describe PediaTrac as low-cost throughout the manuscript, but the protocol does not describe an economic evaluation. a. Low-cost compared to what?• You are correct. The Introduction has been revised and addresses that this the "low cost" is in relation to direct assessment (e.g., Bayley) from a clinician. b. What does low-cost mean and what will you charge for access to PediaTrac in the future? This could be something for the discussion and is important to explain in more detail if developing PediaTrac is a response to the barriers posed by tools and measurement systems managed by commercial publishers? • Again, the key comparison is with the costs of direct assessments in countries that do not have socialized medicine. We also have added text regarding the limited accessibility to direct assessment expertise. 6. The aims and objectives of the study are not very clear and there are significant inconsistencies between the stated aims and the planned analyses. a. It would be helpful if you could number the aims in this section and mirror the number and sequence of these aims later when you present hypotheses and also in the methods section.• Clearly, this was a fundamental problem with the original submission. We have corrected the manuscript so that the Aims are numbered, and parallel structure and numbering is noted in the Analysis. This was an error. b. Hypotheses 1, 3 and 4 are listed but there isn't a second? These also don't map onto the numbered aims in the methods/analysis section. • As noted above, the Aims/Objective and Analysis have been substantially modified for clarity. c. For example you state that one aim of the study is to demonstrate the feasibility of PediaTrac -in what ways are you assessing feasibility?• We have removed this language as we agree that it is confusing. d. Another aim was to establish validity of PediaTrac but perhaps you could be more precise here about the psychometric properties under investigation?• Within the Study Objectives, we have already stated that we are assessing reliability and construct and predictive validity. The hypotheses for Aim 1 provide further clarification regarding our expected outcomes for reliability and validity. e. Another aim was to describe the methods used to refine item bank and scales. However, on page 7 the authors state that this information has been described in a previous publicationthere are no further details in this protocol.• This language in the original manuscript was unclear and we have removed it. The methods used to refine the item bank are, in fact, the current methods. IRT methods will allow us to substantially reduce the current item bank based on our analyses. In our prior publication (Lajiness-O'Neill, 2018), we describe the methods used to develop the full item bank, and the items that were retained from that study were used in this investigation. This has been clarified. It would be length prohibitive and outside of the scope of the current manuscript to again describe the literature search, expert panel reviews, and cognitive interviews with parents, etc. 7. A gold standard assessment at 24 months is highlighted, but it is not clear at any point in the protocol what the gold standard assessment is? I'm not aware of a gold standard broad measure of child development.• We have modified this section and this phrasing was removed. 8. The study draws on a sample of dyads recruited from three sites, each site has a different target sample size. How were these targets derived?• The sites were chosen based on feasibility to recruit the required sample and to ensure a diverse and representative sample. Site 1 (UM) and Site 2 (UH) are large university hospitals from which the preterm infants were recruited (120 from each = 240). The term infants were recruited in equal numbers from all three sites (UM = 120; UH = 120; EMU = 120) which resulted in 240 preterm and 360 term infants as outlined in Figure 1 . EMU does not have a university hospital and was responsible for recruitment at sites 3 and 4 (now listed as #3 to minimize confusion). 9. 15% attrition from 600 would give a sample of 510 not 500 (page 7). • Yes, you are correct! And we have corrected! 10. The description of PediaTrac is not very easy to follow. a. The domains in the tool need to be described/defined either in the methods or in the introduction.• To address this comment, we added a • The parent-report measures chosen were based on those that had good psychometric properties and/or acceptance within the field, sampled constructs that were conceptually aligned with the PediaTrac domains, were correlated with the domains of interest in our pilot investigation, and based on expert opinion. This information has been added. 13. With regards to missing data, each of the legacy measures has established rules on how to tackle item level missing data and the minimum number of items that must be completed. Are you following those rules? It would be useful to explain more fully your strategies for dealing with item and domain-level missing data and loss to follow-up.• We have added text to the Data Analysis section to note that we follow the established rules for identification and remediation for missing data on the legacy instruments. 14. See point above about the lack of consistency in description of the aims in the objectives section and the planned analyses. Aim 2 explores sub group differences these are not described in aims and objectives section and there is no information in the introduction providing a rationale for this analysis. Aim 3 refers to analyses exploring developmental/longitudinal pathways but again this is not consistent with stated aims and objectives of the study.• We have corrected this throughout the document. 15. Aim 4 -surveys of multiple domains -do you mean the total overall score? Confusing not clear what you mean.• Nowe are referring to algorithms that utilize separate domain scores. We have added this language in the Objectives for clarification. 16. 15% attrition rate is described as plausible. Citation is needed.• See power analyses section. We provide further information in this regard. 17. If the final sample size is at your lower estimate of 420 what are the implications for your analysis, particularly analyses that involve dividing the group into smaller subgroups? Particularly as you state 500 as an adequate sample size (this needs a citation).• We now address this in the text added to the power analyses section with multiple citations. 18. Great to see your design take into account potential biases, this is really critical when using selfreport data. The study is mitigating against random responding but respondent fatigue is not listed as one of the potential threats to internal validity at the start of the section. Surely given the time needed at each timepoint and number of items to be completed this is a major risk for your study?• We have added to text to clarify this in the Embedded validity and ETHICS sections. 19. Does the total time per sampling period include the time taken to complete the PediaTrac questions? Participant burden seems very high -particularly for respondents parenting small children? Would be interested to know how that was justified to ethics -was it really necessary to collect so much data? • To clarify, the PediaTrac tool itself will only require 20-30 minutes. The time to complete legacy instruments requires 30-90 minutes depending on the time period; in essence two hours over several weeks as they were not required to complete anything in one sitting. It was necessary to have this number of external criterion tools in order to validate the multiple domains of PediaTrac. 20. Stress arising from time taken to complete data collection is acknowledged as a risk for participants but not addressed in your mitigation strategy.• Text has been added to clarify the burden and note mitigation strategies. 21. Also, broader point here about the sheer amount of data being collected -is it fully justified? • As noted above, we are collecting external criterion measures for multiple facets of development. The data collection burden is similar to that described in the first paper, and in that instance there were no adverse events related to the 1-2 hour time commitments. 22. Difficult to tell without additional detail about the rationale for the selection of legacy measures and why those measures cannot be used within PediaTrac -why new items?• In developing the original item bank as described in Lajiness-O'Neill, 2018, modified content of items for many of the measures used in the development of PediaTrac was used. New items were added based on a more current literature or expert review to support their inclusion. Please see the Study Design and Overview for an elaboration of this method. 23. Useful context is provided here re the development of the tool that should come much earlier in the protocol, possibly in background or methods section.• We have moved that content as suggested and as noted in #22. 24. This section should include an acknowledgement of the limitations of the study -as these will influence the conclusions that may be drawn and the contribution the study is likely to make. For example, the bias arising from a sample that does not include more disadvantaged groups such as those without digital devices and who are not fluent in English.• We have added a Study Limitations section. 25. MINOR COMMENTS: a. Spell out acronyms when first used -e.g. NICHD on page 4. CHADIS page 5.• Addressed b. Use of the phrase 'Midwest' on p.9 -please specify this is the US for international readers.• Addressed c. Tense throughout needs to be consistent -probably future tense -sympathise as it's difficult if the study has already commenced.• We now consistently use the future tense. d. The phrase "legacy measure" needs to be explained/defined when first used. I don't think it will be a familiar term for readers who have no prior knowledge of NIH funded PROMIS such as those based outside the US.• We now define "legacy" simply as "established" in the text. e. Abstract: According to the abstract "response validity indices are being developed", it's not clear whether they are being developed in this study or for this study?• The embedded response bias indices are being developed as part of the tool validation.We have changed the language in the Abstract so that this is clearer. f. It would be useful to clearly state in the abstract exactly which psychometric properties are tested in the study.• The Abstract now states that we will be examining reliability, construct (discriminant)

