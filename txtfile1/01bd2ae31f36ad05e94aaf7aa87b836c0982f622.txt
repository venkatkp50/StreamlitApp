INTRODUCTION

Although the amount of news at our disposal seems to be everexpanding, traditional media companies and professional journalists remain the key to the production and communication of news. The way in which news is disseminated has become more intricate than in the past, with social media playing a fundamental role [12] .The ephemeral, fast-paced nature of social media, the brevity of the messages circulating on them, the short attention span of their users, their preference for multimedia rather than textual content, and in general the fierce competition for attention, has forced journalists to adapt in order to survive in the attention economy [36] . As a consequence, news outlets are increasingly using catchy headlines, as well as outlandish and out-of-context claims that perform well in terms of attracting eyeballs and clicks [46] .When mainstream news media communicate scientific content to the public, the situation is by no means different [48] . Oversimplified scientific claims are rapidly shared in social media, while the scientific evidence that may support or refute them remains absent or locked behind pay-walled journals. For instance, on March 11th, 2020, an article in The Lancet Respiratory Medicine theorized that nonsteroidal anti-inflammatory drugs such as Ibuprofen could worsen COVID-19 symptoms [13] . Without referencing explicitly to this article, but motivated by it, the Minister of Health of France posted on Twitter, advising people to avoid Ibuprofen when possible. 1 His message was re-posted nearly 43 times and liked nearly 40 times. In contrast, a World Health Organization's message posted four days later, which insisted Ibuprofen was safe, was reposted only 7.5 times and liked only 8.5 times. 2 Fact-checking portals such as ScienceFeedback.co, among others, work closely with domain experts and scientists to debunk misinformation and bring nuance to potentially misleading claims. This remains, however, a labor-intensive and time-consuming task [19] . Table 1 : Approaches for Extraction, Clustering, and Contextualization as proposed by selected references F a c t-C h e c k in g P o r ta ls H a s s a n e t a l. [ 2 0 ] P o p a t e t a l. [ 4 3 ] Ja r a d a t e t a l. [ 2 1 ] S h a a r e t a l. [ 5 0 ] H a n s e n e t a l. [ 1 8 ] Z la tk o v a e t a l. [ 6 2 ] K a r a g ia n n is e t a l. [ 2 5 ] P in to e t a l. [ 4 2 ] P a v ll o e t a l. [ 4 0 ] S m e r o s e t a l. [ 5 3 ] L e v y e t a l. [ 2 9 ] S ta b e t a l. [ 5 4 ] P a tw a r i e t a l. [ 3 9 ] L ip p i a n d T o r r o n i [ 3 2 ] Ji a n g e t a l. [ 2 2 ] R e im e r s e t a l. [ 4 4 ] Y a o e t a l. [ 6 0 ] Z h o u e t a l. [ 6 1 ] H a m il to n e t a l. [ 1 7 ] W a n g e t a l. [ 5 7 ] D u o n g e t a l. [ 1 1 ] K o c h k in a e t a l. [ 2 6 ] S h a o e t a l. [ 5 1 ] C ia m p a g li a e t a l. [ 9 ] N a d e e m e t a l. [ 3 7 ] G a d -E lr a b e t a l. [ 1 6 ] C h e n e t a l. [ 7 ] SciClopsOn the other hand, despite misinformation circulating online exceeding the capacity of manual fact-checking, traditional news outlets are skeptical towards adopting fully-automated methods [52] . Their main concern is that such tools provide poorly-interpretable evidence (according to the journalistic standards), and any false judgment can lead to a downfall of the outlet's reputation. Indeed, even big tech companies were forced to suspend automated factchecking features due to similar criticism from news outlets [15] . Hence, the consensus regarding the usage of automation in journalism is that it should assist but not replace journalists and news consumers when they validate the veracity of news, enabling the movement onward the era of citizen journalism [38] . Our work focuses on scientific claims in news articles and social media postings. As scientific claims, we consider sentence-level segments that involve one or more scientific entities and are eligible for fact-checking. For example, the sentence "Ibuprofen can worsen COVID-19 symptoms" is a scientific claim because it involves two scientific entities (Ibuprofen and COVID-19) and implies a causal relation between them. To increase the coverage of our definition, we bound neither the number of entities nor the type of relation between them. Such non-deterministic definition makes the detection of scientific claims a challenging task, even for human annotators (details in §6.1). To address this task and enable the discovery of complex-structured claims, there is a need for advanced language models which are fine-tuned with domain-specific knowledge.Once we identify candidate scientific claims, we seek evidence that proves or contradicts them via contextualization, i.e., via building an enhanced context of trustworthy information. In the scientific domain, the appropriate context consists of related scientific papers. Grouping similar claims and linking them to related scientific literature is a complex task, to a large extent because of the different nature of the items that we are seeking to connect (i.e., social media postings, news articles, and scientific papers). These contain key passages that determine such connections, but are fundamentally different in terms of: i) verbosity, ranging from character-limited postings to extended scientific papers, and ii) complexity, ranging from a "social media friendly" style of writing to the more formal registry of journalism and academic writing.Finally, since there is a plethora of controversial claims (especially in the times of a pandemic), there is a need for a checkworthiness ranking that considers the prevalence and the reliability of the broadcasting medium. Providing a scientific context enables non-expert fact-checkers to verify claims with more precision than commercial fact-checking systems, and more confidence since the provided context is fully-interpretable (details in §6.3).Our Contribution. In this paper we describe SciClops (Figure 1 ), a method to assist manual verification of dubious claims, in scientific fields with open-access literature and limited fact-checking coverage. The technical contributions we introduce are the following:• pretrained and fine-tuned transformer-based models for scientific claim extraction from news and social media ( §3); • multimodal, joint clustering models for claims and papers that utilize both content and graph information ( §4); • methods for ranking check-worthy claims using a custom knowledge graph, and methods for creating enhanced scientific contexts to assist manual fact-checking ( §5); and • extensive experiments involving expert and non-expert users, strong baselines and commercial fact-checking systems ( §6).

RELATED WORK

Fact Checking Portals in general (Snopes.com), political (Politi-Fact.com), and scientific (ScienceFeedback.co) domains employ specialized journalists who manually: i) detect suspicious claims (extraction), ii) discover variants of these claims published in social and news media (clustering), and iii) find the appropriate prism under which they assess their credibility (contextualization). We summarize some automated methods tackling these steps in Table 1 .Claim Extraction. On weakly supervised models, Pavllo et al. [40] and Smeros et al. [53] generate complex rule-based heuristics to extract quotes from, respectively, general and scientific news articles. On traditional ML models, Levy et al. [29] and Stab et al. [54] propose learning models for claim detection and argument mining and introduce publicly available datasets, which we utilize to train our extraction models (details in §6.1). Hassan et al. [20] and Popat et al. [43] propose claim classification models that use the aforementioned fact-checking portals to verify political claims, while Patwari et al. [39] and Lippi and Torroni [32] propose, respectively, an ensemble and a context-independent model for claim extraction. Finally, Zlatkova et al. [62] propose a claim extraction model for images, Karagiannis et al. [25] propose a framework for statistical claims verification, and Pinto et al. [42] propose a method for identifying pairwise relationships between scientific entities.On neural ML models, Jaradat et al. [21] and Shaar et al. [50] detect and rank previously fact-checked claims using deep neural models, while Hansen et al. [18] also train a neural ranking model for check-worthy claims using weak supervision. Furthermore, Jiang et al. [22] use contextualized embeddings to factor fact-checked claims, while Reimers et al. [44] use also contextualized embeddings for claim extraction and clustering. Finally, CheckThat! Lab [2] features claim extraction and check-worthiness tasks which are oriented towards political debates in social media platforms.While the other approaches cover the cases of political, statistical, and visual claims, our approach provides the first dedicated solution for scientific claims. Given the complex nature of the scientific claims in terms of structure and vocabulary, our approach is based on advanced language models with contextualized embeddings that are fine-tuned with domain-specific knowledge. Furthermore, our approach works with arbitrary input text, e.g., from social media postings, blog posts, or news articles.Claim-Paper Clustering. Since our data contains multimodal information (the textual representation of claims and papers and the interconnections between them), we present multimodal clustering approaches that combine text and graph data modalities.Yao et al. [60] propose a unified convolutional network of terms and documents, while Zhou et al. [61] use weighted graphs that encode the attribute similarity of the clustered nodes. Hamilton et al. [17] introduce a methodology for jointly training embeddings based on text and graph information, while Reimers et al. [44] apply a numerical clustering on top of such embeddings. Finally, Wang et al. [57] propose a technique for training network embeddings that preserves the communities (clusters) of a graph, while Duong et al. [11] provide interpretable such embeddings.In our approach, we jointly cluster scientific claims and referenced papers, using both content and graph information. To the best of our knowledge, this is the first approach that deals with heterogeneous passages in terms of length and vocabulary type, which are also interconnected through a bipartite graph.Claim Contextualization. In addition to the extraction methods described above, the majority of which also provide contextualization/verification techniques (details in Table 1 ), Kochkina et al. [26] and Shao et al. [51] propose methods for automatic rumor verification using well-known fact-checking portals. Ciampaglia et al. [9] , Nadeem et al. [37] , and Chen et al. [7] use Wikipedia for fact-validation, while Gad-Elrab et al. [16] use custom knowledge graphs for generating interpretable explanations for candidate facts.While other approaches describe this step as "verification", since essentially they lookup a claim in a ground-truth knowledge base, we consider the general case in which claims rarely appear in such knowledge bases. As we observe in §6.3, this is a pragmatic assumption since the majority of the fact-checking effort targets non-scientific topics. As the verification of scientific claims is typically more demanding than other types of claims (e.g., ScienceFeedback.co has built an entire peer-reviewing system for this purpose), we propose a methodology that contextualizes claims based on related scientific literature and ranks them based on the prevalence and the reliability of the broadcasting medium.

CLAIM EXTRACTION

We address claim extraction as a classification problem at the sentence level, i.e., we want to distinguish between claim-containing and non-containing sentences. Below, we present the baseline and the advanced extractors that we evaluate in §6.1.

Baseline Extractors

We implement several baseline extractors that cover most of the related work on claim extraction described in §2: i) two complex heuristics which are used by state-of-the-art weakly supervised models [40, 53] ; ii) an off-the-shelf classifier trained with standard textual features which is used by state-of-the-art traditional ML models [20, 32] ; and iii) a transformer model which is used by stateof-the-art neural ML models [44, 50] .3.1.1 Grammar-Based Heuristic. The usage of reporting verbs such as "say, " "claim, " or "report, " is a typical element of pattern-matching heuristics for finding claims. Another element is the usage of domain-specific vocabulary; in the scientific context, common verbs in claims include "prove" and "analyze. " Thus, we compile a seed set of such verbs, which we extend with synonyms from WordNet [35] . In the following, we refer to this set of reporting verbs as . Scientific claims fundamentally refer to scientific studies, scientists or, more generally, scientific notions. Thus, we employ a shortlist of nouns related to studies and scientists (including "survey" or "researcher"). In the following, we refer to this set of nouns, together with the set of Person and Organization entities, as .Finally, to capture the syntactic structure of claims, we obtain part-of-speech tags from the candidate claim-containing sentences. Using this information, we construct a series of complex expressions over classes of words such as the following:where is a sentence, root (.) returns the root verb of the syntactic tree of a sentence, nsubj(.) returns the nominal subject, and dobj(.) the direct object of a sentence.

Context-Based

Heuristic. This heuristic is based on a frequent non-syntactic pattern, which is quite evident in our data: if an article is posted on social media, then its central claim is typically re-stated or minimally paraphrased in the postings. We investigate pairs (s,p) of candidate sentences , extracted from news articles, and postings , referencing these news articles. Our heuristic has the form:where sim( , ) denotes the cosine similarity between the embeddings representations of and , and pop( ) denotes the normalized popularity of , i.e., the raw popularity of over the sum of the popularity of all the 's that refer to . As popularity, we consider the sum of the re-postings and likes. Finally, threshold is a hyperparameter of our heuristic, which in our implementation is fixed to 0.9, yielding a good compromise of precision and recall. We note that this is the only proposed extractor that is not purely content-based since it also requires contextual information.

Random Forest Classifier.

To train this classifier, we apply a standard text-preprocessing pipeline, including stop-words removal and part-of-speech tagging. Then, we transform the candidate claim-containing sentences into embeddings by averaging the word embeddings provided by GloVe [41] . As we see in our evaluation ( §6.1), this classifier performs better than the aforementioned baselines; we also note that, compared to the complex transformer models, it is substantially less intensive in terms of computational resources and training time needed.

BERT Model.

One of the most successful state-of-the-art approaches to several NLP tasks, including classification, is the transformer model [10] . In our implementation we use the wellknown model BERT and particularly its version named bert-baseuncased [59] . The configuration parameters of the model are those suggested in a widely used software release of this model. 3 As the last layer of the transformer architecture of BERT (and the variants we introduce next), we add a standard binary classification layer with two output neurons, which we train using the datasets described in §6.1. During the training, we keep the rest of the layers of the model frozen at their initial parameters.

Fine-Tuned Transformer Extractors

Since BERT is originally trained on the generic corpus of Wikipedia, the word representations it generates are also generic. However, scientific claim extraction is a downstream task, where the model has to recognize patterns of a more narrow domain. Thus, we introduce three variants of BERT with domain-specific fine-tuning namely, SciBERT, NewsBERT and SciNewsBERT :• SciBERT is pretrained on top of BERT with a corpus from Seman-ticScholar.org containing~1 papers [3] . SciBERT has its own vocabulary that is built to best match the scientific domain. • NewsBERT is a new model that we introduce, built on top of BERT and pretrained on a freely-available corpus of~1 headlines published by the Australian Broadcasting Corporation [27] . • SciNewsBERT is also a new model that is pretrained like News-BERT, albeit, it is built on top of SciBERT instead of BERT. For training NewsBERT and SciNewsBERT we employ the standard tasks for training BERT -like models: i) Masked Language Modeling, where the model has to predict the randomly masked words in a sequence of text, and ii) Next Word Prediction, where the model has to predict the next word, given a set of preceding words. The hyperparameters used for training the models are the default proposed by the software release referenced above. Since both NewsBERT and SciNewsBERT need substantial computational power and training time, we make them publicly available for research purposes ( §7).

CLAIM-PAPER CLUSTERING

Contextualizing scientific claims requires to connect them with related scientific papers. To achieve this, our approach employs a clustering methodology. The clusters, composed of a mixture of claims and papers, must have high semantic coherence and ideally maintain the connections that exist between some of these claims and papers. These implicit connections are hyperlinks starting from news articles and social media postings containing these claims and ending on referenced papers, forming a sparse bipartite graph.The clustering methods that we employ are: i) Content-Based methods on top of either the raw text or an embeddings representation of the passages, ii) Graph-Based methods on top of the bipartite graph between the claims and the papers, or iii) Hybrid methods that combine the Content-Based and the Graph-Based methods. Furthermore, we consider both soft (overlapping) clustering (i.e., passages Table 2 : Clustering notation. The embeddings dimension (dim) of our models is 300. Matrix L has a 1 in position (c, p), iff a news article or a social media posting containing claim c has a hyperlink to paper p. Each row of the clustering matrices (C ′ and P ′ ) contains the probability of a claim or a paper to belong to a cluster; for hard clustering it is "one-hot", i.e., it has a single non-zero element, and for soft clustering it is a general probability distribution.

Symbol

Descriptioncan belong to more than one cluster), and hard (non-overlapping) clustering (i.e., passages must belong to exactly one cluster). The notation used in this section is summarized in Table 2 .

Content-Based Clustering

Our baseline is content-based (topic) clustering. According to this approach, we assume that claims and papers are represented in the same latent space, in which we compute topical joint clusters. This approach does not consider the interconnections (i.e., the bipartite graph) between the claims and the papers.For topic modeling, we use Latent Dirichlet Allocation (LDA), an unsupervised statistical model that computes a soft topic clustering of a given set of passages [4] . We also use Gibbs Sampling Dirichlet Mixture Model (GSDMM), which assumes a hard topic clustering and is more appropriate for small passages such as claims [30] . When the passages are projected in an embeddings space, we use either the generic Gaussian Mixture Model (GMM), which computes a soft clustering by combining multivariate Gaussian distributions [45] , or K-Means [33] , which computes a hard clustering. Finally, we test these methods with and without reducing the embeddings dimensions using Principal Component Analysis (PCA) [14] .

Graph-Based Clustering

Since our data is multimodal, an alternative to pure Content-Based clustering is pure Graph-Based clustering. We define this problem as an optimization problem, introducing an appropriate loss function that we want to minimize. Our goal is to compute the optimal clusters C ′ and P ′ , and our evaluation criterion is the extent to which C ′ and P ′ fit with the interconnection matrix L. Hence, we propose the following loss function:This loss function is also known as the Reconstruction Error and is commonly used in Linear Algebra for factorization and approximation problems. By applying this loss function, we force C ′ and P ′ to be aligned with L: the claims that appear in a news article should belong to the same cluster as the papers referenced by this article.A degenerate solution to the problem, if we use only this loss function, is a uniform clustering for both claims and papers. The loss is minimized, but the clustering is useless, because the probability of any claim and any paper to belong to any cluster is uniform. To overcome this problem, we exploit the following technique that is widely used in image processing [28] .In row-stochastic matrices (i.e., matrices that each row sums to 1), a uniform soft clustering has lower Frobenius Norm than a non-uniform clustering. Consequently, any hard clustering has the maximum possible Frobenius Norm. Thus, we introduce a regularizer that imposes non-uniformity on the clusters by penalizing low Frobenius Norms for C ′ and P ′ :where V is the set of optimizable variables of our model, and a hyper-parameter that in our experiments defaults to = 0.3. We use a different regularizer in each alternative version of the model that we describe below. These alternative versions have varying flexibility, i.e., either both C ′ and P ′ are optimizable variables (C ′ , P ′ ∈ V), or one of them is fixed, thus not optimizable (C ′ ∉ V or P ′ ∉ V). If both of them are fixed (C ′ , P ′ ∉ V) then the model has no optimizable variables (V = ∅). Below we present the alternative versions of the model.

Graph-Based Adaptation.

In this alternative (entitled GBA-CP), we start with arbitrary cluster assignments for C ′ and P ′ , which we both optimize based on the loss function. This approach completely ignores the semantic information of C and P and adapts arbitrarily the clusters to the interconnection matrix L. This behavior of GBA-CP is confirmed in our experiments ( §6.2). In a less aggressive approach, we fix either C ′ or P ′ using one of the Content-Based algorithms explained above, and optimize only one clustering (the non-fixed) based on the loss function. We entitle these alternatives as GBA-C for optimizing C ′ , and GBA-P for optimizing P ′ .

Graph-Based Transformation.

In this alternative (entitled GBT-CP), instead of optimizing directly C ′ and P ′ , we optimize the weights of the non-linear neural transformations f C and f P . The architecture of f C and f P consists of a hidden layer of neurons with a rectified linear unit (ReLU ), and a linear Softmax classifier that computes the overall cluster-membership distribution. We use the same loss function as above where C ′ = f C (C) and P ′ = f P (P).Similarly as above, in a less aggressive approach, we fix C ′ or P ′ using a Content-Based algorithm, and optimize only the weights of one transformation (f C or f P ). We entitle these alternatives as GBT-C for optimizing f C , and GBT-P for optimizing f P .

Hybrid Clustering

The last clustering model that we propose is a Hybrid model that combines a Content-Based and a Graph-Based model. As we point out in our experimental evaluation ( §6.2), there is a trade-off between these two approaches in terms of the semantic and interconnection coherence of the computed clusters. Hence, we introduce a tunable model that controls this trade-off.Our model initializes the clusters C ′ init and P ′ init using a Content-Based model. Then, it uses an Alternate Optimization (AO) approach to jointly compute the final C ′ and P ′ that adjust best to L. More specifically, it iteratively freezes one of the two clusters and adjusts the other, until they both converge to an optimal state. The loss function of this model is the following:where is a hyper-parameter that controls the trade-off between Content-Based and Graph-Based clustering. In our experiments for brevity we present results for three values: AO-Content for = 0.1, AO-Balanced for = 0.5, and AO-Graph for = 0.9.

CLAIM CONTEXTUALIZATION

In the previous section, we explain how we construct claim-paper clusterings in an unsupervised fashion. These clusterings give already an initial context for claims since they relate them with relevant scientific literature. In this section, we describe how we rank claims within clusters based on their check-worthiness and how we complement their fact-checking context by discovering (when available) previously verified related scientific claims.

Check-Worthy Claim Ranking

The check-worthiness of a scientific claim depends on its intent (e.g., whether it implies a causal relation or describes a particular aspect of an entity) and its prevalence (e.g., in news and social media). We construct a custom in-cluster knowledge graph in which we encode the intent of the claims into the topology of the graph and the prevalence of the claims into the weighting of the graph.In-Cluster Knowledge Graph. We construct a knowledge graph by using terms from a domain-specific vocabulary as nodes. The edges of the graph denote the co-occurrence of two terms in the same claim (e.g., the claim "Ibuprofen can worsen COVID-19 symptoms" contributes the edge (Ibuprofen -COVID-19) ).Since the dataset we use in our evaluation is health-related (details in §6), we use the vocabulary of CDC A-Z Index 4 that includes health terms used by laypeople and professionals. We note that the rest of the methodology is independent of the domain of the dataset, and can be simply adapted by selecting an appropriate vocabulary.Graph Topology. We distinguish between two types of topologies based on two different intents:• Causality-Based topologies which contain nodes from distinct classes such as: i) "Diseases and Disorders" (e.g., Depression, Influenza, and Cancer), and ii) "Conditions, Symptoms, Medications, and Nutrients" (e.g., Pregnancy, Fever, and Red Meat). A directed edge between two nodes of a different class denotes, to a certain degree, a causal relation between these nodes [8] . • Aspect-Based topologies which focus on the "ego-network" for one particular node (e.g., "COVID-19") and the different aspects regarding this node (e.g., "Origin" or "Mortality Rate") [34] .Graph Weighting. The weighting scheme that we employ combines two criteria, namely the popularity and the reputation of the primary sources (i.e., the social media postings and the news articles) from which the claims were extracted. The popularity of a posting is computed as the sum of the number of re-postings and likes. If multiple postings share the same claim, then their popularity is aggregated. Then, Box-Cox transformation ( = 0) [5] , to diminish the effect of the long-tail distribution, and Min-Max normalization in the interval [0, 1] are applied.On the other hand, the reputation of a news article is entailed from the reputation of the news outlet that publishes the article. In the context of this paper, we use the outlet scores compiled by the American Council on Science and Health (ACSH ) [1] , which we also normalize in the interval [0, 1]. News outlets that are not on ACSH 's list (i.e., "long-tail" outlets hosting only 13.5% of the total articles in our collection) are assigned a neutral score (0.5).Since we want to discover claims that are popular and come from low-reputable sources, we linearly combine the two metrics for each edge , using a tuning parameter as follows:weight( ) = popularity( ) + (1 − ) (1 − reputation( )) In our implementation, we slightly favorite low reputation over popularity; thus, we use = 0.4.Claim Ranking. We rank the edges, and consequently the claims, of the Causality-Based topologies using the Betweenness Centrality metric [6] , and the Aspect-Based topologies using the in-Degree metric. Examples of check-worthy claims in our data include the term pairs: (Autism -Vaccines), (Breast Cancer -Abortion), and (Chemotherapy -Cannabis) (details in §6.3).

Enhanced Fact-Checking Context

The final step for contextualizing the claims is to relate them (when available) with previously verified claims. To retrieve such claims, we use ClaimsKG [55] , a knowledge graph that aggregates claims and reviews published using ClaimReview 5 . After filtering out, based on the mentioned entities, claims with non-scientific content (i.e., 62.3% of the total claims), we end up with a final set of˜4 scientific claims, out of which 79.8% has been determined to be False, and 20.2% has been determined to be True. We relate claims by computing their Semantic Textual Similarity [31] and setting an appropriate threshold (0.9 in our experiments).Our final fact-checking context for scientific claims consists of related scientific papers and news articles from the same cluster, and, if available, related, previously verified claims. As we see in our experiments ( §6.3), this enhanced context improves the verification accuracy and confidence of non-expert fact-checkers and helps them outperform commercial fact-checking systems.

EXPERIMENTAL EVALUATION

In this section we evaluate the methods for extraction ( §6.1), clustering ( §6.2), and contextualization ( §6.3) of scientific claims.Raw Dataset. We evaluate all three methods on a state-of-theart dataset for measuring health-related scientific misinformation [53] . This dataset has the form of a directed graph, from social media postings to news articles to scientific papers, where edges denote a hyperlink connection. The˜50 social media postings of the dataset include the text of the postings as well as popularity indicators such as the number of re-postings and likes. The˜12 news articles of the dataset include articles from mainstream news outlets (e.g., theguardian.com or popsci.com), as well as from alternative blogging platforms (e.g., mercola.com or foodbabe.com). Finally, the˜24 scientific papers of the dataset include peer-reviewed or gray literature 6 papers hosted at universities, academic publishers, or scientific repositories (e.g., Scopus, PubMed, JSTOR, and CDC). We note that the overall volume of the dataset simulates the typical news coverage on health-related topics for a period of four months.

Evaluation of Claim Extraction

The evaluation of the extractors is two-fold; first, we validate their accuracy using a widely-used clean and labeled dataset, and then, we use them in a real-world scenario where we apply them on the raw dataset described above, and evaluate them via crowdsourcing.

Training.

Since there is no specific training dataset for the task of scientific claim extraction, we use two datasets mainly used for argumentation mining, namely UKP [54] and IBM [29] . We train our classifiers using the balanced union of the two datasets (˜11 positive and negative samples). In the following, we refer to this dataset as the Generic Dataset of claims. We also train our classifiers with a "science-flavored" dataset derived from the UKP and IBM datasets. Specifically, in this dataset, we oversample claims regarding, e.g., "abortion" and downsample claims regarding, e.g., "school uniforms". We apply this data augmentation by manually processing based on the "general topic" field that exists in both UKP and IBM datasets. The described dataset is also balanced, containing˜16 positive and negative samples, and in the following, we refer to it as the Scientific Dataset of claims.

Cross Validation.

We perform a 5-fold cross validation over the datasets described above; the results are shown in Table 3 . We observe that the Heuristic-Based extractors perform poorly for this task, which confirms that it is a demanding task with many corner cases. Remarkably, the Context-Based heuristic, which is domain-agnostic, achieves identical accuracy with the Grammar-Based heuristic, which contains manually curated grammar rules. We also observe that the Random Forest classifier does not perform extremely worse than the Transformer-Based models, while being more eco-friendly in terms of resources and training time needed.The performance of the transformer-based models confirms the fact that they are state-of-the-art in most NLP tasks. However, from this task, we do not see the benefits of the domain-specific pretraining. On the Generic Dataset, BERT, which is pre-trained on a generic corpus, performs better, while on the Scientific Dataset, SciNewsBERT, which is pre-trained on a scientific and a news corpus, performs better; nonetheless, their difference is negligible. The real difference among these models is shown in the next experiment. 

Crowd Evaluation.

We collect boolean labels for 700 sentences extracted from the raw dataset described above by asking the crowd workers a simple classification question (i.e., whether a given sentence contains a scientific claim or not). We use the platform Mechanical Turk, asking input from three independent crowd workers per sentence (57 in total). To ensure high-quality annotations, we employ what the platform calls Master Workers, i.e., the most experienced workers with approval rate greater than 80%. Finally, we consider Strong Agreement among crowd-workers, the 3 out of 3 agreement, and Weak Agreement the 2 out of 3 agreement. We note that there are 77 out of the 700 sentences for which the majority of the annotators answered N/A, because they could not distinguish whether these sentences contain a claim or not. For example, interrogative sentences like "What? Ibuprofen Can Make You Deaf?" confused the annotators, while similar affirmative sentences like "Tylenol PM Causes Brain Damage" were easily identified as scientific claims. The remaining 623 sentences are divided into two subsets; i) sentences having Strong Agreement among annotators, with 82 claims (positive examples) and 242 non-claims (negative examples), and ii) sentences having Weak Agreement among annotators, with 125 claims and 174 non-claims.We observe that especially the subset with Strong Agreement is highly unbalanced, which is indeed a realistic scenario if we consider the ratio of claim and non-claim containing sentences in typical news articles. Furthermore, annotators fully agree that a sentence contains a scientific claim for less than 12% of total the sentences, which confirms it is a highly confusing task.Results. The overall results of the comparison of the extraction models are summarized in Table 4 . For all the models, we use the following naming convention: the suffix -gen is used to denote that models are trained on the Generic Dataset explained in §6.1.1, while suffix -sci is used to denote that models are trained on the Scientific Dataset also explained in §6.1.1. This convention does not apply to heuristic models that do not require training.We observe that all the -gen models have better or equally good recall as the respective -sci models. This happens because -gen models have been trained equally towards all the labeled claims and have learned to better recognize the structure of a claim. After analyzing the errors of the models, we noticed that claims with simple structure like "Repetitive behaviors in autism show sex bias early in life" were identified more from -gen than from -sci models. On the other hand, -sci models, which have been optimized for the narrow scientific domain, are more selective, hence they show in general better precision than the respective -gen models.Focusing more on the variants of BERT, we observe that taskspecific pretraining boosts the performance of the model, which is not visible in the first experiment. Specifically, we see that pretraining on both scientific and news domain gives the best results. One illuminative example is the claim "Galactosides Treat Urinary Tract Infections Without Antibiotics", where Galactosides is a word that does not appear in the basic vocabulary of BERT 7 , however, it appears in the extended vocabulary of SciBERT 8 and SciNewsBERT.Finally, it is noteworthy that the Random Forest model provides quite comparable results to the transformer-based models, while being, as stated above, a much lighter and faster-to-train model.

Evaluation of Claim-Paper Clustering

Since we construct a bimodal clustering of claims and papers, we evaluate its quality with respect to two axes; a good-quality clustering must contain clusters of semantically related claims and papers (Semantic Coherence), and adhere to the implicit connections between these claims and papers (Interconnection Coherence). Semantic Coherence. To measure the semantic coherence of a clustering, we compute a modified version of the Average Silhouette Width (ASW ) [47] . The first modification is that the distance used is not a metric distance (e.g., Euclidean distance) but a semantic distance (Semantic Textual Similarity (STS)). The second modification is that we generalize the metric for two (or more) joint clusterings. The original metric computes the average distance between the centroid of each cluster and its elements. In our case, since we have two joint clusterings for claims and papers, we compute the metric for all the combinations of centroids (¯) and elements ( ) of each cluster. Thus, the modified ASW is computed as follows:where centroids consists of the claims centroid and the papers centroid of each cluster. Finally, we report the mean ASW across all clusters. This cross-computation of the metric allows capturing the semantic coherence of the clusters both individually and jointly. Interconnection Coherence. To measure the interconnection coherence of the clusterings (i.e., the adaptivity of the clusterings towards the interconnection matrix L), we use ideas from linkbased recommendation. First, we compute a hard clustering for claims and papers:Since, as we explain in Table 2 , each row of C ′ and P ′ contains the probability of a claim or a paper to belong to a cluster, when we compute over rows we obtain a hard clustering, while when we compute over columns we obtain the cluster centroids. For example, given a single claim and three clusters 0 , 1 , 2 : Next, we use one clustering (e.g., of claims) to recommend possible instances of the other clustering (e.g., of papers). The recommendation is content-agnostic and exploits only the interconnection matrix L. Formally:where ⊙ is the Hadamard (element-wise) product. For the same claim , papers 1 and 2, and clusters 0 , 1 , 2 we have: To compute the recommendation quality, we utilize the metric of Recall@k (R@k), which measures the ratio in which the correct cluster is recommended among the top-k results. We report the mean of the R@k for the claims and the papers clustering.Results. The results of the evaluation are shown in Table 5 . As we observe, the Content-Based (baseline) clustering techniques that use a textual representation of claims and papers (i.e., LDA and GSDMM), generate clusters with lower Semantic Coherence than the ones that use an embeddings representation (i.e., GMM and K-Means). This is partially explained by a vocabulary mismatch: the language used in papers is more complex and contains more scientific terms than the one used in social and news media (where the claims derive from). Thus, embeddings representations have the advantage of capturing the semantic proximity of topics, even if these topics occur from two heterogeneous vocabularies. Furthermore, we observe that soft clustering techniques (i.e., LDA and GMM) generate, in general, clusters with higher Semantic Coherence than the respective hard clustering techniques (i.e., GSDMM and K-Means), indicating that the theme of claims and papers is usually multifaceted. Finally, we observe that the dimensionality reduction, performed by PCA, is not helpful in the context of this task.Regarding the Graph-Based techniques, we see that they construct clusters with high Interconnections Coherence but the lowest Semantic Coherence. Not surprisingly, GBA-CP achieves the maximum Interconnections Coherence since, as we explain in §4.2, it arbitrarily adapts the clusters to the interconnection matrix L.Overall, we observe that the most robust technique in terms of balance between Semantic and Interconnections Coherence is the Hybrid technique (AO-Balanced), which computes a soft clustering based on an embeddings representation and considers both the text and the graph modality of the dataset equally.

Evaluation of Claim Contextualization

The overall evaluation of our method is performed with an experiment that involves expert and non-expert fact-checkers as well as two state-of-the-art commercial systems. Using SciClops, we extract, cluster, and finally select the top-40 check-worthy scientific claims in the data collection. The topics of the claims are heterogeneous, covering controversial online discussions such as the usage of therapeutic cannabis in modern medicine, the consumption of small amounts of alcohol during pregnancy, and the effect of vaccines in disorders such as autism. Claim Post-Processing. We notice that in some of the claims, redundant information that could confuse the fact-checkers is mentioned (e.g., we find the claim "Donald Trump has said vaccines cause autism, " in which the scientific question is whether "vaccines cause autism" and not whether Donald Trump made this statement). Thus, to avoid misinterpretations and to mitigate preexisting biases for or against public figures, we replace from these claims all the Person and Organization entities with indefinite pronouns. Non-Experts. We employ crowdsourcing workers using the same setup described in §6.1, and ask them to evaluate the Validity of each claim in a Likert Scale [23] (from "Highly Invalid" to "Highly Valid"). We also ask them to rate their Effort to find evidence and their Confidence that the evidence they found is correct.We divide non-experts into one control group of Non-Experts Without Context, and two experimental groups of Non-Experts With Partial Context and Non-Experts With Enhanced Context: • Non-Experts Without Context are shown a bare scientific claim with no additional information, as they would read it online in, e.g., a messaging app or a social media posting. • Non-Experts With Partial Context are shown a scientific claim and its source news article, i.e., the news article from which the claim was extracted. • Non-Experts With Enhanced Context are shown a scientific claim, its source news article, and: i) the top-k news articles where the same or similar claims were found, ii) the top-k most relevant papers, and, if available, iii) the top-k most similar, previously verified claims. To avoid overwhelming this experimental group with redundant information, we set = 3.Experts. We ask two independent experts to evaluate the validity of the claims. Each expert evaluated all 40 claims independently, and was given the chance to cross-check the ratings by the other expert and revise their own ratings, if deemed appropriate. Overall, we use the average of the two expert ratings as ground-truth. Commercial Systems. Finally, for the verification of the same scientific claims, we use two commercial systems for fact-checking, namely ClaimBuster [20] and Google Fact Check Explorer 9 :• ClaimBuster is a system used massively by journalists which initially aimed at detecting important factual claims in political discourses; however, its current architecture allows for investigating any kind of check-worthy claims (details in §2). • Google Fact Check Explorer is also an exploration tool used by journalists to verify claims published using the tagging system of ClaimReview; we note that ClaimReview is also exploited in the contextualization step of SciClops (details in §5.2).To homogenize the scores of these systems with the scores of the fact-checkers, we quantize them to the aforementioned Likert Scale.Results. Results are summarized in Table 6 . Given the groundtruth provided by the experts, we measure the accuracy of the three aforementioned groups of non-experts and the two commercial systems using the Root Mean Square Error (RMSE). We observe that ClaimBuster performs better than our control group of Non-Experts Without Context while providing a solution without human intervention. Furthermore, we observe that Google Fact Check Explorer performs poorly, mainly because only 20% of the queried claims were present in the fact-checking portals it monitors (e.g., the claim "Vaccines cause Autism" is present in the fact-checking section of USA Today [56] , while the Contradictory Claims described next are absent from all the fact-checking portals).Finally, regarding the non-expert human fact-checkers, we observe that the more contextual information is available, the more accurately they rate the claims. Indicatively, the RMSE of Non-Experts With Enhanced Context is only 50% greater than the RMSE across Experts. Overall, we see that, when the under-verification claims derive from a narrow scientific domain, non-expert human fact-checkers, provided with the proper fact-checking context, may outperform state-of-the-art commercial systems.Case Study: Contradictory Claims. Within the set of underverification claims, we noticed two contradictory claims. The first claim opposes the use of therapeutic cannabis for treating Post-Traumatic Stress Disorder (PTSD) and comes from a mainstream news outlet (CNN ). 10 The second claim supports the use of cannabis for treating PTSD and comes from a popular health blog (MensJournal). 11 Current scientific understanding supports the first claim (from CNN ), but not the second one (from MensJournal), as evidenced by a paper of the Journal of Clinical Psychiatry [58] . 10 CNN : "Marijuana does not treat chronic pain or post-traumatic stress disorder." [49] 11 MensJournal: "Marijuana can help battle depression, anxiety, post-traumatic stress disorder, and even addictions to alcohol and painkillers." [24] As we show in Table 6 , ClaimBuster and all the groups of Non-Experts mostly support the claim from CNN as valid. Moreover, as discussed above, Google Fact Check Explorer provides no answer for these two claims since they are not present in the monitored fact-checking portals. Indeed, only Non-Experts With Enhanced Context were able to indicate that the claim from MensJournal is invalid, mainly because SciClops provided a fact-checking context that included the paper from the Journal of Clinical Psychiatry which debunks the claim even in its title. 12 Case Study: Confidence & Effort. As we observe in Figure 2 , Non-Experts that were shown the Enhanced Context of claims were more confident in their verification, additionally to being more accurate than the other two groups of users, which is partially explained by the fact that the provided context is fully-interpretable (as explained above), thus more trustworthy. However, the same users' self-assessment of their effort as well as their actual work time was higher than the other two groups of users, which is explained by the fact that they had to visit more potential verification sources.

CONCLUSIONS

We have presented an effective method for assisting non-experts in the verification of scientific claims. We have shown that transformer models are indeed the state-of-the-art on scientific claim detection, however, they require domain-specific fine-tuning to perform better than other baselines. We have also shown that, by exploiting the text of a claim and its connections to scientific papers, we effectively cluster topically-related claims and papers, as well as that, by building an in-cluster knowledge graph, we enable the detection of check-worthy claims. Overall, we have shown that SciClops can build the appropriate fact-checking context to help non-expert fact-checkers verify complex scientific claims, outperforming commercial systems. We believe that our method complements these systems in domains with sparse or non-existing ground-truth evidence, such as the critical domains of science and health. Reproducibility. All the data, code, models, as well as expert and crowd annotations used for this paper are publicly available for research purposes in http:// scilens.epfl.ch.

