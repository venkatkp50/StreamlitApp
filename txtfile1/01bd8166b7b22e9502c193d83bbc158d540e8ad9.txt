

and they view the latter two more favorably [7] . In Fiske's Stereotype Content Model [9] , which expresses common stereotypes as a combination of "competence" and "warmth," Asians belong to a set of "highstatus, competitive out-groups" and rank high in competence but low in warmth [10] .The New York Times, which calls itself the "Newspaper of Record," is the most influential newspaper in the USA and possibly even in the Anglophonic world. The digital edition boasts 91 million unique monthly visitors in the US alone [11] , and while the paper's reach may be impressive, it is yet more significant that the readership of The New York Times represents an elite subset of the American public. Print subscribers to the New York Times have a median household income of $191,000, three times the median income of US households writ large [12] . Despite the paper's haughty and sometimes condescending reporting, the paper "has had and still has immense social, political, and economic influence on American and the world" [13, page 81].A small body of prior work studies the The New York Times and howThe New York Times reports on China. Blood and Phillips uses autoregression methods on time series data to predict public opinion [14] . Wu et al. use a similar autoregression technique and find that public sentiment regarding the economy predicts economic performance and that people pay more attention to economic news during recessions [15] .Peng finds that coverage of China in the paper has been consistently negative but increasingly frequent as China became an economic powerhouse [16] . There is very little other scholarship that applies language processing methods to large corpora of articles from The New York Times or other leading papers. Atalay et al. is an exception that uses statistical techniques for parsing natural languages to analyze a corpus of newspaper articles from The New York Times, the Wall Street Journal, and other leading papers in order to investigate the increasing frequency of information technologies in newspaper classifieds [17] .Our paper aims to advance understanding of how Americans form their attitudes on China with a case study of how The New York Times may shape public opinion. We hypothesize that media coverage of foreign nations affects how Americans view the rest of the world. This model deliberately simplifies the interactions between audience and media and sidesteps many debates. We confine our analysis to a corpus of 267,907 articles on China from The New York Times articles because of the paper's singular influence and importance. We quantify media sentiment with BERT, a state-of-the-art natural language processing model with deep neural networks, and segment sentiment into eight topics to capture the nuance of American opinion toward China. We then use conventional statistical methods to link media sentiment to a longitudinal data set constructed from 101 cross-sectional surveys of the American public's views on China. We find strong correlations between how The New York Times reports on China in one year and the views of the public on China in the next. The correlations agree with our hypothesis and imply a strong connection between media sentiment and public opinion.

Results

We begin with a demonstration of how the reporting of The New York Times on China changes over time, and we follow this with an analysis of how coverage of China might influence public opinion toward China.

Quantifying media sentiment and public opinion

We quantify media sentiment with a natural language model on a large-scale corpus of 267,907 articles on China from The New York Times published between 1970 and 2019. To explore sentiment from this corpus in greater detail, we map every article to a sentiment category (positive, negative, or neutral) in eight topics: ideology, government & administration, democracy, economic development, marketization, welfare and well-being, globalization, and culture.We do this with a three-stage modeling procedure. First, two human coders annotate 873 randomly selected articles with a total of 18,598 sentences as expressing either positive, negative, or neutral sentiment in each topic. We treat irrelevant articles as neutral sentiment. Secondly, we fine-tune a natural language processing model BERT (Bidirectional Encoder Representations from Transformers [18] ) with the human-coded labels. The model uses a deep neural network with 12 layers. It accepts sentences (i.e., word sequences of no more than 128 words) as input and outputs a probability for each category.We end up with two binary classifiers for each topic for a grand total of 16 classifiers: an assignment classifier that determines whether a sentence expresses sentiment in a given topic domain and a sentiment classifier that then distinguishes positive and negative sentiment in a sentence classified as belonging to a given topic domain. Thirdly, we run the 16 trained classifiers on each sentence in our corpus and assign category probabilities to every sentence. We then use the probabilities of all the sentences in an article to determine the article's overall sentiment category (i.e., positive, negative, or neutral) in every topic.As demonstrated in Table 1 A positive value shows a more favorable attitude than that in 1974, and a negative value represents a less favorable attitude than that in 1974.Because of this, the trends in sentiment changes year-over-year are of interest, but the absolute values of sentiment in a given year are not (See Methods for details). As shown in Figure 1 , public opinion towards China has varied greatly from 1974 to 2019. It steadily climbed from a low of -24% in 1976 to a high of 73% in 1987, and has fluctuated between 10% to 48% in the intervening 30 years.

Trend of media sentiment

The New York Times has maintained steady interest in China and Chinaadjacent topics over the years in our sample and has published at least 3,000 articles on China in every year of our corpus. Figure Despite the remarkable diversity of sentiment toward China across the eight topics, sentiment within each of the topics is startlingly consistent over time. This consistency attests to the incredible stability of American stereotypes towards China. If there is any trend to be found here, it is that the main direction of sentiment in each topic, positive or negative, has grown more prevalent since the 1970s. This is to say that reporting on China has become more polarized, which is reflective of broader trends of media polarization [4], [19] .

Media sentiment affects public opinion

To reveal the connection between media sentiment and public opinion, we run a linear regression model (Equ 1) to fit public opinion with media sentiment from current and preceding years.where µ t denotes public opinion in year t with possible values ranging from −1 to 1. F kjs is the fraction of positive (s = positive) or negative (s = negative) articles on topic k in year j. A non-negative coefficient β kjs quantifies the importance of F kjs in predicting µ t .There is inertia to public opinion. A broadly-held opinion is hard to change in the short term, and it may require a while for media sentiment to affect how the public views a given issue. For this reason, j is allowedanywhere from zero to a couple of years ahead of t. In other words, we inspect lagged values of media sentiment as candidate predictors for public attitudes towards China.We seek an optimal solution of media sentiment predictors to explain the largest fraction of variance (r 2 ) of public opinion. To reduce the risk of overfitting, we first constrain the coefficients to be non-negative after reverse-coding negative sentiment variables, which means we assume that positive articles have either no impact or positive impact and that negative articles have either zero or negative impact on public opinion.Secondly, we require that the solution be sparse and contain no more than one non-zero coefficient in each topic:gives the number of non-zero coefficients of topic k predictors.The solution varies with the number of topics we let the model use for fitting. As shown in Table 2 , if we allow fitting with only one topic, we find that sentiment on Chinese culture has the most explanatory power, accounting for 31.2% of the variance in public opinion. We run a greedy strategy to add additional topics that yield the greatest increase in explanatory power, resulting in eight nested models ( Table 2 ).The explanatory power of our models increases monotonically with the number of allowed topics but reaches a saturation point at which the marginal increase in variance explained per topics decreases after only two topics are introduced (See Table 2 ). To strike a balance between simplicity and explanatory power, we use the top two predictors, which are the positive sentiment of culture and the negative sentiment of democracy in the previous year, to build a linear predictor of public opinion that can be written as:where 

New York Times data

The New York Times data were accessed using official online APIs. We use The New York Times query API to search for 267,907 articles fromThe New York Times that mention the keywords China, Chinese, Beijing, Peking, or Shanghai.

Topics

We use eight predefined topics to segment content from The New York Culture includes sports, music, movies, literature, art, architecture, food, and museums about China.

Media sentiment annotation and extraction

Media sentiment is defined as perceived sentiment, i.e., asking if an article would make a typical American reader view China more or less favorably. For each of eight possible topics mentioned in an article, we assign a sentiment label: strongly positive, neutral, or strongly negative.Weak sentiments are considered neutral, and most articles typically cover 3 to 4 topics.We extract media attitudes from our data with a three-stage procedure. Firstly, all authors read and labelled 42 representative articles to create a golden standard for our labeling criteria. We then used these articles to train two undergraduate students to read and annotate an additional 712 articles. Finally, we use Bidirectional Encoder Representations from Transformers (BERT) [18] , a natural language processing model, to learn from the tagged text and classify the rest of the articles in our sample. We started with BERT's pre-trained 12-layer uncased neural network, fine-tuned it with 873 hand-labeled, and used the tuned model to predict the sentiments of the full sample of 267,907 articles.We built two BERT classifiers for each topic, an "assignment" classi- where η it ∼ N (0, 1) is a normally distributed idiosyncratic error of individual i away from the population mean µ t .Responding a survey q is actually a measurement that "discretize" an real-valued individual y * it to an integer response y itq , satisfyingwhere survey-specific parameters τ q,Kq define attitude ranges [τ q0 = −∞, τ q1 , . . . , τ qKq = +∞] that partition continuous attitude y * it into categorical responses y itq with K q mutually exclusive ordered values. Here we assume that for any given survey, the level of response scales are understood consistently by all respondents, regardless of the time of the interview. Therefore survey-specific parameters τ q,· are assumed to be independent from time.Combining Equ 3 and Equ 4 predictsEquation 5 suggests that the individual's idiosyncrasies can be expressed as differences between the threshold and the sample mean.Note that we assume the population mean µ t and individual attitude y * it are shared by all surveys, i.e., they do not vary across surveys, though the measurements y itq could be different.Now we are able to write down the probability an individual i responding y itq = k in the form of η it ,where Φ(z) is the cumulative density functions of normal distribution.The unknown parameters τ , µ, η are estimated by maximizing the joint likelihood L, L = where N qtk = i I(y itq = k) is the number of individuals responding k to survey q in year t.Note that L is translation invariant, L(τ, µ, η) = L(τ + c, µ + c, η + c), ∀τ, µ, η, constant c,has an infinite number of parallel optimal solutions by finding any single solution (τ ,μ,η) and adding an arbitrary constant c. There-fore we need to manually fix the value of one parameter in (τ ,μ,η) to avoid infinitely parallel solutions. Without loss of generality, in practice µ of the first year of the data (1974) is set to 0. This will not affect the estimated trend except a constant shift. Our estimation turns out a constrained optimization problem as follows,A key to solving Equ 7 with multiple surveys relies on an assumption that µ t does not vary across surveys,In fact, µ t propagates the information between surveys to properly anchor their thresholds τ . Two surveys can be anchored only when 

Fitting public attitudes

We 

