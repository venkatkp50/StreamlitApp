Abstract

148 30 Introduction, Results and Discussion (excluding figure captions and tables): 5228 31 Methods: 3103 32 2 Abstract 33As humans we communicate important information through fine nuances in our facial expressions, 34 but because conscious motor representations are noisy, we might not be able to report these fine 35 but meaningful movements. Here we measured how much explicit metacognitive information 36 young adults have about their own facial expressions. Participants imitated pictures of themselves 37 making facial expressions and triggered a camera to take a picture of them while doing so. They 38 then rated confidence (how well they thought they imitated each expression). We defined 39 metacognitive access to facial expressions as the relationship between objective performance 40 (how well the two pictures matched) and subjective confidence ratings. Metacognitive access to 41 facial expressions was very poor when we considered all face features indiscriminately. Instead, 42 machine learning analyses revealed that participants rated confidence based on idiosyncratic 43 subsets of features. We conclude that metacognitive access to own facial expressions is partial, 44 and surprisingly limited.3IntroductionPrecise motor planning and execution can occur without the brain having explicit, conscious 48 access to the exact position of our limbs, or the exact degree of contraction of our muscles 1-3 . For 49 instance, we can simultaneously walk, speak, and gesticulate successfully while concentrating on 50 an argument and not on the movements that enable it, and we are furthermore unable to 51 accurately report the state of each of our muscles. Although explicit access to proprioceptive 52 signals in highly routinary tasks like walking or talking may be unnecessary, it might be beneficial 53 in some other cases. For example, it has been suggested 4 that metacognitive reasoning plays a 54 central role in developing and improving motor expertise: if an experienced actor has a detailed 55 and sophisticated representation of an ideal facial expression to communicate emotion, they are 56 better able to detect and correct deviations from the ideal, leading in turn to more accurate and 57 consistent performance.Proprioceptive information about our limbs and their movements is thought to originate primarily 59 from muscle spindles, together with skin receptors, Golgi tendon organs, and joint receptors 5-7 .Artificial vibration of the muscles can lead to activation of the muscle spindles, showing that their 61 activation is sufficient to alter the representation of the body and its position 8,9 . In addition, position 62 estimates have been found to be more precise following active vs. passive movements,suggesting that efferent motor commands may either affect or inform proprioceptive 64 representations 10-12 . Finally, proprioceptive information is combined with visual information, when 65 available, to form a multisensory and integrated representation 13-17 .Facial expressions present a particularly important yet poorly studied instance of motor control.On the one hand, we communicate a great deal of information with small, nuanced facial 68 movements (on the order of 10 mm or less 18,19 ). On the other hand, we hardly ever see ourselves 69 while making them. Perhaps with the exception of actors or public speakers who practice in front 70 of a mirror (or the increased number of video-conferences during the 2020 SARS-CoV-2 71 pandemic), we do not usually have online visual feedback about our facial muscles. If visual 72 feedback information is indeed critical to give rise to precise motor representations, facial 73 movements might be very poorly represented. Together, the combination of the high social 74 relevance of small movements in our facial muscles and the general lack of visual information 75 about them raise the interesting question: How much do we know about how we look when we 76 communicate with others? 77 4 Previous studies have focused on related questions. One line of research has quantified 78 metacognitive access to others' facial expressions 20-22 and operationalized metacognitive 79 performance as the precision of participants' representations of uncertainty. While our ability to 80 accurately represent both the facial expressions of others and our certainty about them is clearly 81 critical for social interactions, it is equally important to correctly represent and adequately control 82 one's own expressions 23 . In line with this notion, another line of research has aimed at measuring 83 how accurate the representation of one's own face is (under a neutral facial expression). One 84 study 24 found that participants showed a systematic bias to underestimate the length of their faces 85 and slightly overestimate their width, mimicking what has been described for whole bodies 25 and 86 hands 26 . More recently, large inter-individual differences have been described in how accurately 87 healthy young adults can represent their own faces 27 . These previous studies investigated relaxed 88 faces with neutral expressions and captured, in essence, individuals' ability to accurately describe 89 their face, or to discriminate it from the face of another. Importantly, static features of one's face 90 are irrelevant to social interactions, which instead are based on dynamic information. Here, we 91 focussed instead on metacognitive knowledge about how one's face varies when making different 92 expressions. In a pre-registered experiment, we asked participants to imitate expressions shown 93 in pictures of themselves and to rate how well they thought they had imitated the expression. We 94 then measured participants' metacognitive access to their own facial expressions as the 95 correspondence between subjective ratings and an objective measure of performance.First, participants completed a task to measure their metacognitive access to facial expressions 97 (Figure 1) , consisting of three parts. Briefly, in the first part of the task, participants took pictures 98 of themselves imitating different cue images done by actors 28 to generate 32 participant-specific 99 target images. In the second part, participants saw each of the target images on the screen and, 100 while still looking directly into the digital camera, imitated themselves (Figure 1.B) . In both the first 101 and second parts of the task, participants pressed a keyboard key to trigger the digital camera. In 102 the second part only, they additionally rated how confident they were in their own performance on 103 a continuous confidence scale ranging from "Very unsure" to "Very sure". Finally, in the third part 104 of the task, participants saw the target and response pictures side-by-side and rated them for 105 similarity on a continuous scale with the same labels as for the confidence rating. We quantified 106 the distance between each image pair based on landmarks placed automatically on the pictures. 107 5 108 Figure 1: Experimental Design. (A.) Procedure. Cue stimuli were pictures of facial expressions taken 109 from the MPI Small Facial Expression Database (Cunningham et al., 2005), but the images were replaced 110 here with illustrations, to comply with the journal's data privacy regulations. They were performed by actors 111 and represented non-stereotypical expressions (e.g., "You lose the way in a foreign city", see Methods for 112 further details). Participants used these images as cues to produce 32 participant-specific target images.In part 2, each of the 32 target images (of the participants' faces displaying the expression generated in 114 part 1) was shown eight times (256 trials total). Participants reproduced their own expressions shown in the 115 target pictures, pressed a key while holding their expression, and subsequently rated confidence in their 116 own performance. The experiment was self-paced. Squares around the pictures indicate that they were 117 displayed to participants, whereas pictures without a square frame around them represent pictures collected 118 but not shown back to participants. (Expression drawing: Freepik.com) (B.) Predictions. The correlation 119 between the two variables indicates the precision of the metacognitive representation. Confidence ratings 120 were expected to be negatively correlated with the distance between two images if participants have 121 metacognitive access to the low-level aspects of their facial expressions (solid line). Confidence ratings 122 were not expected to vary with distance if participants had no metacognitive access to their own facial 123 expressions (dashed line.124 125 126 Results 127 Confirmatory Analyses 128

